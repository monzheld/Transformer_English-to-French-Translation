{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_Section4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 프로젝트 개요:\n",
        "\n",
        "> 직접 설정한 포지션에 적합한 데이터셋을 구하고, 그에 맞는 가설을 세워 딥러닝 파이프라인을 구축해 가설을 검증하는 것\n",
        "\n"
      ],
      "metadata": {
        "id": "idbrgHQS1BeT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 프로젝트 절차:\n",
        "<br>\n",
        "\n",
        "### **1) 주제 선정** <br>\n",
        "- 프로젝트 주제\n",
        "\n",
        "<br>\n",
        "\n",
        "### **2) 데이터 선정** <br>\n",
        "- 데이터셋\n",
        "\n",
        "- Features\n",
        "\n",
        "<br>\n",
        "\n",
        "### **3) 데이터 선정 이유** <br>\n",
        "\n",
        "- 데이터 선정 이유\n",
        "\n",
        "<br>\n",
        "\n",
        "### **4) 문제 정의 및 가설 수립** <br>\n",
        "\n",
        "- 문제 정의 \n",
        "\n",
        "- 가설 설정\n",
        "\n",
        "<br>\n",
        "\n",
        "### **5) 데이터 전처리** <br>\n",
        "\n",
        "- EDA\n",
        "\n",
        "- 텍스트 전처리\n",
        "\n",
        "<br>\n",
        "\n",
        "### **6) 딥러닝 모델 생성** <br>\n",
        "\n",
        "- 모델 파이프라인 구축\n",
        "\n",
        "\n",
        "<br> \n",
        "\n",
        "### **7) 모델 평가** <br>\n",
        "\n",
        "- 모델 평가\n",
        "\n",
        "<br>\n",
        "\n",
        "### **8) 결론** <br>\n",
        "\n",
        "- 결론\n",
        "\n",
        "- 한계점 및 추후 해결 방안\n"
      ],
      "metadata": {
        "id": "e0QmwAtA12-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "8lSeWwhzjj5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 주제 선정"
      ],
      "metadata": {
        "id": "viBNIV3fqzdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "영어-프랑스어 기계 번역 모델"
      ],
      "metadata": {
        "id": "-v6xqttwq2Q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 데이터 선정"
      ],
      "metadata": {
        "id": "NC7hnmFD4KDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) 데이터셋\n",
        "\n",
        "- [French - English](http://www.manythings.org/anki/)\n",
        "\n",
        "   - 영어-프랑스어 병렬 말뭉치 "
      ],
      "metadata": {
        "id": "kFyQVE5c4OWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Features\n",
        "\n",
        "- 영어\n",
        "- 프랑스어"
      ],
      "metadata": {
        "id": "wAZbRvRY4iaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 데이터 선정 이유"
      ],
      "metadata": {
        "id": "R0_WJPWp4xfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 전 세계 사람들이 언어 장벽 없이 자유롭게 소통할 수 있을 만큼 정확한 번역 모델을 만드는 데 기여하는 것이 목표\n",
        "\n",
        "- 전공을 적용해 보고 싶어 프랑스어로 선택"
      ],
      "metadata": {
        "id": "bk7_t1cL405_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 문제 정의 및 가설 수립"
      ],
      "metadata": {
        "id": "LiIRzbPN5ADr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) 문제 정의\n"
      ],
      "metadata": {
        "id": "33LSSFfa5BxE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**자연어 이해(NLU)**와 **자연어 생성(NLG)** <br>\n",
        "\n",
        "=> 기계 번역(Machine Translation)"
      ],
      "metadata": {
        "id": "pogyU8kp5I51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) 가설 수립"
      ],
      "metadata": {
        "id": "LyqXh07U9EM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> 10만 개의 데이터로 학습된 번역 모델은 문법적 특성을 지키지 못한다.\n",
        "\n"
      ],
      "metadata": {
        "id": "NOAwRhV55NTW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 데이터 전처리\n"
      ],
      "metadata": {
        "id": "NBh3Vt5q5RbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) 패키지와 라이브러리 불러오기"
      ],
      "metadata": {
        "id": "jMkjgZvz5jXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorflow-text"
      ],
      "metadata": {
        "id": "OxkjDGqPPW84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import logging\n",
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "ftHhopIBPNA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.getLogger('tensorflow').setLevel(logging.ERROR) "
      ],
      "metadata": {
        "id": "4I8WyiUwPWc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) 데이터셋 로드 "
      ],
      "metadata": {
        "id": "pC67QfVq6BPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9E3swA5CSIza",
        "outputId": "ca81f06e-8b66-4f56-c986-f40024d2cab5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 영어-불어 데이터셋 불러오기\n",
        "#fra_txt = '/content/drive/MyDrive/Colab Notebooks/AI 부트캠프 9기/Section 04/Project_Section 04/데이터셋_Project_Section 04/원본/fra.txt'\n",
        "#f_txt = pd.read_table(fra_txt)"
      ],
      "metadata": {
        "id": "FfwgJnoeSn4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# txt 파일을 csv 파일로 변환\n",
        "#f_txt.to_csv(\"./fra_csv.csv\",index=False)"
      ],
      "metadata": {
        "id": "0chhPVphTAKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 영어-불어 데이터셋 csv 파일 불러오기\n",
        "fra_csv = '/content/drive/MyDrive/Colab Notebooks/AI 부트캠프 9기/Section 04/Project_Section 04/데이터셋_Project_Section 04/csv/fra_csv.csv'\n",
        "f_csv = pd.read_csv(fra_csv)"
      ],
      "metadata": {
        "id": "dZNAQeM9TWy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_csv "
      ],
      "metadata": {
        "id": "NKiY0WGrUjgB",
        "outputId": "55b44d5d-3e7e-410d-84fa-71ef91f6407f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                      Go.  \\\n",
              "0                                                     Go.   \n",
              "1                                                     Go.   \n",
              "2                                                     Hi.   \n",
              "3                                                     Hi.   \n",
              "4                                                    Run!   \n",
              "...                                                   ...   \n",
              "192335  A carbon footprint is the amount of carbon dio...   \n",
              "192336  Death is something that we're often discourage...   \n",
              "192337  Since there are usually multiple websites on a...   \n",
              "192338  If someone who doesn't know your background sa...   \n",
              "192339  It may be impossible to get a completely error...   \n",
              "\n",
              "                                                     Va !  \\\n",
              "0                                                 Marche.   \n",
              "1                                                 Bouge !   \n",
              "2                                                 Salut !   \n",
              "3                                                  Salut.   \n",
              "4                                                 Cours !   \n",
              "...                                                   ...   \n",
              "192335  Une empreinte carbone est la somme de pollutio...   \n",
              "192336  La mort est une chose qu'on nous décourage sou...   \n",
              "192337  Puisqu'il y a de multiples sites web sur chaqu...   \n",
              "192338  Si quelqu'un qui ne connaît pas vos antécédent...   \n",
              "192339  Il est peut-être impossible d'obtenir un Corpu...   \n",
              "\n",
              "       CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)  \n",
              "0       CC-BY 2.0 (France) Attribution: tatoeba.org #2...                               \n",
              "1       CC-BY 2.0 (France) Attribution: tatoeba.org #2...                               \n",
              "2       CC-BY 2.0 (France) Attribution: tatoeba.org #5...                               \n",
              "3       CC-BY 2.0 (France) Attribution: tatoeba.org #5...                               \n",
              "4       CC-BY 2.0 (France) Attribution: tatoeba.org #9...                               \n",
              "...                                                   ...                               \n",
              "192335  CC-BY 2.0 (France) Attribution: tatoeba.org #1...                               \n",
              "192336  CC-BY 2.0 (France) Attribution: tatoeba.org #1...                               \n",
              "192337  CC-BY 2.0 (France) Attribution: tatoeba.org #9...                               \n",
              "192338  CC-BY 2.0 (France) Attribution: tatoeba.org #9...                               \n",
              "192339  CC-BY 2.0 (France) Attribution: tatoeba.org #2...                               \n",
              "\n",
              "[192340 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d7016092-a94c-4a7d-ae02-b9e593325c0b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Go.</th>\n",
              "      <th>Va !</th>\n",
              "      <th>CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) &amp; #1158250 (Wittydev)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Marche.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Bouge !</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut !</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Cours !</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192335</th>\n",
              "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
              "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192336</th>\n",
              "      <td>Death is something that we're often discourage...</td>\n",
              "      <td>La mort est une chose qu'on nous décourage sou...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192337</th>\n",
              "      <td>Since there are usually multiple websites on a...</td>\n",
              "      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192338</th>\n",
              "      <td>If someone who doesn't know your background sa...</td>\n",
              "      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192339</th>\n",
              "      <td>It may be impossible to get a completely error...</td>\n",
              "      <td>Il est peut-être impossible d'obtenir un Corpu...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>192340 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d7016092-a94c-4a7d-ae02-b9e593325c0b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d7016092-a94c-4a7d-ae02-b9e593325c0b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d7016092-a94c-4a7d-ae02-b9e593325c0b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) EDA"
      ],
      "metadata": {
        "id": "k3h4ZP0W6Qrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# column 전처리 함수 \n",
        "\n",
        "def drop_rename_col(df):\n",
        "\n",
        "  # 필요없는 column 삭제\n",
        "  dels = [col for col in df.columns if ('CC' in col)]\n",
        "  df.drop(columns=dels, inplace=True)\n",
        "\n",
        "  # 칼럼 명 변경\n",
        "  df.columns = ['eng', 'fra']\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "_sKBhYWrUr5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋에 drop_rename_col 함수 적용 \n",
        "eng_fra_csv = drop_rename_col(f_csv)"
      ],
      "metadata": {
        "id": "eLPog-CiVScR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_fra_csv"
      ],
      "metadata": {
        "id": "taK1Xo8oVajC",
        "outputId": "aff33b45-97a6-4031-9e92-a1a72fc61e2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                      eng  \\\n",
              "0                                                     Go.   \n",
              "1                                                     Go.   \n",
              "2                                                     Hi.   \n",
              "3                                                     Hi.   \n",
              "4                                                    Run!   \n",
              "...                                                   ...   \n",
              "192335  A carbon footprint is the amount of carbon dio...   \n",
              "192336  Death is something that we're often discourage...   \n",
              "192337  Since there are usually multiple websites on a...   \n",
              "192338  If someone who doesn't know your background sa...   \n",
              "192339  It may be impossible to get a completely error...   \n",
              "\n",
              "                                                      fra  \n",
              "0                                                 Marche.  \n",
              "1                                                 Bouge !  \n",
              "2                                                 Salut !  \n",
              "3                                                  Salut.  \n",
              "4                                                 Cours !  \n",
              "...                                                   ...  \n",
              "192335  Une empreinte carbone est la somme de pollutio...  \n",
              "192336  La mort est une chose qu'on nous décourage sou...  \n",
              "192337  Puisqu'il y a de multiples sites web sur chaqu...  \n",
              "192338  Si quelqu'un qui ne connaît pas vos antécédent...  \n",
              "192339  Il est peut-être impossible d'obtenir un Corpu...  \n",
              "\n",
              "[192340 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b0778154-c135-4b62-b94e-25029653ba81\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>fra</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Marche.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Bouge !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Cours !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192335</th>\n",
              "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
              "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192336</th>\n",
              "      <td>Death is something that we're often discourage...</td>\n",
              "      <td>La mort est une chose qu'on nous décourage sou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192337</th>\n",
              "      <td>Since there are usually multiple websites on a...</td>\n",
              "      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192338</th>\n",
              "      <td>If someone who doesn't know your background sa...</td>\n",
              "      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192339</th>\n",
              "      <td>It may be impossible to get a completely error...</td>\n",
              "      <td>Il est peut-être impossible d'obtenir un Corpu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>192340 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0778154-c135-4b62-b94e-25029653ba81')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b0778154-c135-4b62-b94e-25029653ba81 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b0778154-c135-4b62-b94e-25029653ba81');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 크기\n",
        "eng_fra_csv.shape"
      ],
      "metadata": {
        "id": "OvgRoRLeVsvZ",
        "outputId": "db1840e7-b28c-4b4f-98f5-2e365460d792",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(192340, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# info\n",
        "eng_fra_csv.info()"
      ],
      "metadata": {
        "id": "Iv7F9g3eVwrO",
        "outputId": "91cb8f73-3f19-482d-bc28-b31d1010097a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 192340 entries, 0 to 192339\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count   Dtype \n",
            "---  ------  --------------   ----- \n",
            " 0   eng     192340 non-null  object\n",
            " 1   fra     192340 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 2.9+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 결측치 확인\n",
        "eng_fra_csv.isnull().sum()"
      ],
      "metadata": {
        "id": "VxLDwGMeVzFZ",
        "outputId": "42b8818b-ebc8-4335-ca19-7fd720b75d4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "eng    0\n",
              "fra    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 중복 확인\n",
        "eng_fra_csv.T.duplicated()"
      ],
      "metadata": {
        "id": "RG_FjSWPV3k5",
        "outputId": "0fc08a75-e5ae-4ddf-8082-8202b1be94b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "eng    False\n",
              "fra    False\n",
              "dtype: bool"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10만 개의 데이터 랜덤 추출 함수\n",
        "\n",
        "def extract_sample(df):\n",
        "\n",
        "  # 추출할 데이터 개수: 100000, 비복원 추출, random_state=42, index reset \n",
        "  df = df.sample(n=100000, replace=False, random_state=42, ignore_index=True)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "FIk4jVdqGeDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 데이터셋에서 10만 개의 데이터 추출\n",
        "eng_fra_10 = extract_sample(eng_fra_csv)"
      ],
      "metadata": {
        "id": "UAR3A7nUGk7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_fra_10"
      ],
      "metadata": {
        "id": "koN7nK57G2eZ",
        "outputId": "511032bc-e605-4d7e-d84e-225cca96bb74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                     eng  \\\n",
              "0                             I'm as curious as you are.   \n",
              "1                What kind of things do you enjoy doing?   \n",
              "2                                       Just stay there.   \n",
              "3                                     It's your bedtime.   \n",
              "4                    Who wears the pants in your family?   \n",
              "...                                                  ...   \n",
              "99995                               Nobody really knows.   \n",
              "99996  All in all, the international conference was a...   \n",
              "99997                         Let's hope he's all right.   \n",
              "99998  I'm begging you. Don't make me laugh. I did to...   \n",
              "99999                               Who did you go with?   \n",
              "\n",
              "                                                     fra  \n",
              "0                        Je suis aussi curieux que vous.  \n",
              "1         À quelle sorte de choses prenez-vous plaisir ?  \n",
              "2                                             Reste ici.  \n",
              "3                      C'est l'heure d'aller te coucher.  \n",
              "4              Qui porte la culotte dans votre famille ?  \n",
              "...                                                  ...  \n",
              "99995                         Personne ne sait vraiment.  \n",
              "99996  Globalement, la conférence internationale fut ...  \n",
              "99997                            Espérons qu'il va bien.  \n",
              "99998  Je t'en supplie. Ne me fais pas rire. J'ai fai...  \n",
              "99999                          Avec qui êtes-vous allé ?  \n",
              "\n",
              "[100000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2cc451b0-374d-4961-82a7-158ba563b819\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>fra</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I'm as curious as you are.</td>\n",
              "      <td>Je suis aussi curieux que vous.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What kind of things do you enjoy doing?</td>\n",
              "      <td>À quelle sorte de choses prenez-vous plaisir ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Just stay there.</td>\n",
              "      <td>Reste ici.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>It's your bedtime.</td>\n",
              "      <td>C'est l'heure d'aller te coucher.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Who wears the pants in your family?</td>\n",
              "      <td>Qui porte la culotte dans votre famille ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99995</th>\n",
              "      <td>Nobody really knows.</td>\n",
              "      <td>Personne ne sait vraiment.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99996</th>\n",
              "      <td>All in all, the international conference was a...</td>\n",
              "      <td>Globalement, la conférence internationale fut ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99997</th>\n",
              "      <td>Let's hope he's all right.</td>\n",
              "      <td>Espérons qu'il va bien.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99998</th>\n",
              "      <td>I'm begging you. Don't make me laugh. I did to...</td>\n",
              "      <td>Je t'en supplie. Ne me fais pas rire. J'ai fai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99999</th>\n",
              "      <td>Who did you go with?</td>\n",
              "      <td>Avec qui êtes-vous allé ?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2cc451b0-374d-4961-82a7-158ba563b819')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2cc451b0-374d-4961-82a7-158ba563b819 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2cc451b0-374d-4961-82a7-158ba563b819');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train/test 셋으로 나누기\n",
        "train, test = train_test_split(eng_fra_10, test_size=0.2)\n",
        "\n",
        "print('train shape :', train.shape)\n",
        "print('test shape :', test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "px9STh9xRYji",
        "outputId": "b7de828f-cb8b-4974-95db-313dd7f52eed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train shape : (80000, 2)\n",
            "test shape : (20000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.data.Dataset 으로 변환하는 함수 \n",
        "\n",
        "def tf_dataset(df):\n",
        "\n",
        "  eng = df['eng']\n",
        "  fra = df['fra']\n",
        "  \n",
        "  df_dataset = tf.data.Dataset.from_tensor_slices((eng,fra))\n",
        "\n",
        "  return df_dataset"
      ],
      "metadata": {
        "id": "AXxOJxnLNlNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train/test 셋을 tf.data.Dataset 으로 변환\n",
        "train_dataset = tf_dataset(train)\n",
        "test_dataset = tf_dataset(test)"
      ],
      "metadata": {
        "id": "DWr7MG92Rv-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gC7-BvafR0Hy",
        "outputId": "4466f0ee-8b1e-4d14-df70-61103d9a089a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt-0tko5TMcL",
        "outputId": "fbb6f5ba-01a7-44e2-a4ac-3b028251a864"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset의 데이터 확인\n",
        "for en_examples, fr_examples in train_dataset.batch(3).take(1):\n",
        "  for en in en_examples.numpy():\n",
        "    print(en.decode('utf-8'))\n",
        "\n",
        "  print()\n",
        "\n",
        "  for fr in fr_examples.numpy():\n",
        "    print(fr.decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2w7VyhJsR3Ar",
        "outputId": "71934d14-bf4f-4619-8397-4846eeae08ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He stayed in bed because he wasn't feeling well.\n",
            "First of all, I must say this.\n",
            "She was clinging to her father.\n",
            "\n",
            "Il est resté au lit parce qu'il ne se sentait pas bien.\n",
            "Tout d'abord, je dois dire ceci.\n",
            "Elle se cramponnait à son père.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) 텍스트 전처리"
      ],
      "metadata": {
        "id": "wsnY2wjm8bZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for en, fr in train_dataset.take(1):\n",
        "  print(\"English: \", en.numpy().decode('utf-8'))\n",
        "  print(\"French: \", fr.numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uneox-riTaTJ",
        "outputId": "c4638922-39db-44dc-8305-0f0ea549ed90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English:  He stayed in bed because he wasn't feeling well.\n",
            "French:  Il est resté au lit parce qu'il ne se sentait pas bien.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_en = train_dataset.map(lambda en, fr: en)\n",
        "train_fr = train_dataset.map(lambda en, fr: fr)"
      ],
      "metadata": {
        "id": "Qs5_nBhETl6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋에서 어휘 생성"
      ],
      "metadata": {
        "id": "eSsf-e7OT7XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
      ],
      "metadata": {
        "id": "G-quhaIoUGJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tokenizer_params=dict(lower_case=True)\n",
        "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "\n",
        "bert_vocab_args = dict(\n",
        "    vocab_size = 15000,\n",
        "    reserved_tokens=reserved_tokens,\n",
        "    bert_tokenizer_params=bert_tokenizer_params,\n",
        "    learn_params={},\n",
        ")"
      ],
      "metadata": {
        "id": "4rRYro3-UHcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 영어 데이터에서 어휘 생성"
      ],
      "metadata": {
        "id": "PobZGE4GUulb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_en.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g6lx-jaUZbE",
        "outputId": "6a3b9e5a-9060-454b-bd86-e6b1dc787008"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 31.2 s, sys: 645 ms, total: 31.8 s\n",
            "Wall time: 30.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 어휘 확인\n",
        "print(en_vocab[1000:1010])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zZlJ05MU8Mg",
        "outputId": "69d85a9c-b61e-40f8-95ca-8ad69868e20e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['guitar', 'impossible', 'joke', 'knife', 'movies', 'passed', 'showed', 'staying', 'television', 'vacation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 어휘 파일 작성\n",
        "def write_vocab_file(filepath, vocab):\n",
        "  with open(filepath, 'w') as f:\n",
        "    for token in vocab:\n",
        "      print(token, file=f)"
      ],
      "metadata": {
        "id": "CWJHB53ZUygu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_vocab_file('en_vocab.txt', en_vocab)"
      ],
      "metadata": {
        "id": "m4EpXImUVSCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 프랑스어 데이터에서 어휘 생성"
      ],
      "metadata": {
        "id": "kqVVuvNaVcH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "fr_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_fr.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1GzKlr4Vfk9",
        "outputId": "88c69c0a-2897-4b85-9de1-82e586efdc27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 3s, sys: 682 ms, total: 1min 3s\n",
            "Wall time: 1min 12s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 어휘 확인\n",
        "print(fr_vocab[1000:1010])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aILKxDLsVmPQ",
        "outputId": "8e523a72-06f6-4690-a175-3686948da1a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['pommes', 'quitte', 'regles', 'tableau', '##rer', '##ront', 'arme', 'aviez', 'calme', 'doute']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 어휘 파일 생성\n",
        "write_vocab_file('fr_vocab.txt', fr_vocab)"
      ],
      "metadata": {
        "id": "4kkaiwRPVpZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CustomTokenizer 생성"
      ],
      "metadata": {
        "id": "ZTsAeikFWmbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
        "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
        "\n",
        "def add_start_end(ragged):\n",
        "  count = ragged.bounding_shape()[0]\n",
        "  starts = tf.fill([count,1], START)\n",
        "  ends = tf.fill([count,1], END)\n",
        "  return tf.concat([starts, ragged, ends], axis=1)"
      ],
      "metadata": {
        "id": "_fO1rZh2WqKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanup_text(reserved_tokens, token_txt):\n",
        "  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
        "  bad_token_re = \"|\".join(bad_tokens)\n",
        "\n",
        "  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
        "  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
        "\n",
        "  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "6Vded5i4Wtb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTokenizer(tf.Module):\n",
        "  def __init__(self, reserved_tokens, vocab_path):\n",
        "    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
        "    self._reserved_tokens = reserved_tokens\n",
        "    self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
        "\n",
        "    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
        "    self.vocab = tf.Variable(vocab)\n",
        " \n",
        "\n",
        "    self.tokenize.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None], dtype=tf.string))\n",
        "\n",
        "    self.detokenize.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.detokenize.get_concrete_function(\n",
        "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "\n",
        "    self.lookup.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.lookup.get_concrete_function(\n",
        "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "\n",
        "    self.get_vocab_size.get_concrete_function()\n",
        "    self.get_vocab_path.get_concrete_function()\n",
        "    self.get_reserved_tokens.get_concrete_function()\n",
        "\n",
        "  @tf.function\n",
        "  def tokenize(self, strings):\n",
        "    enc = self.tokenizer.tokenize(strings)\n",
        "    enc = enc.merge_dims(-2,-1)\n",
        "    enc = add_start_end(enc)\n",
        "    return enc\n",
        "\n",
        "  @tf.function\n",
        "  def detokenize(self, tokenized):\n",
        "    words = self.tokenizer.detokenize(tokenized)\n",
        "    return cleanup_text(self._reserved_tokens, words)\n",
        "\n",
        "  @tf.function\n",
        "  def lookup(self, token_ids):\n",
        "    return tf.gather(self.vocab, token_ids)\n",
        "\n",
        "  @tf.function\n",
        "  def get_vocab_size(self):\n",
        "    return tf.shape(self.vocab)[0]\n",
        "\n",
        "  @tf.function\n",
        "  def get_vocab_path(self):\n",
        "    return self._vocab_path\n",
        "\n",
        "  @tf.function\n",
        "  def get_reserved_tokens(self):\n",
        "    return tf.constant(self._reserved_tokens)"
      ],
      "metadata": {
        "id": "GaJiPaYOWPd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 언어에 대해 CustomTokenizer 생성\n",
        "tokenizers = tf.Module()\n",
        "tokenizers.en = CustomTokenizer(reserved_tokens, 'en_vocab.txt')\n",
        "tokenizers.fr = CustomTokenizer(reserved_tokens, 'fr_vocab.txt')"
      ],
      "metadata": {
        "id": "Ik7HxlCLWZRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizers 저장하기 \n",
        "model_name = 'en_fr_tokenizers'\n",
        "tf.saved_model.save(tokenizers, model_name)"
      ],
      "metadata": {
        "id": "wV8GW_ILW12m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 저장된 tokenizers 가져오기\n",
        "tokenizers = tf.saved_model.load('en_fr_tokenizers')"
      ],
      "metadata": {
        "id": "lvseLNW_X-69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizers.en.get_vocab_size().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2vlqgx8YEbj",
        "outputId": "6dad1416-002a-4846-c187-658753bb1146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3347"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizers.fr.get_vocab_size().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RkLzaNnYFz0",
        "outputId": "809dd2e5-de5d-4682-da28-72495f2fae3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4377"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 프랑스어 문장 예시 \n",
        "for fr in fr_examples.numpy():\n",
        "  print(fr.decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KITFzFAYYqe",
        "outputId": "79529c33-f908-4308-8edc-da5b68f1b087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Il est resté au lit parce qu'il ne se sentait pas bien.\n",
            "Tout d'abord, je dois dire ceci.\n",
            "Elle se cramponnait à son père.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화\n",
        "encoded = tokenizers.fr.tokenize(fr_examples)\n",
        "\n",
        "for row in encoded.to_list():\n",
        "  print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWD-o-9kYmcc",
        "outputId": "f7b659e6-321a-4c46-b2e7-91788ee76223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 68, 64, 293, 101, 403, 630, 86, 9, 68, 70, 106, 2012, 65, 142, 15, 3]\n",
            "[2, 93, 33, 9, 1620, 13, 62, 182, 146, 201, 15, 3]\n",
            "[2, 85, 106, 32, 574, 1364, 1416, 3246, 336, 30, 118, 217, 15, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# detokenize로 다시 텍스트로 변환\n",
        "round_trip = tokenizers.fr.detokenize(encoded)\n",
        "for line in round_trip.numpy():\n",
        "  print(line.decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "detxHiDbYqup",
        "outputId": "a847a085-3eb8-4133-bf1f-6871a81f28ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "il est reste au lit parce qu ' il ne se sentait pas bien .\n",
            "tout d ' abord , je dois dire ceci .\n",
            "elle se cramponnait a son pere .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 파이프라인 설정"
      ],
      "metadata": {
        "id": "OkojJp-QZBMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 배치 인코딩\n",
        "def tokenize_pairs(en, fr):\n",
        "\n",
        "    en = tokenizers.en.tokenize(en)\n",
        "    en = en.to_tensor()\n",
        "\n",
        "    fr = tokenizers.fr.tokenize(fr)\n",
        "    fr = fr.to_tensor()\n",
        "    \n",
        "    return en, fr"
      ],
      "metadata": {
        "id": "ipCg8b6YZDle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "iHtdXHwfZWRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batches 생성\n",
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .cache()\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      .prefetch(tf.data.AUTOTUNE))"
      ],
      "metadata": {
        "id": "jLc_6kRpZap9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train/test dataset에 적용\n",
        "train_batches = make_batches(train_dataset)\n",
        "test_batches = make_batches(test_dataset)"
      ],
      "metadata": {
        "id": "0eftMntaZpMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. 딥러닝 모델 생성"
      ],
      "metadata": {
        "id": "iCv2Tg7mqppZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) 위치 인코딩 (positional encoding)"
      ],
      "metadata": {
        "id": "VSKD51uOrXRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sin, cos 안에 들어갈 수치 구하기\n",
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ],
      "metadata": {
        "id": "7ZuNxuKD--1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 위치 인코딩 (positional encoding)\n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # 짝수번째 인덱스에 sin 함수 적용\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # 홀수번째 인덱스에 cos 함수 적용\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "ixNV58SD_Ezk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) padding mask"
      ],
      "metadata": {
        "id": "sw12fRKjYptB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# padding_mask\n",
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  "
      ],
      "metadata": {
        "id": "OeUJx3q7_Oy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) look ahead mask"
      ],
      "metadata": {
        "id": "78asjUyfYtMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# look_ahead_mask\n",
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  "
      ],
      "metadata": {
        "id": "pAOxkXpF_Yy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) scaled dot product attention "
      ],
      "metadata": {
        "id": "PMUljmy_Yyj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scaled_dot_product_attention \n",
        "# Attention의 가중치를 구하는 함수\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  \n",
        "\n",
        "  # 쿼리와 키의 내적을 dk의 제곱근으로 스케일링\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # 마스킹 적용\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "  # softmax 함수로 attention 가중치 구하기\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) \n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  \n",
        "\n",
        "  return output, attention_weights"
      ],
      "metadata": {
        "id": "ZNIcqy7K_fjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5) Multi-Head (Self) Attention"
      ],
      "metadata": {
        "id": "lQOh5keSY9uC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def split_heads(self, x, batch_size):\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "\n",
        "    q = self.wq(q)  \n",
        "    k = self.wk(k)  \n",
        "    v = self.wv(v)  \n",
        "\n",
        "    q = self.split_heads(q, batch_size)  \n",
        "    k = self.split_heads(k, batch_size)  \n",
        "    v = self.split_heads(v, batch_size)  \n",
        "\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))  \n",
        "\n",
        "    output = self.dense(concat_attention)  \n",
        "\n",
        "    return output, attention_weights"
      ],
      "metadata": {
        "id": "i1zELCF2At_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6) FFNN (Feed Forward Neural Network)"
      ],
      "metadata": {
        "id": "w9qBsR9NZlya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FFNN (Feed Forward Neural Network)\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  \n",
        "      tf.keras.layers.Dense(d_model)  \n",
        "  ])"
      ],
      "metadata": {
        "id": "lNYYgYHBBAOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7) Encoder Layer\n",
        "\n",
        "- Multi-Head (Self) Attention\n",
        "- FFNN"
      ],
      "metadata": {
        "id": "9RHZ1Y4ZZrdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  \n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  \n",
        "\n",
        "    ffn_output = self.ffn(out1) \n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output) \n",
        "\n",
        "    return out2"
      ],
      "metadata": {
        "id": "1eieyhtGBN8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8) Decoder Layer\n",
        "\n",
        "- Masked Multi-Head (Self) Attention\n",
        "- Multi-Head (Encoder-Decoder) Attention\n",
        "- FFNN"
      ],
      "metadata": {
        "id": "Yq9vSqJ7Z1Qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  \n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  \n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  \n",
        "\n",
        "    ffn_output = self.ffn(out2)  \n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2) \n",
        "\n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ],
      "metadata": {
        "id": "wLbfXM4oBmb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9) Encoder \n",
        "\n",
        "- Input Embedding\n",
        "- positional encoding\n",
        "- 6 Encoder Layer "
      ],
      "metadata": {
        "id": "1QWKmlE8aEBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
        "                                            self.d_model)\n",
        "\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
        "                       for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "\n",
        "    # embedding, position encoding 추가\n",
        "    x = self.embedding(x) \n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x  "
      ],
      "metadata": {
        "id": "kgirY39XBzvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10) Decoder \n",
        "\n",
        "- Output Embedding\n",
        "- positional encoding\n",
        "- 6 Decoder Layer "
      ],
      "metadata": {
        "id": "H9s5gRjaad_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
        "                       for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "\n",
        "    x = self.embedding(x)  \n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "\n",
        "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "    return x, attention_weights"
      ],
      "metadata": {
        "id": "CxA5FIFiCEW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11) Transformer\n",
        "\n",
        "- Encoder\n",
        "- Decoder\n",
        "- Final Linear Layer"
      ],
      "metadata": {
        "id": "rjR2rxTNan7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
        "                             input_vocab_size, pe_input, rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
        "                           target_vocab_size, pe_target, rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs, training):\n",
        "\n",
        "    inp, tar = inputs\n",
        "\n",
        "    enc_padding_mask, look_ahead_mask, dec_padding_mask = self.create_masks(inp, tar)\n",
        "\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)  \n",
        "\n",
        "\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "    final_output = self.final_layer(dec_output)  \n",
        "\n",
        "    return final_output, attention_weights\n",
        "\n",
        "  def create_masks(self, inp, tar):\n",
        "    # Encoder padding_mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Decoder의 2번째 Attention 블록에서 사용\n",
        "    # Encoder의 output들을 padding_mask 하기 위해 사용\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Decoder의 첫 번째 Attention 블록에서 사용\n",
        "    # Decoder로부터 받은 input에 look_ahead_mask와 padding_mask 사용\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return enc_padding_mask, look_ahead_mask, dec_padding_mask"
      ],
      "metadata": {
        "id": "v7m1si-vCY0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12) 하이퍼파라미터"
      ],
      "metadata": {
        "id": "VrXsvvINbCkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 6\n",
        "d_model = 256\n",
        "dff = 2048\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ],
      "metadata": {
        "id": "ipa70u2ID_Q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13) 학습률(learning_rate)과 옵티마이저(optimizer)"
      ],
      "metadata": {
        "id": "jZwCK62CbH_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning Rate Schedule\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "metadata": {
        "id": "1LNuCG8DEKJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습률(learning_rate)과 옵티마이저(optimizer -> Adam)\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "metadata": {
        "id": "BPGa0GuPEPe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14) 손실 함수 (loss function)"
      ],
      "metadata": {
        "id": "GLWUvrxEbNtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ],
      "metadata": {
        "id": "E9YALB-wEisn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "DmGzOEKrElGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15) 측정 항목 (metrics)"
      ],
      "metadata": {
        "id": "bdL1fsgLbUdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ],
      "metadata": {
        "id": "EeyyUUB0Eown"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 16) Transformer 모델 생성"
      ],
      "metadata": {
        "id": "bkEBNGrNba2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=tokenizers.en.get_vocab_size().numpy(), # input: 영어\n",
        "    target_vocab_size=tokenizers.fr.get_vocab_size().numpy(), # target: 프랑스어\n",
        "    pe_input=1000,\n",
        "    pe_target=1000,\n",
        "    rate=dropout_rate)"
      ],
      "metadata": {
        "id": "y2wtkKrcEsxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 17) checkpoint"
      ],
      "metadata": {
        "id": "t_Vs3C1sbj_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint 경로, 매니저 생성\n",
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print('Latest checkpoint restored.')"
      ],
      "metadata": {
        "id": "RVVD5yiYE3_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 18) 모델 학습"
      ],
      "metadata": {
        "id": "CuFfsPkebrU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 100"
      ],
      "metadata": {
        "id": "sSLqQCIqFCr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer([inp, tar_inp],\n",
        "                                 training = True)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(tar_real, predictions))"
      ],
      "metadata": {
        "id": "PhofgvvHFGiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  # inp -> en, tar -> fr\n",
        "  for (batch, (inp, tar)) in enumerate(train_batches): \n",
        "    train_step(inp, tar)\n",
        "\n",
        "    if batch % 50 == 0:\n",
        "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
      ],
      "metadata": {
        "id": "mX_cb1_eFI2w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "373c86ca-541d-42c3-c9d8-d350a59027bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 8.4814 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 8.1757 Accuracy 0.0382\n",
            "Epoch 1 Batch 100 Loss 7.8937 Accuracy 0.0603\n",
            "Epoch 1 Batch 150 Loss 7.6729 Accuracy 0.0734\n",
            "Epoch 1 Batch 200 Loss 7.4214 Accuracy 0.0955\n",
            "Epoch 1 Batch 250 Loss 7.1559 Accuracy 0.1154\n",
            "Epoch 1 Batch 300 Loss 6.9035 Accuracy 0.1335\n",
            "Epoch 1 Batch 350 Loss 6.6745 Accuracy 0.1492\n",
            "Epoch 1 Batch 400 Loss 6.4787 Accuracy 0.1633\n",
            "Epoch 1 Batch 450 Loss 6.2973 Accuracy 0.1786\n",
            "Epoch 1 Batch 500 Loss 6.1339 Accuracy 0.1929\n",
            "Epoch 1 Batch 550 Loss 5.9885 Accuracy 0.2057\n",
            "Epoch 1 Batch 600 Loss 5.8536 Accuracy 0.2176\n",
            "Epoch 1 Batch 650 Loss 5.7317 Accuracy 0.2283\n",
            "Epoch 1 Batch 700 Loss 5.6185 Accuracy 0.2380\n",
            "Epoch 1 Batch 750 Loss 5.5137 Accuracy 0.2470\n",
            "Epoch 1 Batch 800 Loss 5.4203 Accuracy 0.2547\n",
            "Epoch 1 Batch 850 Loss 5.3289 Accuracy 0.2623\n",
            "Epoch 1 Batch 900 Loss 5.2469 Accuracy 0.2690\n",
            "Epoch 1 Batch 950 Loss 5.1680 Accuracy 0.2752\n",
            "Epoch 1 Batch 1000 Loss 5.0937 Accuracy 0.2814\n",
            "Epoch 1 Batch 1050 Loss 5.0252 Accuracy 0.2870\n",
            "Epoch 1 Batch 1100 Loss 4.9605 Accuracy 0.2923\n",
            "Epoch 1 Batch 1150 Loss 4.8983 Accuracy 0.2973\n",
            "Epoch 1 Batch 1200 Loss 4.8381 Accuracy 0.3023\n",
            "Epoch 1 Loss 4.7827 Accuracy 0.3070\n",
            "Time taken for 1 epoch: 161.21 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 3.4569 Accuracy 0.3918\n",
            "Epoch 2 Batch 50 Loss 3.4000 Accuracy 0.4197\n",
            "Epoch 2 Batch 100 Loss 3.3710 Accuracy 0.4225\n",
            "Epoch 2 Batch 150 Loss 3.3334 Accuracy 0.4272\n",
            "Epoch 2 Batch 200 Loss 3.3265 Accuracy 0.4273\n",
            "Epoch 2 Batch 250 Loss 3.3104 Accuracy 0.4286\n",
            "Epoch 2 Batch 300 Loss 3.2932 Accuracy 0.4300\n",
            "Epoch 2 Batch 350 Loss 3.2784 Accuracy 0.4316\n",
            "Epoch 2 Batch 400 Loss 3.2612 Accuracy 0.4328\n",
            "Epoch 2 Batch 450 Loss 3.2461 Accuracy 0.4343\n",
            "Epoch 2 Batch 500 Loss 3.2310 Accuracy 0.4358\n",
            "Epoch 2 Batch 550 Loss 3.2140 Accuracy 0.4373\n",
            "Epoch 2 Batch 600 Loss 3.1948 Accuracy 0.4393\n",
            "Epoch 2 Batch 650 Loss 3.1777 Accuracy 0.4411\n",
            "Epoch 2 Batch 700 Loss 3.1598 Accuracy 0.4429\n",
            "Epoch 2 Batch 750 Loss 3.1442 Accuracy 0.4445\n",
            "Epoch 2 Batch 800 Loss 3.1299 Accuracy 0.4460\n",
            "Epoch 2 Batch 850 Loss 3.1172 Accuracy 0.4471\n",
            "Epoch 2 Batch 900 Loss 3.1034 Accuracy 0.4484\n",
            "Epoch 2 Batch 950 Loss 3.0916 Accuracy 0.4495\n",
            "Epoch 2 Batch 1000 Loss 3.0779 Accuracy 0.4508\n",
            "Epoch 2 Batch 1050 Loss 3.0650 Accuracy 0.4521\n",
            "Epoch 2 Batch 1100 Loss 3.0530 Accuracy 0.4532\n",
            "Epoch 2 Batch 1150 Loss 3.0407 Accuracy 0.4543\n",
            "Epoch 2 Batch 1200 Loss 3.0296 Accuracy 0.4554\n",
            "Epoch 2 Loss 3.0197 Accuracy 0.4563\n",
            "Time taken for 1 epoch: 142.84 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 2.7429 Accuracy 0.4853\n",
            "Epoch 3 Batch 50 Loss 2.6742 Accuracy 0.4890\n",
            "Epoch 3 Batch 100 Loss 2.6810 Accuracy 0.4899\n",
            "Epoch 3 Batch 150 Loss 2.6658 Accuracy 0.4919\n",
            "Epoch 3 Batch 200 Loss 2.6541 Accuracy 0.4934\n",
            "Epoch 3 Batch 250 Loss 2.6482 Accuracy 0.4937\n",
            "Epoch 3 Batch 300 Loss 2.6434 Accuracy 0.4939\n",
            "Epoch 3 Batch 350 Loss 2.6424 Accuracy 0.4934\n",
            "Epoch 3 Batch 400 Loss 2.6368 Accuracy 0.4942\n",
            "Epoch 3 Batch 450 Loss 2.6354 Accuracy 0.4942\n",
            "Epoch 3 Batch 500 Loss 2.6241 Accuracy 0.4954\n",
            "Epoch 3 Batch 550 Loss 2.6177 Accuracy 0.4958\n",
            "Epoch 3 Batch 600 Loss 2.6126 Accuracy 0.4960\n",
            "Epoch 3 Batch 650 Loss 2.6045 Accuracy 0.4972\n",
            "Epoch 3 Batch 700 Loss 2.6001 Accuracy 0.4978\n",
            "Epoch 3 Batch 750 Loss 2.5942 Accuracy 0.4985\n",
            "Epoch 3 Batch 800 Loss 2.5876 Accuracy 0.4994\n",
            "Epoch 3 Batch 850 Loss 2.5834 Accuracy 0.4998\n",
            "Epoch 3 Batch 900 Loss 2.5797 Accuracy 0.5002\n",
            "Epoch 3 Batch 950 Loss 2.5747 Accuracy 0.5008\n",
            "Epoch 3 Batch 1000 Loss 2.5712 Accuracy 0.5012\n",
            "Epoch 3 Batch 1050 Loss 2.5664 Accuracy 0.5016\n",
            "Epoch 3 Batch 1100 Loss 2.5625 Accuracy 0.5020\n",
            "Epoch 3 Batch 1150 Loss 2.5578 Accuracy 0.5026\n",
            "Epoch 3 Batch 1200 Loss 2.5530 Accuracy 0.5033\n",
            "Epoch 3 Loss 2.5501 Accuracy 0.5035\n",
            "Time taken for 1 epoch: 127.77 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 2.6212 Accuracy 0.4805\n",
            "Epoch 4 Batch 50 Loss 2.3850 Accuracy 0.5232\n",
            "Epoch 4 Batch 100 Loss 2.3924 Accuracy 0.5191\n",
            "Epoch 4 Batch 150 Loss 2.3800 Accuracy 0.5213\n",
            "Epoch 4 Batch 200 Loss 2.3723 Accuracy 0.5227\n",
            "Epoch 4 Batch 250 Loss 2.3735 Accuracy 0.5224\n",
            "Epoch 4 Batch 300 Loss 2.3690 Accuracy 0.5225\n",
            "Epoch 4 Batch 350 Loss 2.3719 Accuracy 0.5226\n",
            "Epoch 4 Batch 400 Loss 2.3660 Accuracy 0.5230\n",
            "Epoch 4 Batch 450 Loss 2.3649 Accuracy 0.5233\n",
            "Epoch 4 Batch 500 Loss 2.3617 Accuracy 0.5234\n",
            "Epoch 4 Batch 550 Loss 2.3599 Accuracy 0.5236\n",
            "Epoch 4 Batch 600 Loss 2.3551 Accuracy 0.5244\n",
            "Epoch 4 Batch 650 Loss 2.3481 Accuracy 0.5254\n",
            "Epoch 4 Batch 700 Loss 2.3452 Accuracy 0.5257\n",
            "Epoch 4 Batch 750 Loss 2.3411 Accuracy 0.5263\n",
            "Epoch 4 Batch 800 Loss 2.3355 Accuracy 0.5271\n",
            "Epoch 4 Batch 850 Loss 2.3326 Accuracy 0.5276\n",
            "Epoch 4 Batch 900 Loss 2.3282 Accuracy 0.5283\n",
            "Epoch 4 Batch 950 Loss 2.3232 Accuracy 0.5291\n",
            "Epoch 4 Batch 1000 Loss 2.3192 Accuracy 0.5296\n",
            "Epoch 4 Batch 1050 Loss 2.3149 Accuracy 0.5301\n",
            "Epoch 4 Batch 1100 Loss 2.3111 Accuracy 0.5307\n",
            "Epoch 4 Batch 1150 Loss 2.3064 Accuracy 0.5313\n",
            "Epoch 4 Batch 1200 Loss 2.3019 Accuracy 0.5319\n",
            "Epoch 4 Loss 2.2980 Accuracy 0.5326\n",
            "Time taken for 1 epoch: 125.42 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.9960 Accuracy 0.5896\n",
            "Epoch 5 Batch 50 Loss 2.1212 Accuracy 0.5547\n",
            "Epoch 5 Batch 100 Loss 2.1020 Accuracy 0.5582\n",
            "Epoch 5 Batch 150 Loss 2.0970 Accuracy 0.5594\n",
            "Epoch 5 Batch 200 Loss 2.0961 Accuracy 0.5596\n",
            "Epoch 5 Batch 250 Loss 2.0915 Accuracy 0.5608\n",
            "Epoch 5 Batch 300 Loss 2.0845 Accuracy 0.5617\n",
            "Epoch 5 Batch 350 Loss 2.0792 Accuracy 0.5621\n",
            "Epoch 5 Batch 400 Loss 2.0815 Accuracy 0.5616\n",
            "Epoch 5 Batch 450 Loss 2.0787 Accuracy 0.5620\n",
            "Epoch 5 Batch 500 Loss 2.0791 Accuracy 0.5616\n",
            "Epoch 5 Batch 550 Loss 2.0765 Accuracy 0.5621\n",
            "Epoch 5 Batch 600 Loss 2.0750 Accuracy 0.5621\n",
            "Epoch 5 Batch 650 Loss 2.0721 Accuracy 0.5629\n",
            "Epoch 5 Batch 700 Loss 2.0681 Accuracy 0.5637\n",
            "Epoch 5 Batch 750 Loss 2.0655 Accuracy 0.5639\n",
            "Epoch 5 Batch 800 Loss 2.0617 Accuracy 0.5644\n",
            "Epoch 5 Batch 850 Loss 2.0584 Accuracy 0.5651\n",
            "Epoch 5 Batch 900 Loss 2.0551 Accuracy 0.5657\n",
            "Epoch 5 Batch 950 Loss 2.0541 Accuracy 0.5657\n",
            "Epoch 5 Batch 1000 Loss 2.0519 Accuracy 0.5660\n",
            "Epoch 5 Batch 1050 Loss 2.0466 Accuracy 0.5667\n",
            "Epoch 5 Batch 1100 Loss 2.0435 Accuracy 0.5670\n",
            "Epoch 5 Batch 1150 Loss 2.0409 Accuracy 0.5675\n",
            "Epoch 5 Batch 1200 Loss 2.0397 Accuracy 0.5677\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
            "Epoch 5 Loss 2.0369 Accuracy 0.5682\n",
            "Time taken for 1 epoch: 117.77 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.6539 Accuracy 0.6369\n",
            "Epoch 6 Batch 50 Loss 1.8762 Accuracy 0.5915\n",
            "Epoch 6 Batch 100 Loss 1.8698 Accuracy 0.5927\n",
            "Epoch 6 Batch 150 Loss 1.8737 Accuracy 0.5924\n",
            "Epoch 6 Batch 200 Loss 1.8777 Accuracy 0.5919\n",
            "Epoch 6 Batch 250 Loss 1.8784 Accuracy 0.5915\n",
            "Epoch 6 Batch 300 Loss 1.8781 Accuracy 0.5922\n",
            "Epoch 6 Batch 350 Loss 1.8771 Accuracy 0.5926\n",
            "Epoch 6 Batch 400 Loss 1.8788 Accuracy 0.5927\n",
            "Epoch 6 Batch 450 Loss 1.8766 Accuracy 0.5929\n",
            "Epoch 6 Batch 500 Loss 1.8734 Accuracy 0.5934\n",
            "Epoch 6 Batch 550 Loss 1.8724 Accuracy 0.5934\n",
            "Epoch 6 Batch 600 Loss 1.8693 Accuracy 0.5938\n",
            "Epoch 6 Batch 650 Loss 1.8679 Accuracy 0.5940\n",
            "Epoch 6 Batch 700 Loss 1.8647 Accuracy 0.5944\n",
            "Epoch 6 Batch 750 Loss 1.8621 Accuracy 0.5948\n",
            "Epoch 6 Batch 800 Loss 1.8606 Accuracy 0.5948\n",
            "Epoch 6 Batch 850 Loss 1.8576 Accuracy 0.5954\n",
            "Epoch 6 Batch 900 Loss 1.8540 Accuracy 0.5960\n",
            "Epoch 6 Batch 950 Loss 1.8545 Accuracy 0.5960\n",
            "Epoch 6 Batch 1000 Loss 1.8536 Accuracy 0.5962\n",
            "Epoch 6 Batch 1050 Loss 1.8537 Accuracy 0.5961\n",
            "Epoch 6 Batch 1100 Loss 1.8535 Accuracy 0.5962\n",
            "Epoch 6 Batch 1150 Loss 1.8527 Accuracy 0.5963\n",
            "Epoch 6 Batch 1200 Loss 1.8506 Accuracy 0.5966\n",
            "Epoch 6 Loss 1.8504 Accuracy 0.5967\n",
            "Time taken for 1 epoch: 116.68 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.7833 Accuracy 0.5969\n",
            "Epoch 7 Batch 50 Loss 1.7052 Accuracy 0.6172\n",
            "Epoch 7 Batch 100 Loss 1.7202 Accuracy 0.6148\n",
            "Epoch 7 Batch 150 Loss 1.7150 Accuracy 0.6170\n",
            "Epoch 7 Batch 200 Loss 1.7183 Accuracy 0.6168\n",
            "Epoch 7 Batch 250 Loss 1.7127 Accuracy 0.6182\n",
            "Epoch 7 Batch 300 Loss 1.7196 Accuracy 0.6166\n",
            "Epoch 7 Batch 350 Loss 1.7166 Accuracy 0.6173\n",
            "Epoch 7 Batch 400 Loss 1.7168 Accuracy 0.6171\n",
            "Epoch 7 Batch 450 Loss 1.7200 Accuracy 0.6167\n",
            "Epoch 7 Batch 500 Loss 1.7235 Accuracy 0.6160\n",
            "Epoch 7 Batch 550 Loss 1.7256 Accuracy 0.6158\n",
            "Epoch 7 Batch 600 Loss 1.7202 Accuracy 0.6167\n",
            "Epoch 7 Batch 650 Loss 1.7188 Accuracy 0.6171\n",
            "Epoch 7 Batch 700 Loss 1.7190 Accuracy 0.6172\n",
            "Epoch 7 Batch 750 Loss 1.7178 Accuracy 0.6172\n",
            "Epoch 7 Batch 800 Loss 1.7167 Accuracy 0.6172\n",
            "Epoch 7 Batch 850 Loss 1.7135 Accuracy 0.6179\n",
            "Epoch 7 Batch 900 Loss 1.7130 Accuracy 0.6180\n",
            "Epoch 7 Batch 950 Loss 1.7125 Accuracy 0.6180\n",
            "Epoch 7 Batch 1000 Loss 1.7113 Accuracy 0.6181\n",
            "Epoch 7 Batch 1050 Loss 1.7100 Accuracy 0.6183\n",
            "Epoch 7 Batch 1100 Loss 1.7093 Accuracy 0.6184\n",
            "Epoch 7 Batch 1150 Loss 1.7094 Accuracy 0.6185\n",
            "Epoch 7 Batch 1200 Loss 1.7090 Accuracy 0.6186\n",
            "Epoch 7 Loss 1.7073 Accuracy 0.6189\n",
            "Time taken for 1 epoch: 116.49 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.6056 Accuracy 0.6268\n",
            "Epoch 8 Batch 50 Loss 1.5911 Accuracy 0.6386\n",
            "Epoch 8 Batch 100 Loss 1.5950 Accuracy 0.6364\n",
            "Epoch 8 Batch 150 Loss 1.5857 Accuracy 0.6384\n",
            "Epoch 8 Batch 200 Loss 1.5915 Accuracy 0.6376\n",
            "Epoch 8 Batch 250 Loss 1.5896 Accuracy 0.6380\n",
            "Epoch 8 Batch 300 Loss 1.5910 Accuracy 0.6374\n",
            "Epoch 8 Batch 350 Loss 1.5896 Accuracy 0.6373\n",
            "Epoch 8 Batch 400 Loss 1.5948 Accuracy 0.6369\n",
            "Epoch 8 Batch 450 Loss 1.5966 Accuracy 0.6368\n",
            "Epoch 8 Batch 500 Loss 1.5972 Accuracy 0.6367\n",
            "Epoch 8 Batch 550 Loss 1.5982 Accuracy 0.6364\n",
            "Epoch 8 Batch 600 Loss 1.5969 Accuracy 0.6366\n",
            "Epoch 8 Batch 650 Loss 1.5943 Accuracy 0.6370\n",
            "Epoch 8 Batch 700 Loss 1.5965 Accuracy 0.6366\n",
            "Epoch 8 Batch 750 Loss 1.5965 Accuracy 0.6365\n",
            "Epoch 8 Batch 800 Loss 1.5972 Accuracy 0.6365\n",
            "Epoch 8 Batch 850 Loss 1.5946 Accuracy 0.6370\n",
            "Epoch 8 Batch 900 Loss 1.5942 Accuracy 0.6371\n",
            "Epoch 8 Batch 950 Loss 1.5913 Accuracy 0.6376\n",
            "Epoch 8 Batch 1000 Loss 1.5920 Accuracy 0.6377\n",
            "Epoch 8 Batch 1050 Loss 1.5917 Accuracy 0.6377\n",
            "Epoch 8 Batch 1100 Loss 1.5906 Accuracy 0.6378\n",
            "Epoch 8 Batch 1150 Loss 1.5909 Accuracy 0.6379\n",
            "Epoch 8 Batch 1200 Loss 1.5924 Accuracy 0.6377\n",
            "Epoch 8 Loss 1.5916 Accuracy 0.6378\n",
            "Time taken for 1 epoch: 116.78 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.3971 Accuracy 0.6741\n",
            "Epoch 9 Batch 50 Loss 1.4844 Accuracy 0.6535\n",
            "Epoch 9 Batch 100 Loss 1.4835 Accuracy 0.6541\n",
            "Epoch 9 Batch 150 Loss 1.4811 Accuracy 0.6556\n",
            "Epoch 9 Batch 200 Loss 1.4849 Accuracy 0.6553\n",
            "Epoch 9 Batch 250 Loss 1.4889 Accuracy 0.6544\n",
            "Epoch 9 Batch 300 Loss 1.4872 Accuracy 0.6546\n",
            "Epoch 9 Batch 350 Loss 1.4879 Accuracy 0.6551\n",
            "Epoch 9 Batch 400 Loss 1.4912 Accuracy 0.6544\n",
            "Epoch 9 Batch 450 Loss 1.4911 Accuracy 0.6544\n",
            "Epoch 9 Batch 500 Loss 1.4937 Accuracy 0.6540\n",
            "Epoch 9 Batch 550 Loss 1.4966 Accuracy 0.6535\n",
            "Epoch 9 Batch 600 Loss 1.4954 Accuracy 0.6538\n",
            "Epoch 9 Batch 650 Loss 1.4959 Accuracy 0.6541\n",
            "Epoch 9 Batch 700 Loss 1.4935 Accuracy 0.6545\n",
            "Epoch 9 Batch 750 Loss 1.4910 Accuracy 0.6547\n",
            "Epoch 9 Batch 800 Loss 1.4894 Accuracy 0.6549\n",
            "Epoch 9 Batch 850 Loss 1.4896 Accuracy 0.6549\n",
            "Epoch 9 Batch 900 Loss 1.4916 Accuracy 0.6547\n",
            "Epoch 9 Batch 950 Loss 1.4908 Accuracy 0.6548\n",
            "Epoch 9 Batch 1000 Loss 1.4911 Accuracy 0.6547\n",
            "Epoch 9 Batch 1050 Loss 1.4916 Accuracy 0.6547\n",
            "Epoch 9 Batch 1100 Loss 1.4913 Accuracy 0.6546\n",
            "Epoch 9 Batch 1150 Loss 1.4911 Accuracy 0.6547\n",
            "Epoch 9 Batch 1200 Loss 1.4922 Accuracy 0.6545\n",
            "Epoch 9 Loss 1.4925 Accuracy 0.6545\n",
            "Time taken for 1 epoch: 116.36 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.4631 Accuracy 0.6785\n",
            "Epoch 10 Batch 50 Loss 1.4262 Accuracy 0.6645\n",
            "Epoch 10 Batch 100 Loss 1.3980 Accuracy 0.6699\n",
            "Epoch 10 Batch 150 Loss 1.3971 Accuracy 0.6705\n",
            "Epoch 10 Batch 200 Loss 1.3984 Accuracy 0.6705\n",
            "Epoch 10 Batch 250 Loss 1.3974 Accuracy 0.6707\n",
            "Epoch 10 Batch 300 Loss 1.3981 Accuracy 0.6708\n",
            "Epoch 10 Batch 350 Loss 1.3988 Accuracy 0.6704\n",
            "Epoch 10 Batch 400 Loss 1.4017 Accuracy 0.6702\n",
            "Epoch 10 Batch 450 Loss 1.4020 Accuracy 0.6701\n",
            "Epoch 10 Batch 500 Loss 1.4049 Accuracy 0.6694\n",
            "Epoch 10 Batch 550 Loss 1.4069 Accuracy 0.6691\n",
            "Epoch 10 Batch 600 Loss 1.4078 Accuracy 0.6688\n",
            "Epoch 10 Batch 650 Loss 1.4085 Accuracy 0.6686\n",
            "Epoch 10 Batch 700 Loss 1.4085 Accuracy 0.6684\n",
            "Epoch 10 Batch 750 Loss 1.4091 Accuracy 0.6683\n",
            "Epoch 10 Batch 800 Loss 1.4097 Accuracy 0.6684\n",
            "Epoch 10 Batch 850 Loss 1.4079 Accuracy 0.6687\n",
            "Epoch 10 Batch 900 Loss 1.4069 Accuracy 0.6691\n",
            "Epoch 10 Batch 950 Loss 1.4071 Accuracy 0.6689\n",
            "Epoch 10 Batch 1000 Loss 1.4060 Accuracy 0.6691\n",
            "Epoch 10 Batch 1050 Loss 1.4063 Accuracy 0.6691\n",
            "Epoch 10 Batch 1100 Loss 1.4062 Accuracy 0.6692\n",
            "Epoch 10 Batch 1150 Loss 1.4071 Accuracy 0.6691\n",
            "Epoch 10 Batch 1200 Loss 1.4088 Accuracy 0.6688\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n",
            "Epoch 10 Loss 1.4096 Accuracy 0.6687\n",
            "Time taken for 1 epoch: 117.71 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 1.1767 Accuracy 0.7241\n",
            "Epoch 11 Batch 50 Loss 1.3229 Accuracy 0.6809\n",
            "Epoch 11 Batch 100 Loss 1.3208 Accuracy 0.6812\n",
            "Epoch 11 Batch 150 Loss 1.3226 Accuracy 0.6812\n",
            "Epoch 11 Batch 200 Loss 1.3254 Accuracy 0.6814\n",
            "Epoch 11 Batch 250 Loss 1.3201 Accuracy 0.6830\n",
            "Epoch 11 Batch 300 Loss 1.3246 Accuracy 0.6822\n",
            "Epoch 11 Batch 350 Loss 1.3227 Accuracy 0.6824\n",
            "Epoch 11 Batch 400 Loss 1.3246 Accuracy 0.6823\n",
            "Epoch 11 Batch 450 Loss 1.3251 Accuracy 0.6823\n",
            "Epoch 11 Batch 500 Loss 1.3295 Accuracy 0.6818\n",
            "Epoch 11 Batch 550 Loss 1.3302 Accuracy 0.6817\n",
            "Epoch 11 Batch 600 Loss 1.3299 Accuracy 0.6817\n",
            "Epoch 11 Batch 650 Loss 1.3319 Accuracy 0.6816\n",
            "Epoch 11 Batch 700 Loss 1.3317 Accuracy 0.6817\n",
            "Epoch 11 Batch 750 Loss 1.3305 Accuracy 0.6820\n",
            "Epoch 11 Batch 800 Loss 1.3291 Accuracy 0.6823\n",
            "Epoch 11 Batch 850 Loss 1.3288 Accuracy 0.6823\n",
            "Epoch 11 Batch 900 Loss 1.3302 Accuracy 0.6822\n",
            "Epoch 11 Batch 950 Loss 1.3296 Accuracy 0.6824\n",
            "Epoch 11 Batch 1000 Loss 1.3310 Accuracy 0.6820\n",
            "Epoch 11 Batch 1050 Loss 1.3312 Accuracy 0.6819\n",
            "Epoch 11 Batch 1100 Loss 1.3319 Accuracy 0.6819\n",
            "Epoch 11 Batch 1150 Loss 1.3319 Accuracy 0.6820\n",
            "Epoch 11 Batch 1200 Loss 1.3333 Accuracy 0.6817\n",
            "Epoch 11 Loss 1.3334 Accuracy 0.6818\n",
            "Time taken for 1 epoch: 116.42 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 1.2066 Accuracy 0.7130\n",
            "Epoch 12 Batch 50 Loss 1.2216 Accuracy 0.7012\n",
            "Epoch 12 Batch 100 Loss 1.2431 Accuracy 0.6964\n",
            "Epoch 12 Batch 150 Loss 1.2370 Accuracy 0.6983\n",
            "Epoch 12 Batch 200 Loss 1.2381 Accuracy 0.6988\n",
            "Epoch 12 Batch 250 Loss 1.2440 Accuracy 0.6978\n",
            "Epoch 12 Batch 300 Loss 1.2513 Accuracy 0.6967\n",
            "Epoch 12 Batch 350 Loss 1.2542 Accuracy 0.6959\n",
            "Epoch 12 Batch 400 Loss 1.2579 Accuracy 0.6949\n",
            "Epoch 12 Batch 450 Loss 1.2600 Accuracy 0.6944\n",
            "Epoch 12 Batch 500 Loss 1.2565 Accuracy 0.6951\n",
            "Epoch 12 Batch 550 Loss 1.2584 Accuracy 0.6947\n",
            "Epoch 12 Batch 600 Loss 1.2600 Accuracy 0.6945\n",
            "Epoch 12 Batch 650 Loss 1.2602 Accuracy 0.6945\n",
            "Epoch 12 Batch 700 Loss 1.2617 Accuracy 0.6945\n",
            "Epoch 12 Batch 750 Loss 1.2636 Accuracy 0.6943\n",
            "Epoch 12 Batch 800 Loss 1.2648 Accuracy 0.6941\n",
            "Epoch 12 Batch 850 Loss 1.2647 Accuracy 0.6942\n",
            "Epoch 12 Batch 900 Loss 1.2637 Accuracy 0.6944\n",
            "Epoch 12 Batch 950 Loss 1.2640 Accuracy 0.6945\n",
            "Epoch 12 Batch 1000 Loss 1.2657 Accuracy 0.6942\n",
            "Epoch 12 Batch 1050 Loss 1.2661 Accuracy 0.6940\n",
            "Epoch 12 Batch 1100 Loss 1.2667 Accuracy 0.6940\n",
            "Epoch 12 Batch 1150 Loss 1.2672 Accuracy 0.6939\n",
            "Epoch 12 Batch 1200 Loss 1.2682 Accuracy 0.6938\n",
            "Epoch 12 Loss 1.2692 Accuracy 0.6936\n",
            "Time taken for 1 epoch: 116.63 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 1.4275 Accuracy 0.6630\n",
            "Epoch 13 Batch 50 Loss 1.1929 Accuracy 0.7057\n",
            "Epoch 13 Batch 100 Loss 1.1773 Accuracy 0.7085\n",
            "Epoch 13 Batch 150 Loss 1.1799 Accuracy 0.7092\n",
            "Epoch 13 Batch 200 Loss 1.1829 Accuracy 0.7098\n",
            "Epoch 13 Batch 250 Loss 1.1804 Accuracy 0.7100\n",
            "Epoch 13 Batch 300 Loss 1.1884 Accuracy 0.7083\n",
            "Epoch 13 Batch 350 Loss 1.1907 Accuracy 0.7077\n",
            "Epoch 13 Batch 400 Loss 1.1913 Accuracy 0.7075\n",
            "Epoch 13 Batch 450 Loss 1.1921 Accuracy 0.7072\n",
            "Epoch 13 Batch 500 Loss 1.1927 Accuracy 0.7070\n",
            "Epoch 13 Batch 550 Loss 1.1961 Accuracy 0.7062\n",
            "Epoch 13 Batch 600 Loss 1.1976 Accuracy 0.7059\n",
            "Epoch 13 Batch 650 Loss 1.1986 Accuracy 0.7057\n",
            "Epoch 13 Batch 700 Loss 1.2000 Accuracy 0.7054\n",
            "Epoch 13 Batch 750 Loss 1.2009 Accuracy 0.7054\n",
            "Epoch 13 Batch 800 Loss 1.2014 Accuracy 0.7056\n",
            "Epoch 13 Batch 850 Loss 1.2017 Accuracy 0.7058\n",
            "Epoch 13 Batch 900 Loss 1.2008 Accuracy 0.7061\n",
            "Epoch 13 Batch 950 Loss 1.2027 Accuracy 0.7056\n",
            "Epoch 13 Batch 1000 Loss 1.2037 Accuracy 0.7055\n",
            "Epoch 13 Batch 1050 Loss 1.2035 Accuracy 0.7054\n",
            "Epoch 13 Batch 1100 Loss 1.2041 Accuracy 0.7053\n",
            "Epoch 13 Batch 1150 Loss 1.2064 Accuracy 0.7047\n",
            "Epoch 13 Batch 1200 Loss 1.2073 Accuracy 0.7045\n",
            "Epoch 13 Loss 1.2078 Accuracy 0.7046\n",
            "Time taken for 1 epoch: 116.39 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.1979 Accuracy 0.7048\n",
            "Epoch 14 Batch 50 Loss 1.1454 Accuracy 0.7152\n",
            "Epoch 14 Batch 100 Loss 1.1453 Accuracy 0.7161\n",
            "Epoch 14 Batch 150 Loss 1.1290 Accuracy 0.7190\n",
            "Epoch 14 Batch 200 Loss 1.1272 Accuracy 0.7198\n",
            "Epoch 14 Batch 250 Loss 1.1232 Accuracy 0.7203\n",
            "Epoch 14 Batch 300 Loss 1.1297 Accuracy 0.7191\n",
            "Epoch 14 Batch 350 Loss 1.1296 Accuracy 0.7192\n",
            "Epoch 14 Batch 400 Loss 1.1341 Accuracy 0.7185\n",
            "Epoch 14 Batch 450 Loss 1.1371 Accuracy 0.7179\n",
            "Epoch 14 Batch 500 Loss 1.1369 Accuracy 0.7181\n",
            "Epoch 14 Batch 550 Loss 1.1413 Accuracy 0.7170\n",
            "Epoch 14 Batch 600 Loss 1.1410 Accuracy 0.7171\n",
            "Epoch 14 Batch 650 Loss 1.1428 Accuracy 0.7170\n",
            "Epoch 14 Batch 700 Loss 1.1438 Accuracy 0.7169\n",
            "Epoch 14 Batch 750 Loss 1.1464 Accuracy 0.7163\n",
            "Epoch 14 Batch 800 Loss 1.1473 Accuracy 0.7163\n",
            "Epoch 14 Batch 850 Loss 1.1485 Accuracy 0.7160\n",
            "Epoch 14 Batch 900 Loss 1.1492 Accuracy 0.7158\n",
            "Epoch 14 Batch 950 Loss 1.1498 Accuracy 0.7157\n",
            "Epoch 14 Batch 1000 Loss 1.1502 Accuracy 0.7157\n",
            "Epoch 14 Batch 1050 Loss 1.1503 Accuracy 0.7156\n",
            "Epoch 14 Batch 1100 Loss 1.1511 Accuracy 0.7155\n",
            "Epoch 14 Batch 1150 Loss 1.1527 Accuracy 0.7153\n",
            "Epoch 14 Batch 1200 Loss 1.1539 Accuracy 0.7151\n",
            "Epoch 14 Loss 1.1543 Accuracy 0.7150\n",
            "Time taken for 1 epoch: 116.32 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.1871 Accuracy 0.6902\n",
            "Epoch 15 Batch 50 Loss 1.0679 Accuracy 0.7309\n",
            "Epoch 15 Batch 100 Loss 1.0664 Accuracy 0.7299\n",
            "Epoch 15 Batch 150 Loss 1.0681 Accuracy 0.7304\n",
            "Epoch 15 Batch 200 Loss 1.0719 Accuracy 0.7295\n",
            "Epoch 15 Batch 250 Loss 1.0786 Accuracy 0.7284\n",
            "Epoch 15 Batch 300 Loss 1.0770 Accuracy 0.7289\n",
            "Epoch 15 Batch 350 Loss 1.0795 Accuracy 0.7285\n",
            "Epoch 15 Batch 400 Loss 1.0801 Accuracy 0.7283\n",
            "Epoch 15 Batch 450 Loss 1.0839 Accuracy 0.7276\n",
            "Epoch 15 Batch 500 Loss 1.0875 Accuracy 0.7268\n",
            "Epoch 15 Batch 550 Loss 1.0881 Accuracy 0.7269\n",
            "Epoch 15 Batch 600 Loss 1.0904 Accuracy 0.7264\n",
            "Epoch 15 Batch 650 Loss 1.0910 Accuracy 0.7263\n",
            "Epoch 15 Batch 700 Loss 1.0924 Accuracy 0.7261\n",
            "Epoch 15 Batch 750 Loss 1.0935 Accuracy 0.7258\n",
            "Epoch 15 Batch 800 Loss 1.0948 Accuracy 0.7255\n",
            "Epoch 15 Batch 850 Loss 1.0964 Accuracy 0.7252\n",
            "Epoch 15 Batch 900 Loss 1.0997 Accuracy 0.7244\n",
            "Epoch 15 Batch 950 Loss 1.1004 Accuracy 0.7243\n",
            "Epoch 15 Batch 1000 Loss 1.1018 Accuracy 0.7239\n",
            "Epoch 15 Batch 1050 Loss 1.1011 Accuracy 0.7241\n",
            "Epoch 15 Batch 1100 Loss 1.1017 Accuracy 0.7240\n",
            "Epoch 15 Batch 1150 Loss 1.1020 Accuracy 0.7240\n",
            "Epoch 15 Batch 1200 Loss 1.1037 Accuracy 0.7237\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\n",
            "Epoch 15 Loss 1.1049 Accuracy 0.7236\n",
            "Time taken for 1 epoch: 117.48 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.0276 Accuracy 0.7346\n",
            "Epoch 16 Batch 50 Loss 1.0200 Accuracy 0.7414\n",
            "Epoch 16 Batch 100 Loss 1.0254 Accuracy 0.7411\n",
            "Epoch 16 Batch 150 Loss 1.0330 Accuracy 0.7382\n",
            "Epoch 16 Batch 200 Loss 1.0312 Accuracy 0.7378\n",
            "Epoch 16 Batch 250 Loss 1.0288 Accuracy 0.7383\n",
            "Epoch 16 Batch 300 Loss 1.0338 Accuracy 0.7369\n",
            "Epoch 16 Batch 350 Loss 1.0373 Accuracy 0.7361\n",
            "Epoch 16 Batch 400 Loss 1.0388 Accuracy 0.7358\n",
            "Epoch 16 Batch 450 Loss 1.0419 Accuracy 0.7351\n",
            "Epoch 16 Batch 500 Loss 1.0445 Accuracy 0.7343\n",
            "Epoch 16 Batch 550 Loss 1.0465 Accuracy 0.7340\n",
            "Epoch 16 Batch 600 Loss 1.0488 Accuracy 0.7334\n",
            "Epoch 16 Batch 650 Loss 1.0482 Accuracy 0.7336\n",
            "Epoch 16 Batch 700 Loss 1.0486 Accuracy 0.7335\n",
            "Epoch 16 Batch 750 Loss 1.0500 Accuracy 0.7335\n",
            "Epoch 16 Batch 800 Loss 1.0506 Accuracy 0.7336\n",
            "Epoch 16 Batch 850 Loss 1.0520 Accuracy 0.7336\n",
            "Epoch 16 Batch 900 Loss 1.0522 Accuracy 0.7336\n",
            "Epoch 16 Batch 950 Loss 1.0529 Accuracy 0.7334\n",
            "Epoch 16 Batch 1000 Loss 1.0550 Accuracy 0.7331\n",
            "Epoch 16 Batch 1050 Loss 1.0565 Accuracy 0.7329\n",
            "Epoch 16 Batch 1100 Loss 1.0584 Accuracy 0.7326\n",
            "Epoch 16 Batch 1150 Loss 1.0592 Accuracy 0.7324\n",
            "Epoch 16 Batch 1200 Loss 1.0604 Accuracy 0.7322\n",
            "Epoch 16 Loss 1.0620 Accuracy 0.7319\n",
            "Time taken for 1 epoch: 116.67 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.1778 Accuracy 0.7141\n",
            "Epoch 17 Batch 50 Loss 0.9860 Accuracy 0.7448\n",
            "Epoch 17 Batch 100 Loss 0.9864 Accuracy 0.7445\n",
            "Epoch 17 Batch 150 Loss 0.9857 Accuracy 0.7456\n",
            "Epoch 17 Batch 200 Loss 0.9808 Accuracy 0.7472\n",
            "Epoch 17 Batch 250 Loss 0.9856 Accuracy 0.7468\n",
            "Epoch 17 Batch 300 Loss 0.9890 Accuracy 0.7459\n",
            "Epoch 17 Batch 350 Loss 0.9920 Accuracy 0.7453\n",
            "Epoch 17 Batch 400 Loss 0.9956 Accuracy 0.7445\n",
            "Epoch 17 Batch 450 Loss 0.9977 Accuracy 0.7442\n",
            "Epoch 17 Batch 500 Loss 0.9997 Accuracy 0.7437\n",
            "Epoch 17 Batch 550 Loss 1.0001 Accuracy 0.7436\n",
            "Epoch 17 Batch 600 Loss 1.0045 Accuracy 0.7428\n",
            "Epoch 17 Batch 650 Loss 1.0073 Accuracy 0.7423\n",
            "Epoch 17 Batch 700 Loss 1.0072 Accuracy 0.7423\n",
            "Epoch 17 Batch 750 Loss 1.0078 Accuracy 0.7421\n",
            "Epoch 17 Batch 800 Loss 1.0102 Accuracy 0.7416\n",
            "Epoch 17 Batch 850 Loss 1.0108 Accuracy 0.7416\n",
            "Epoch 17 Batch 900 Loss 1.0112 Accuracy 0.7414\n",
            "Epoch 17 Batch 950 Loss 1.0125 Accuracy 0.7411\n",
            "Epoch 17 Batch 1000 Loss 1.0129 Accuracy 0.7410\n",
            "Epoch 17 Batch 1050 Loss 1.0142 Accuracy 0.7408\n",
            "Epoch 17 Batch 1100 Loss 1.0155 Accuracy 0.7405\n",
            "Epoch 17 Batch 1150 Loss 1.0169 Accuracy 0.7403\n",
            "Epoch 17 Batch 1200 Loss 1.0177 Accuracy 0.7401\n",
            "Epoch 17 Loss 1.0194 Accuracy 0.7399\n",
            "Time taken for 1 epoch: 116.92 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.9319 Accuracy 0.7436\n",
            "Epoch 18 Batch 50 Loss 0.9629 Accuracy 0.7492\n",
            "Epoch 18 Batch 100 Loss 0.9455 Accuracy 0.7524\n",
            "Epoch 18 Batch 150 Loss 0.9525 Accuracy 0.7513\n",
            "Epoch 18 Batch 200 Loss 0.9531 Accuracy 0.7516\n",
            "Epoch 18 Batch 250 Loss 0.9554 Accuracy 0.7514\n",
            "Epoch 18 Batch 300 Loss 0.9556 Accuracy 0.7516\n",
            "Epoch 18 Batch 350 Loss 0.9609 Accuracy 0.7506\n",
            "Epoch 18 Batch 400 Loss 0.9639 Accuracy 0.7498\n",
            "Epoch 18 Batch 450 Loss 0.9658 Accuracy 0.7497\n",
            "Epoch 18 Batch 500 Loss 0.9663 Accuracy 0.7495\n",
            "Epoch 18 Batch 550 Loss 0.9687 Accuracy 0.7491\n",
            "Epoch 18 Batch 600 Loss 0.9697 Accuracy 0.7489\n",
            "Epoch 18 Batch 650 Loss 0.9708 Accuracy 0.7486\n",
            "Epoch 18 Batch 700 Loss 0.9718 Accuracy 0.7485\n",
            "Epoch 18 Batch 750 Loss 0.9731 Accuracy 0.7482\n",
            "Epoch 18 Batch 800 Loss 0.9741 Accuracy 0.7480\n",
            "Epoch 18 Batch 850 Loss 0.9744 Accuracy 0.7479\n",
            "Epoch 18 Batch 900 Loss 0.9736 Accuracy 0.7480\n",
            "Epoch 18 Batch 950 Loss 0.9735 Accuracy 0.7479\n",
            "Epoch 18 Batch 1000 Loss 0.9738 Accuracy 0.7480\n",
            "Epoch 18 Batch 1050 Loss 0.9759 Accuracy 0.7478\n",
            "Epoch 18 Batch 1100 Loss 0.9767 Accuracy 0.7476\n",
            "Epoch 18 Batch 1150 Loss 0.9771 Accuracy 0.7476\n",
            "Epoch 18 Batch 1200 Loss 0.9777 Accuracy 0.7475\n",
            "Epoch 18 Loss 0.9798 Accuracy 0.7472\n",
            "Time taken for 1 epoch: 116.47 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.9053 Accuracy 0.7689\n",
            "Epoch 19 Batch 50 Loss 0.9179 Accuracy 0.7614\n",
            "Epoch 19 Batch 100 Loss 0.9115 Accuracy 0.7619\n",
            "Epoch 19 Batch 150 Loss 0.9120 Accuracy 0.7613\n",
            "Epoch 19 Batch 200 Loss 0.9169 Accuracy 0.7606\n",
            "Epoch 19 Batch 250 Loss 0.9174 Accuracy 0.7602\n",
            "Epoch 19 Batch 300 Loss 0.9181 Accuracy 0.7600\n",
            "Epoch 19 Batch 350 Loss 0.9220 Accuracy 0.7591\n",
            "Epoch 19 Batch 400 Loss 0.9251 Accuracy 0.7586\n",
            "Epoch 19 Batch 450 Loss 0.9271 Accuracy 0.7583\n",
            "Epoch 19 Batch 500 Loss 0.9286 Accuracy 0.7582\n",
            "Epoch 19 Batch 550 Loss 0.9304 Accuracy 0.7579\n",
            "Epoch 19 Batch 600 Loss 0.9313 Accuracy 0.7576\n",
            "Epoch 19 Batch 650 Loss 0.9333 Accuracy 0.7571\n",
            "Epoch 19 Batch 700 Loss 0.9341 Accuracy 0.7571\n",
            "Epoch 19 Batch 750 Loss 0.9361 Accuracy 0.7567\n",
            "Epoch 19 Batch 800 Loss 0.9355 Accuracy 0.7568\n",
            "Epoch 19 Batch 850 Loss 0.9360 Accuracy 0.7567\n",
            "Epoch 19 Batch 900 Loss 0.9378 Accuracy 0.7563\n",
            "Epoch 19 Batch 950 Loss 0.9367 Accuracy 0.7564\n",
            "Epoch 19 Batch 1000 Loss 0.9382 Accuracy 0.7561\n",
            "Epoch 19 Batch 1050 Loss 0.9402 Accuracy 0.7558\n",
            "Epoch 19 Batch 1100 Loss 0.9416 Accuracy 0.7554\n",
            "Epoch 19 Batch 1150 Loss 0.9433 Accuracy 0.7551\n",
            "Epoch 19 Batch 1200 Loss 0.9446 Accuracy 0.7548\n",
            "Epoch 19 Loss 0.9456 Accuracy 0.7546\n",
            "Time taken for 1 epoch: 116.68 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.0164 Accuracy 0.7293\n",
            "Epoch 20 Batch 50 Loss 0.8805 Accuracy 0.7676\n",
            "Epoch 20 Batch 100 Loss 0.8679 Accuracy 0.7707\n",
            "Epoch 20 Batch 150 Loss 0.8743 Accuracy 0.7690\n",
            "Epoch 20 Batch 200 Loss 0.8772 Accuracy 0.7686\n",
            "Epoch 20 Batch 250 Loss 0.8837 Accuracy 0.7675\n",
            "Epoch 20 Batch 300 Loss 0.8861 Accuracy 0.7669\n",
            "Epoch 20 Batch 350 Loss 0.8908 Accuracy 0.7662\n",
            "Epoch 20 Batch 400 Loss 0.8932 Accuracy 0.7654\n",
            "Epoch 20 Batch 450 Loss 0.8960 Accuracy 0.7644\n",
            "Epoch 20 Batch 500 Loss 0.8970 Accuracy 0.7641\n",
            "Epoch 20 Batch 550 Loss 0.8973 Accuracy 0.7637\n",
            "Epoch 20 Batch 600 Loss 0.9000 Accuracy 0.7634\n",
            "Epoch 20 Batch 650 Loss 0.9024 Accuracy 0.7628\n",
            "Epoch 20 Batch 700 Loss 0.9032 Accuracy 0.7626\n",
            "Epoch 20 Batch 750 Loss 0.9023 Accuracy 0.7628\n",
            "Epoch 20 Batch 800 Loss 0.9022 Accuracy 0.7629\n",
            "Epoch 20 Batch 850 Loss 0.9026 Accuracy 0.7627\n",
            "Epoch 20 Batch 900 Loss 0.9025 Accuracy 0.7628\n",
            "Epoch 20 Batch 950 Loss 0.9034 Accuracy 0.7627\n",
            "Epoch 20 Batch 1000 Loss 0.9054 Accuracy 0.7622\n",
            "Epoch 20 Batch 1050 Loss 0.9065 Accuracy 0.7621\n",
            "Epoch 20 Batch 1100 Loss 0.9084 Accuracy 0.7617\n",
            "Epoch 20 Batch 1150 Loss 0.9094 Accuracy 0.7614\n",
            "Epoch 20 Batch 1200 Loss 0.9114 Accuracy 0.7610\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\n",
            "Epoch 20 Loss 0.9135 Accuracy 0.7606\n",
            "Time taken for 1 epoch: 117.39 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.9810 Accuracy 0.7442\n",
            "Epoch 21 Batch 50 Loss 0.8413 Accuracy 0.7772\n",
            "Epoch 21 Batch 100 Loss 0.8459 Accuracy 0.7750\n",
            "Epoch 21 Batch 150 Loss 0.8445 Accuracy 0.7756\n",
            "Epoch 21 Batch 200 Loss 0.8448 Accuracy 0.7755\n",
            "Epoch 21 Batch 250 Loss 0.8509 Accuracy 0.7740\n",
            "Epoch 21 Batch 300 Loss 0.8555 Accuracy 0.7733\n",
            "Epoch 21 Batch 350 Loss 0.8588 Accuracy 0.7724\n",
            "Epoch 21 Batch 400 Loss 0.8621 Accuracy 0.7716\n",
            "Epoch 21 Batch 450 Loss 0.8630 Accuracy 0.7714\n",
            "Epoch 21 Batch 500 Loss 0.8629 Accuracy 0.7714\n",
            "Epoch 21 Batch 550 Loss 0.8648 Accuracy 0.7710\n",
            "Epoch 21 Batch 600 Loss 0.8673 Accuracy 0.7706\n",
            "Epoch 21 Batch 650 Loss 0.8690 Accuracy 0.7702\n",
            "Epoch 21 Batch 700 Loss 0.8689 Accuracy 0.7702\n",
            "Epoch 21 Batch 750 Loss 0.8715 Accuracy 0.7696\n",
            "Epoch 21 Batch 800 Loss 0.8724 Accuracy 0.7692\n",
            "Epoch 21 Batch 850 Loss 0.8733 Accuracy 0.7690\n",
            "Epoch 21 Batch 900 Loss 0.8745 Accuracy 0.7687\n",
            "Epoch 21 Batch 950 Loss 0.8751 Accuracy 0.7685\n",
            "Epoch 21 Batch 1000 Loss 0.8764 Accuracy 0.7683\n",
            "Epoch 21 Batch 1050 Loss 0.8773 Accuracy 0.7682\n",
            "Epoch 21 Batch 1100 Loss 0.8788 Accuracy 0.7678\n",
            "Epoch 21 Batch 1150 Loss 0.8801 Accuracy 0.7676\n",
            "Epoch 21 Batch 1200 Loss 0.8808 Accuracy 0.7674\n",
            "Epoch 21 Loss 0.8821 Accuracy 0.7671\n",
            "Time taken for 1 epoch: 116.59 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.9002 Accuracy 0.7685\n",
            "Epoch 22 Batch 50 Loss 0.7923 Accuracy 0.7845\n",
            "Epoch 22 Batch 100 Loss 0.8005 Accuracy 0.7838\n",
            "Epoch 22 Batch 150 Loss 0.8093 Accuracy 0.7825\n",
            "Epoch 22 Batch 200 Loss 0.8155 Accuracy 0.7809\n",
            "Epoch 22 Batch 250 Loss 0.8187 Accuracy 0.7803\n",
            "Epoch 22 Batch 300 Loss 0.8252 Accuracy 0.7790\n",
            "Epoch 22 Batch 350 Loss 0.8270 Accuracy 0.7788\n",
            "Epoch 22 Batch 400 Loss 0.8313 Accuracy 0.7780\n",
            "Epoch 22 Batch 450 Loss 0.8325 Accuracy 0.7776\n",
            "Epoch 22 Batch 500 Loss 0.8343 Accuracy 0.7772\n",
            "Epoch 22 Batch 550 Loss 0.8353 Accuracy 0.7770\n",
            "Epoch 22 Batch 600 Loss 0.8357 Accuracy 0.7767\n",
            "Epoch 22 Batch 650 Loss 0.8371 Accuracy 0.7763\n",
            "Epoch 22 Batch 700 Loss 0.8396 Accuracy 0.7757\n",
            "Epoch 22 Batch 750 Loss 0.8407 Accuracy 0.7755\n",
            "Epoch 22 Batch 800 Loss 0.8416 Accuracy 0.7753\n",
            "Epoch 22 Batch 850 Loss 0.8431 Accuracy 0.7751\n",
            "Epoch 22 Batch 900 Loss 0.8430 Accuracy 0.7752\n",
            "Epoch 22 Batch 950 Loss 0.8450 Accuracy 0.7748\n",
            "Epoch 22 Batch 1000 Loss 0.8467 Accuracy 0.7744\n",
            "Epoch 22 Batch 1050 Loss 0.8479 Accuracy 0.7742\n",
            "Epoch 22 Batch 1100 Loss 0.8500 Accuracy 0.7738\n",
            "Epoch 22 Batch 1150 Loss 0.8510 Accuracy 0.7736\n",
            "Epoch 22 Batch 1200 Loss 0.8519 Accuracy 0.7734\n",
            "Epoch 22 Loss 0.8531 Accuracy 0.7732\n",
            "Time taken for 1 epoch: 116.29 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.7689 Accuracy 0.7948\n",
            "Epoch 23 Batch 50 Loss 0.7792 Accuracy 0.7877\n",
            "Epoch 23 Batch 100 Loss 0.7835 Accuracy 0.7876\n",
            "Epoch 23 Batch 150 Loss 0.7880 Accuracy 0.7861\n",
            "Epoch 23 Batch 200 Loss 0.7937 Accuracy 0.7855\n",
            "Epoch 23 Batch 250 Loss 0.7964 Accuracy 0.7846\n",
            "Epoch 23 Batch 300 Loss 0.7967 Accuracy 0.7845\n",
            "Epoch 23 Batch 350 Loss 0.8012 Accuracy 0.7836\n",
            "Epoch 23 Batch 400 Loss 0.8033 Accuracy 0.7832\n",
            "Epoch 23 Batch 450 Loss 0.8048 Accuracy 0.7830\n",
            "Epoch 23 Batch 500 Loss 0.8065 Accuracy 0.7827\n",
            "Epoch 23 Batch 550 Loss 0.8103 Accuracy 0.7819\n",
            "Epoch 23 Batch 600 Loss 0.8107 Accuracy 0.7817\n",
            "Epoch 23 Batch 650 Loss 0.8132 Accuracy 0.7811\n",
            "Epoch 23 Batch 700 Loss 0.8141 Accuracy 0.7809\n",
            "Epoch 23 Batch 750 Loss 0.8152 Accuracy 0.7807\n",
            "Epoch 23 Batch 800 Loss 0.8152 Accuracy 0.7805\n",
            "Epoch 23 Batch 850 Loss 0.8163 Accuracy 0.7803\n",
            "Epoch 23 Batch 900 Loss 0.8170 Accuracy 0.7801\n",
            "Epoch 23 Batch 950 Loss 0.8179 Accuracy 0.7801\n",
            "Epoch 23 Batch 1000 Loss 0.8189 Accuracy 0.7799\n",
            "Epoch 23 Batch 1050 Loss 0.8213 Accuracy 0.7795\n",
            "Epoch 23 Batch 1100 Loss 0.8225 Accuracy 0.7792\n",
            "Epoch 23 Batch 1150 Loss 0.8244 Accuracy 0.7789\n",
            "Epoch 23 Batch 1200 Loss 0.8257 Accuracy 0.7787\n",
            "Epoch 23 Loss 0.8266 Accuracy 0.7785\n",
            "Time taken for 1 epoch: 116.30 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.7940 Accuracy 0.7671\n",
            "Epoch 24 Batch 50 Loss 0.7570 Accuracy 0.7932\n",
            "Epoch 24 Batch 100 Loss 0.7659 Accuracy 0.7912\n",
            "Epoch 24 Batch 150 Loss 0.7703 Accuracy 0.7903\n",
            "Epoch 24 Batch 200 Loss 0.7761 Accuracy 0.7892\n",
            "Epoch 24 Batch 250 Loss 0.7768 Accuracy 0.7892\n",
            "Epoch 24 Batch 300 Loss 0.7763 Accuracy 0.7893\n",
            "Epoch 24 Batch 350 Loss 0.7778 Accuracy 0.7889\n",
            "Epoch 24 Batch 400 Loss 0.7786 Accuracy 0.7888\n",
            "Epoch 24 Batch 450 Loss 0.7810 Accuracy 0.7883\n",
            "Epoch 24 Batch 500 Loss 0.7836 Accuracy 0.7878\n",
            "Epoch 24 Batch 550 Loss 0.7871 Accuracy 0.7871\n",
            "Epoch 24 Batch 600 Loss 0.7872 Accuracy 0.7872\n",
            "Epoch 24 Batch 650 Loss 0.7867 Accuracy 0.7874\n",
            "Epoch 24 Batch 700 Loss 0.7881 Accuracy 0.7872\n",
            "Epoch 24 Batch 750 Loss 0.7892 Accuracy 0.7869\n",
            "Epoch 24 Batch 800 Loss 0.7904 Accuracy 0.7867\n",
            "Epoch 24 Batch 850 Loss 0.7903 Accuracy 0.7866\n",
            "Epoch 24 Batch 900 Loss 0.7919 Accuracy 0.7863\n",
            "Epoch 24 Batch 950 Loss 0.7931 Accuracy 0.7861\n",
            "Epoch 24 Batch 1000 Loss 0.7939 Accuracy 0.7860\n",
            "Epoch 24 Batch 1050 Loss 0.7960 Accuracy 0.7855\n",
            "Epoch 24 Batch 1100 Loss 0.7973 Accuracy 0.7851\n",
            "Epoch 24 Batch 1150 Loss 0.7980 Accuracy 0.7849\n",
            "Epoch 24 Batch 1200 Loss 0.7994 Accuracy 0.7846\n",
            "Epoch 24 Loss 0.8015 Accuracy 0.7842\n",
            "Time taken for 1 epoch: 116.15 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.6299 Accuracy 0.8243\n",
            "Epoch 25 Batch 50 Loss 0.7528 Accuracy 0.7957\n",
            "Epoch 25 Batch 100 Loss 0.7430 Accuracy 0.7980\n",
            "Epoch 25 Batch 150 Loss 0.7431 Accuracy 0.7979\n",
            "Epoch 25 Batch 200 Loss 0.7439 Accuracy 0.7977\n",
            "Epoch 25 Batch 250 Loss 0.7491 Accuracy 0.7962\n",
            "Epoch 25 Batch 300 Loss 0.7502 Accuracy 0.7961\n",
            "Epoch 25 Batch 350 Loss 0.7500 Accuracy 0.7959\n",
            "Epoch 25 Batch 400 Loss 0.7531 Accuracy 0.7953\n",
            "Epoch 25 Batch 450 Loss 0.7568 Accuracy 0.7946\n",
            "Epoch 25 Batch 500 Loss 0.7600 Accuracy 0.7936\n",
            "Epoch 25 Batch 550 Loss 0.7631 Accuracy 0.7928\n",
            "Epoch 25 Batch 600 Loss 0.7643 Accuracy 0.7925\n",
            "Epoch 25 Batch 650 Loss 0.7649 Accuracy 0.7923\n",
            "Epoch 25 Batch 700 Loss 0.7670 Accuracy 0.7918\n",
            "Epoch 25 Batch 750 Loss 0.7674 Accuracy 0.7917\n",
            "Epoch 25 Batch 800 Loss 0.7670 Accuracy 0.7917\n",
            "Epoch 25 Batch 850 Loss 0.7677 Accuracy 0.7916\n",
            "Epoch 25 Batch 900 Loss 0.7694 Accuracy 0.7910\n",
            "Epoch 25 Batch 950 Loss 0.7702 Accuracy 0.7908\n",
            "Epoch 25 Batch 1000 Loss 0.7709 Accuracy 0.7905\n",
            "Epoch 25 Batch 1050 Loss 0.7725 Accuracy 0.7901\n",
            "Epoch 25 Batch 1100 Loss 0.7736 Accuracy 0.7899\n",
            "Epoch 25 Batch 1150 Loss 0.7750 Accuracy 0.7896\n",
            "Epoch 25 Batch 1200 Loss 0.7768 Accuracy 0.7891\n",
            "Saving checkpoint for epoch 25 at ./checkpoints/train/ckpt-5\n",
            "Epoch 25 Loss 0.7780 Accuracy 0.7889\n",
            "Time taken for 1 epoch: 117.18 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.7385 Accuracy 0.7831\n",
            "Epoch 26 Batch 50 Loss 0.7247 Accuracy 0.8008\n",
            "Epoch 26 Batch 100 Loss 0.7168 Accuracy 0.8026\n",
            "Epoch 26 Batch 150 Loss 0.7224 Accuracy 0.8011\n",
            "Epoch 26 Batch 200 Loss 0.7264 Accuracy 0.8003\n",
            "Epoch 26 Batch 250 Loss 0.7270 Accuracy 0.8001\n",
            "Epoch 26 Batch 300 Loss 0.7280 Accuracy 0.8005\n",
            "Epoch 26 Batch 350 Loss 0.7303 Accuracy 0.7998\n",
            "Epoch 26 Batch 400 Loss 0.7348 Accuracy 0.7989\n",
            "Epoch 26 Batch 450 Loss 0.7367 Accuracy 0.7984\n",
            "Epoch 26 Batch 500 Loss 0.7395 Accuracy 0.7978\n",
            "Epoch 26 Batch 550 Loss 0.7415 Accuracy 0.7974\n",
            "Epoch 26 Batch 600 Loss 0.7425 Accuracy 0.7972\n",
            "Epoch 26 Batch 650 Loss 0.7427 Accuracy 0.7972\n",
            "Epoch 26 Batch 700 Loss 0.7446 Accuracy 0.7967\n",
            "Epoch 26 Batch 750 Loss 0.7450 Accuracy 0.7965\n",
            "Epoch 26 Batch 800 Loss 0.7465 Accuracy 0.7961\n",
            "Epoch 26 Batch 850 Loss 0.7475 Accuracy 0.7959\n",
            "Epoch 26 Batch 900 Loss 0.7489 Accuracy 0.7956\n",
            "Epoch 26 Batch 950 Loss 0.7500 Accuracy 0.7954\n",
            "Epoch 26 Batch 1000 Loss 0.7501 Accuracy 0.7953\n",
            "Epoch 26 Batch 1050 Loss 0.7506 Accuracy 0.7951\n",
            "Epoch 26 Batch 1100 Loss 0.7523 Accuracy 0.7948\n",
            "Epoch 26 Batch 1150 Loss 0.7535 Accuracy 0.7945\n",
            "Epoch 26 Batch 1200 Loss 0.7549 Accuracy 0.7941\n",
            "Epoch 26 Loss 0.7559 Accuracy 0.7939\n",
            "Time taken for 1 epoch: 116.29 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.7021 Accuracy 0.7982\n",
            "Epoch 27 Batch 50 Loss 0.7091 Accuracy 0.8056\n",
            "Epoch 27 Batch 100 Loss 0.7018 Accuracy 0.8063\n",
            "Epoch 27 Batch 150 Loss 0.7049 Accuracy 0.8053\n",
            "Epoch 27 Batch 200 Loss 0.7027 Accuracy 0.8058\n",
            "Epoch 27 Batch 250 Loss 0.7042 Accuracy 0.8056\n",
            "Epoch 27 Batch 300 Loss 0.7096 Accuracy 0.8047\n",
            "Epoch 27 Batch 350 Loss 0.7112 Accuracy 0.8039\n",
            "Epoch 27 Batch 400 Loss 0.7123 Accuracy 0.8038\n",
            "Epoch 27 Batch 450 Loss 0.7141 Accuracy 0.8035\n",
            "Epoch 27 Batch 500 Loss 0.7168 Accuracy 0.8026\n",
            "Epoch 27 Batch 550 Loss 0.7174 Accuracy 0.8023\n",
            "Epoch 27 Batch 600 Loss 0.7201 Accuracy 0.8017\n",
            "Epoch 27 Batch 650 Loss 0.7228 Accuracy 0.8013\n",
            "Epoch 27 Batch 700 Loss 0.7237 Accuracy 0.8012\n",
            "Epoch 27 Batch 750 Loss 0.7251 Accuracy 0.8009\n",
            "Epoch 27 Batch 800 Loss 0.7253 Accuracy 0.8008\n",
            "Epoch 27 Batch 850 Loss 0.7263 Accuracy 0.8006\n",
            "Epoch 27 Batch 900 Loss 0.7270 Accuracy 0.8004\n",
            "Epoch 27 Batch 950 Loss 0.7283 Accuracy 0.8001\n",
            "Epoch 27 Batch 1000 Loss 0.7291 Accuracy 0.7998\n",
            "Epoch 27 Batch 1050 Loss 0.7305 Accuracy 0.7994\n",
            "Epoch 27 Batch 1100 Loss 0.7322 Accuracy 0.7991\n",
            "Epoch 27 Batch 1150 Loss 0.7344 Accuracy 0.7987\n",
            "Epoch 27 Batch 1200 Loss 0.7357 Accuracy 0.7983\n",
            "Epoch 27 Loss 0.7360 Accuracy 0.7982\n",
            "Time taken for 1 epoch: 116.65 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.6415 Accuracy 0.8092\n",
            "Epoch 28 Batch 50 Loss 0.7021 Accuracy 0.8053\n",
            "Epoch 28 Batch 100 Loss 0.6874 Accuracy 0.8094\n",
            "Epoch 28 Batch 150 Loss 0.6874 Accuracy 0.8090\n",
            "Epoch 28 Batch 200 Loss 0.6880 Accuracy 0.8082\n",
            "Epoch 28 Batch 250 Loss 0.6907 Accuracy 0.8080\n",
            "Epoch 28 Batch 300 Loss 0.6923 Accuracy 0.8073\n",
            "Epoch 28 Batch 350 Loss 0.6933 Accuracy 0.8071\n",
            "Epoch 28 Batch 400 Loss 0.6959 Accuracy 0.8063\n",
            "Epoch 28 Batch 450 Loss 0.6973 Accuracy 0.8060\n",
            "Epoch 28 Batch 500 Loss 0.6986 Accuracy 0.8059\n",
            "Epoch 28 Batch 550 Loss 0.6994 Accuracy 0.8057\n",
            "Epoch 28 Batch 600 Loss 0.7020 Accuracy 0.8053\n",
            "Epoch 28 Batch 650 Loss 0.7043 Accuracy 0.8048\n",
            "Epoch 28 Batch 700 Loss 0.7062 Accuracy 0.8043\n",
            "Epoch 28 Batch 750 Loss 0.7064 Accuracy 0.8042\n",
            "Epoch 28 Batch 800 Loss 0.7067 Accuracy 0.8042\n",
            "Epoch 28 Batch 850 Loss 0.7061 Accuracy 0.8043\n",
            "Epoch 28 Batch 900 Loss 0.7075 Accuracy 0.8040\n",
            "Epoch 28 Batch 950 Loss 0.7092 Accuracy 0.8037\n",
            "Epoch 28 Batch 1000 Loss 0.7093 Accuracy 0.8037\n",
            "Epoch 28 Batch 1050 Loss 0.7112 Accuracy 0.8033\n",
            "Epoch 28 Batch 1100 Loss 0.7119 Accuracy 0.8031\n",
            "Epoch 28 Batch 1150 Loss 0.7126 Accuracy 0.8029\n",
            "Epoch 28 Batch 1200 Loss 0.7135 Accuracy 0.8027\n",
            "Epoch 28 Loss 0.7144 Accuracy 0.8024\n",
            "Time taken for 1 epoch: 116.76 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.6270 Accuracy 0.8298\n",
            "Epoch 29 Batch 50 Loss 0.6732 Accuracy 0.8118\n",
            "Epoch 29 Batch 100 Loss 0.6630 Accuracy 0.8146\n",
            "Epoch 29 Batch 150 Loss 0.6624 Accuracy 0.8142\n",
            "Epoch 29 Batch 200 Loss 0.6671 Accuracy 0.8134\n",
            "Epoch 29 Batch 250 Loss 0.6723 Accuracy 0.8124\n",
            "Epoch 29 Batch 300 Loss 0.6752 Accuracy 0.8119\n",
            "Epoch 29 Batch 350 Loss 0.6757 Accuracy 0.8116\n",
            "Epoch 29 Batch 400 Loss 0.6770 Accuracy 0.8111\n",
            "Epoch 29 Batch 450 Loss 0.6787 Accuracy 0.8107\n",
            "Epoch 29 Batch 500 Loss 0.6790 Accuracy 0.8106\n",
            "Epoch 29 Batch 550 Loss 0.6806 Accuracy 0.8100\n",
            "Epoch 29 Batch 600 Loss 0.6820 Accuracy 0.8099\n",
            "Epoch 29 Batch 650 Loss 0.6805 Accuracy 0.8100\n",
            "Epoch 29 Batch 700 Loss 0.6830 Accuracy 0.8096\n",
            "Epoch 29 Batch 750 Loss 0.6835 Accuracy 0.8094\n",
            "Epoch 29 Batch 800 Loss 0.6844 Accuracy 0.8092\n",
            "Epoch 29 Batch 850 Loss 0.6842 Accuracy 0.8093\n",
            "Epoch 29 Batch 900 Loss 0.6849 Accuracy 0.8091\n",
            "Epoch 29 Batch 950 Loss 0.6859 Accuracy 0.8090\n",
            "Epoch 29 Batch 1000 Loss 0.6874 Accuracy 0.8087\n",
            "Epoch 29 Batch 1050 Loss 0.6891 Accuracy 0.8083\n",
            "Epoch 29 Batch 1100 Loss 0.6901 Accuracy 0.8081\n",
            "Epoch 29 Batch 1150 Loss 0.6915 Accuracy 0.8078\n",
            "Epoch 29 Batch 1200 Loss 0.6935 Accuracy 0.8075\n",
            "Epoch 29 Loss 0.6948 Accuracy 0.8072\n",
            "Time taken for 1 epoch: 116.41 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.7721 Accuracy 0.8027\n",
            "Epoch 30 Batch 50 Loss 0.6442 Accuracy 0.8188\n",
            "Epoch 30 Batch 100 Loss 0.6431 Accuracy 0.8187\n",
            "Epoch 30 Batch 150 Loss 0.6432 Accuracy 0.8196\n",
            "Epoch 30 Batch 200 Loss 0.6397 Accuracy 0.8204\n",
            "Epoch 30 Batch 250 Loss 0.6437 Accuracy 0.8192\n",
            "Epoch 30 Batch 300 Loss 0.6450 Accuracy 0.8190\n",
            "Epoch 30 Batch 350 Loss 0.6475 Accuracy 0.8180\n",
            "Epoch 30 Batch 400 Loss 0.6500 Accuracy 0.8173\n",
            "Epoch 30 Batch 450 Loss 0.6540 Accuracy 0.8165\n",
            "Epoch 30 Batch 500 Loss 0.6549 Accuracy 0.8165\n",
            "Epoch 30 Batch 550 Loss 0.6567 Accuracy 0.8159\n",
            "Epoch 30 Batch 600 Loss 0.6612 Accuracy 0.8148\n",
            "Epoch 30 Batch 650 Loss 0.6624 Accuracy 0.8147\n",
            "Epoch 30 Batch 700 Loss 0.6638 Accuracy 0.8142\n",
            "Epoch 30 Batch 750 Loss 0.6649 Accuracy 0.8139\n",
            "Epoch 30 Batch 800 Loss 0.6661 Accuracy 0.8137\n",
            "Epoch 30 Batch 850 Loss 0.6663 Accuracy 0.8137\n",
            "Epoch 30 Batch 900 Loss 0.6675 Accuracy 0.8134\n",
            "Epoch 30 Batch 950 Loss 0.6696 Accuracy 0.8128\n",
            "Epoch 30 Batch 1000 Loss 0.6703 Accuracy 0.8127\n",
            "Epoch 30 Batch 1050 Loss 0.6714 Accuracy 0.8124\n",
            "Epoch 30 Batch 1100 Loss 0.6724 Accuracy 0.8121\n",
            "Epoch 30 Batch 1150 Loss 0.6728 Accuracy 0.8120\n",
            "Epoch 30 Batch 1200 Loss 0.6746 Accuracy 0.8116\n",
            "Saving checkpoint for epoch 30 at ./checkpoints/train/ckpt-6\n",
            "Epoch 30 Loss 0.6764 Accuracy 0.8112\n",
            "Time taken for 1 epoch: 117.28 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.5855 Accuracy 0.8368\n",
            "Epoch 31 Batch 50 Loss 0.6207 Accuracy 0.8246\n",
            "Epoch 31 Batch 100 Loss 0.6219 Accuracy 0.8238\n",
            "Epoch 31 Batch 150 Loss 0.6245 Accuracy 0.8237\n",
            "Epoch 31 Batch 200 Loss 0.6284 Accuracy 0.8230\n",
            "Epoch 31 Batch 250 Loss 0.6306 Accuracy 0.8223\n",
            "Epoch 31 Batch 300 Loss 0.6333 Accuracy 0.8216\n",
            "Epoch 31 Batch 350 Loss 0.6371 Accuracy 0.8209\n",
            "Epoch 31 Batch 400 Loss 0.6385 Accuracy 0.8204\n",
            "Epoch 31 Batch 450 Loss 0.6407 Accuracy 0.8199\n",
            "Epoch 31 Batch 500 Loss 0.6411 Accuracy 0.8197\n",
            "Epoch 31 Batch 550 Loss 0.6420 Accuracy 0.8193\n",
            "Epoch 31 Batch 600 Loss 0.6434 Accuracy 0.8188\n",
            "Epoch 31 Batch 650 Loss 0.6464 Accuracy 0.8181\n",
            "Epoch 31 Batch 700 Loss 0.6475 Accuracy 0.8181\n",
            "Epoch 31 Batch 750 Loss 0.6487 Accuracy 0.8177\n",
            "Epoch 31 Batch 800 Loss 0.6492 Accuracy 0.8176\n",
            "Epoch 31 Batch 850 Loss 0.6501 Accuracy 0.8173\n",
            "Epoch 31 Batch 900 Loss 0.6515 Accuracy 0.8171\n",
            "Epoch 31 Batch 950 Loss 0.6521 Accuracy 0.8168\n",
            "Epoch 31 Batch 1000 Loss 0.6541 Accuracy 0.8163\n",
            "Epoch 31 Batch 1050 Loss 0.6558 Accuracy 0.8160\n",
            "Epoch 31 Batch 1100 Loss 0.6566 Accuracy 0.8158\n",
            "Epoch 31 Batch 1150 Loss 0.6577 Accuracy 0.8156\n",
            "Epoch 31 Batch 1200 Loss 0.6595 Accuracy 0.8152\n",
            "Epoch 31 Loss 0.6606 Accuracy 0.8150\n",
            "Time taken for 1 epoch: 116.61 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.6827 Accuracy 0.8237\n",
            "Epoch 32 Batch 50 Loss 0.6140 Accuracy 0.8254\n",
            "Epoch 32 Batch 100 Loss 0.6215 Accuracy 0.8239\n",
            "Epoch 32 Batch 150 Loss 0.6204 Accuracy 0.8234\n",
            "Epoch 32 Batch 200 Loss 0.6177 Accuracy 0.8244\n",
            "Epoch 32 Batch 250 Loss 0.6175 Accuracy 0.8242\n",
            "Epoch 32 Batch 300 Loss 0.6197 Accuracy 0.8232\n",
            "Epoch 32 Batch 350 Loss 0.6207 Accuracy 0.8231\n",
            "Epoch 32 Batch 400 Loss 0.6212 Accuracy 0.8232\n",
            "Epoch 32 Batch 450 Loss 0.6253 Accuracy 0.8222\n",
            "Epoch 32 Batch 500 Loss 0.6269 Accuracy 0.8220\n",
            "Epoch 32 Batch 550 Loss 0.6286 Accuracy 0.8216\n",
            "Epoch 32 Batch 600 Loss 0.6292 Accuracy 0.8215\n",
            "Epoch 32 Batch 650 Loss 0.6306 Accuracy 0.8213\n",
            "Epoch 32 Batch 700 Loss 0.6318 Accuracy 0.8212\n",
            "Epoch 32 Batch 750 Loss 0.6325 Accuracy 0.8211\n",
            "Epoch 32 Batch 800 Loss 0.6334 Accuracy 0.8209\n",
            "Epoch 32 Batch 850 Loss 0.6354 Accuracy 0.8206\n",
            "Epoch 32 Batch 900 Loss 0.6350 Accuracy 0.8206\n",
            "Epoch 32 Batch 950 Loss 0.6359 Accuracy 0.8204\n",
            "Epoch 32 Batch 1000 Loss 0.6368 Accuracy 0.8202\n",
            "Epoch 32 Batch 1050 Loss 0.6377 Accuracy 0.8199\n",
            "Epoch 32 Batch 1100 Loss 0.6396 Accuracy 0.8194\n",
            "Epoch 32 Batch 1150 Loss 0.6411 Accuracy 0.8192\n",
            "Epoch 32 Batch 1200 Loss 0.6421 Accuracy 0.8189\n",
            "Epoch 32 Loss 0.6433 Accuracy 0.8186\n",
            "Time taken for 1 epoch: 116.63 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.6326 Accuracy 0.8235\n",
            "Epoch 33 Batch 50 Loss 0.5920 Accuracy 0.8304\n",
            "Epoch 33 Batch 100 Loss 0.5888 Accuracy 0.8319\n",
            "Epoch 33 Batch 150 Loss 0.5942 Accuracy 0.8308\n",
            "Epoch 33 Batch 200 Loss 0.5978 Accuracy 0.8294\n",
            "Epoch 33 Batch 250 Loss 0.5970 Accuracy 0.8293\n",
            "Epoch 33 Batch 300 Loss 0.6007 Accuracy 0.8286\n",
            "Epoch 33 Batch 350 Loss 0.6028 Accuracy 0.8282\n",
            "Epoch 33 Batch 400 Loss 0.6064 Accuracy 0.8276\n",
            "Epoch 33 Batch 450 Loss 0.6084 Accuracy 0.8273\n",
            "Epoch 33 Batch 500 Loss 0.6108 Accuracy 0.8266\n",
            "Epoch 33 Batch 550 Loss 0.6116 Accuracy 0.8264\n",
            "Epoch 33 Batch 600 Loss 0.6125 Accuracy 0.8260\n",
            "Epoch 33 Batch 650 Loss 0.6129 Accuracy 0.8259\n",
            "Epoch 33 Batch 700 Loss 0.6131 Accuracy 0.8258\n",
            "Epoch 33 Batch 750 Loss 0.6142 Accuracy 0.8255\n",
            "Epoch 33 Batch 800 Loss 0.6157 Accuracy 0.8252\n",
            "Epoch 33 Batch 850 Loss 0.6160 Accuracy 0.8251\n",
            "Epoch 33 Batch 900 Loss 0.6175 Accuracy 0.8248\n",
            "Epoch 33 Batch 950 Loss 0.6186 Accuracy 0.8245\n",
            "Epoch 33 Batch 1000 Loss 0.6199 Accuracy 0.8242\n",
            "Epoch 33 Batch 1050 Loss 0.6216 Accuracy 0.8237\n",
            "Epoch 33 Batch 1100 Loss 0.6233 Accuracy 0.8234\n",
            "Epoch 33 Batch 1150 Loss 0.6239 Accuracy 0.8233\n",
            "Epoch 33 Batch 1200 Loss 0.6250 Accuracy 0.8230\n",
            "Epoch 33 Loss 0.6267 Accuracy 0.8226\n",
            "Time taken for 1 epoch: 116.54 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.5941 Accuracy 0.8386\n",
            "Epoch 34 Batch 50 Loss 0.5930 Accuracy 0.8316\n",
            "Epoch 34 Batch 100 Loss 0.5843 Accuracy 0.8326\n",
            "Epoch 34 Batch 150 Loss 0.5799 Accuracy 0.8336\n",
            "Epoch 34 Batch 200 Loss 0.5832 Accuracy 0.8324\n",
            "Epoch 34 Batch 250 Loss 0.5839 Accuracy 0.8326\n",
            "Epoch 34 Batch 300 Loss 0.5848 Accuracy 0.8323\n",
            "Epoch 34 Batch 350 Loss 0.5890 Accuracy 0.8317\n",
            "Epoch 34 Batch 400 Loss 0.5916 Accuracy 0.8312\n",
            "Epoch 34 Batch 450 Loss 0.5941 Accuracy 0.8306\n",
            "Epoch 34 Batch 500 Loss 0.5954 Accuracy 0.8300\n",
            "Epoch 34 Batch 550 Loss 0.5961 Accuracy 0.8299\n",
            "Epoch 34 Batch 600 Loss 0.5991 Accuracy 0.8293\n",
            "Epoch 34 Batch 650 Loss 0.5998 Accuracy 0.8290\n",
            "Epoch 34 Batch 700 Loss 0.6012 Accuracy 0.8285\n",
            "Epoch 34 Batch 750 Loss 0.6023 Accuracy 0.8281\n",
            "Epoch 34 Batch 800 Loss 0.6030 Accuracy 0.8279\n",
            "Epoch 34 Batch 850 Loss 0.6027 Accuracy 0.8280\n",
            "Epoch 34 Batch 900 Loss 0.6039 Accuracy 0.8277\n",
            "Epoch 34 Batch 950 Loss 0.6055 Accuracy 0.8273\n",
            "Epoch 34 Batch 1000 Loss 0.6064 Accuracy 0.8270\n",
            "Epoch 34 Batch 1050 Loss 0.6070 Accuracy 0.8269\n",
            "Epoch 34 Batch 1100 Loss 0.6092 Accuracy 0.8264\n",
            "Epoch 34 Batch 1150 Loss 0.6102 Accuracy 0.8261\n",
            "Epoch 34 Batch 1200 Loss 0.6117 Accuracy 0.8257\n",
            "Epoch 34 Loss 0.6125 Accuracy 0.8256\n",
            "Time taken for 1 epoch: 116.28 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.6219 Accuracy 0.8265\n",
            "Epoch 35 Batch 50 Loss 0.5548 Accuracy 0.8398\n",
            "Epoch 35 Batch 100 Loss 0.5665 Accuracy 0.8375\n",
            "Epoch 35 Batch 150 Loss 0.5698 Accuracy 0.8363\n",
            "Epoch 35 Batch 200 Loss 0.5656 Accuracy 0.8371\n",
            "Epoch 35 Batch 250 Loss 0.5713 Accuracy 0.8357\n",
            "Epoch 35 Batch 300 Loss 0.5753 Accuracy 0.8345\n",
            "Epoch 35 Batch 350 Loss 0.5766 Accuracy 0.8343\n",
            "Epoch 35 Batch 400 Loss 0.5763 Accuracy 0.8344\n",
            "Epoch 35 Batch 450 Loss 0.5796 Accuracy 0.8336\n",
            "Epoch 35 Batch 500 Loss 0.5817 Accuracy 0.8329\n",
            "Epoch 35 Batch 550 Loss 0.5832 Accuracy 0.8326\n",
            "Epoch 35 Batch 600 Loss 0.5839 Accuracy 0.8325\n",
            "Epoch 35 Batch 650 Loss 0.5848 Accuracy 0.8322\n",
            "Epoch 35 Batch 700 Loss 0.5859 Accuracy 0.8320\n",
            "Epoch 35 Batch 750 Loss 0.5875 Accuracy 0.8315\n",
            "Epoch 35 Batch 800 Loss 0.5887 Accuracy 0.8311\n",
            "Epoch 35 Batch 850 Loss 0.5900 Accuracy 0.8308\n",
            "Epoch 35 Batch 900 Loss 0.5910 Accuracy 0.8307\n",
            "Epoch 35 Batch 950 Loss 0.5923 Accuracy 0.8304\n",
            "Epoch 35 Batch 1000 Loss 0.5941 Accuracy 0.8300\n",
            "Epoch 35 Batch 1050 Loss 0.5947 Accuracy 0.8298\n",
            "Epoch 35 Batch 1100 Loss 0.5961 Accuracy 0.8295\n",
            "Epoch 35 Batch 1150 Loss 0.5976 Accuracy 0.8291\n",
            "Epoch 35 Batch 1200 Loss 0.5990 Accuracy 0.8289\n",
            "Saving checkpoint for epoch 35 at ./checkpoints/train/ckpt-7\n",
            "Epoch 35 Loss 0.6001 Accuracy 0.8286\n",
            "Time taken for 1 epoch: 117.46 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.5299 Accuracy 0.8536\n",
            "Epoch 36 Batch 50 Loss 0.5528 Accuracy 0.8375\n",
            "Epoch 36 Batch 100 Loss 0.5521 Accuracy 0.8395\n",
            "Epoch 36 Batch 150 Loss 0.5529 Accuracy 0.8396\n",
            "Epoch 36 Batch 200 Loss 0.5544 Accuracy 0.8396\n",
            "Epoch 36 Batch 250 Loss 0.5544 Accuracy 0.8396\n",
            "Epoch 36 Batch 300 Loss 0.5580 Accuracy 0.8389\n",
            "Epoch 36 Batch 350 Loss 0.5618 Accuracy 0.8377\n",
            "Epoch 36 Batch 400 Loss 0.5640 Accuracy 0.8372\n",
            "Epoch 36 Batch 450 Loss 0.5666 Accuracy 0.8366\n",
            "Epoch 36 Batch 500 Loss 0.5693 Accuracy 0.8360\n",
            "Epoch 36 Batch 550 Loss 0.5700 Accuracy 0.8358\n",
            "Epoch 36 Batch 600 Loss 0.5720 Accuracy 0.8353\n",
            "Epoch 36 Batch 650 Loss 0.5729 Accuracy 0.8350\n",
            "Epoch 36 Batch 700 Loss 0.5732 Accuracy 0.8352\n",
            "Epoch 36 Batch 750 Loss 0.5739 Accuracy 0.8350\n",
            "Epoch 36 Batch 800 Loss 0.5757 Accuracy 0.8346\n",
            "Epoch 36 Batch 850 Loss 0.5765 Accuracy 0.8343\n",
            "Epoch 36 Batch 900 Loss 0.5777 Accuracy 0.8340\n",
            "Epoch 36 Batch 950 Loss 0.5784 Accuracy 0.8338\n",
            "Epoch 36 Batch 1000 Loss 0.5808 Accuracy 0.8333\n",
            "Epoch 36 Batch 1050 Loss 0.5820 Accuracy 0.8330\n",
            "Epoch 36 Batch 1100 Loss 0.5831 Accuracy 0.8327\n",
            "Epoch 36 Batch 1150 Loss 0.5840 Accuracy 0.8324\n",
            "Epoch 36 Batch 1200 Loss 0.5858 Accuracy 0.8319\n",
            "Epoch 36 Loss 0.5868 Accuracy 0.8317\n",
            "Time taken for 1 epoch: 116.37 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.6294 Accuracy 0.8161\n",
            "Epoch 37 Batch 50 Loss 0.5439 Accuracy 0.8440\n",
            "Epoch 37 Batch 100 Loss 0.5435 Accuracy 0.8430\n",
            "Epoch 37 Batch 150 Loss 0.5455 Accuracy 0.8427\n",
            "Epoch 37 Batch 200 Loss 0.5476 Accuracy 0.8418\n",
            "Epoch 37 Batch 250 Loss 0.5483 Accuracy 0.8417\n",
            "Epoch 37 Batch 300 Loss 0.5484 Accuracy 0.8416\n",
            "Epoch 37 Batch 350 Loss 0.5514 Accuracy 0.8407\n",
            "Epoch 37 Batch 400 Loss 0.5549 Accuracy 0.8400\n",
            "Epoch 37 Batch 450 Loss 0.5567 Accuracy 0.8396\n",
            "Epoch 37 Batch 500 Loss 0.5577 Accuracy 0.8393\n",
            "Epoch 37 Batch 550 Loss 0.5595 Accuracy 0.8390\n",
            "Epoch 37 Batch 600 Loss 0.5598 Accuracy 0.8390\n",
            "Epoch 37 Batch 650 Loss 0.5606 Accuracy 0.8387\n",
            "Epoch 37 Batch 700 Loss 0.5621 Accuracy 0.8382\n",
            "Epoch 37 Batch 750 Loss 0.5629 Accuracy 0.8379\n",
            "Epoch 37 Batch 800 Loss 0.5632 Accuracy 0.8378\n",
            "Epoch 37 Batch 850 Loss 0.5639 Accuracy 0.8376\n",
            "Epoch 37 Batch 900 Loss 0.5649 Accuracy 0.8374\n",
            "Epoch 37 Batch 950 Loss 0.5665 Accuracy 0.8370\n",
            "Epoch 37 Batch 1000 Loss 0.5674 Accuracy 0.8367\n",
            "Epoch 37 Batch 1050 Loss 0.5683 Accuracy 0.8364\n",
            "Epoch 37 Batch 1100 Loss 0.5695 Accuracy 0.8361\n",
            "Epoch 37 Batch 1150 Loss 0.5710 Accuracy 0.8357\n",
            "Epoch 37 Batch 1200 Loss 0.5718 Accuracy 0.8355\n",
            "Epoch 37 Loss 0.5728 Accuracy 0.8353\n",
            "Time taken for 1 epoch: 116.44 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.4815 Accuracy 0.8479\n",
            "Epoch 38 Batch 50 Loss 0.5367 Accuracy 0.8430\n",
            "Epoch 38 Batch 100 Loss 0.5232 Accuracy 0.8470\n",
            "Epoch 38 Batch 150 Loss 0.5240 Accuracy 0.8474\n",
            "Epoch 38 Batch 200 Loss 0.5277 Accuracy 0.8461\n",
            "Epoch 38 Batch 250 Loss 0.5341 Accuracy 0.8444\n",
            "Epoch 38 Batch 300 Loss 0.5365 Accuracy 0.8438\n",
            "Epoch 38 Batch 350 Loss 0.5380 Accuracy 0.8434\n",
            "Epoch 38 Batch 400 Loss 0.5390 Accuracy 0.8431\n",
            "Epoch 38 Batch 450 Loss 0.5419 Accuracy 0.8423\n",
            "Epoch 38 Batch 500 Loss 0.5431 Accuracy 0.8423\n",
            "Epoch 38 Batch 550 Loss 0.5449 Accuracy 0.8420\n",
            "Epoch 38 Batch 600 Loss 0.5454 Accuracy 0.8416\n",
            "Epoch 38 Batch 650 Loss 0.5472 Accuracy 0.8411\n",
            "Epoch 38 Batch 700 Loss 0.5478 Accuracy 0.8411\n",
            "Epoch 38 Batch 750 Loss 0.5495 Accuracy 0.8405\n",
            "Epoch 38 Batch 800 Loss 0.5510 Accuracy 0.8402\n",
            "Epoch 38 Batch 850 Loss 0.5520 Accuracy 0.8399\n",
            "Epoch 38 Batch 900 Loss 0.5534 Accuracy 0.8395\n",
            "Epoch 38 Batch 950 Loss 0.5549 Accuracy 0.8393\n",
            "Epoch 38 Batch 1000 Loss 0.5554 Accuracy 0.8393\n",
            "Epoch 38 Batch 1050 Loss 0.5565 Accuracy 0.8389\n",
            "Epoch 38 Batch 1100 Loss 0.5583 Accuracy 0.8385\n",
            "Epoch 38 Batch 1150 Loss 0.5594 Accuracy 0.8382\n",
            "Epoch 38 Batch 1200 Loss 0.5600 Accuracy 0.8381\n",
            "Epoch 38 Loss 0.5613 Accuracy 0.8378\n",
            "Time taken for 1 epoch: 116.30 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.4900 Accuracy 0.8504\n",
            "Epoch 39 Batch 50 Loss 0.5165 Accuracy 0.8486\n",
            "Epoch 39 Batch 100 Loss 0.5177 Accuracy 0.8493\n",
            "Epoch 39 Batch 150 Loss 0.5207 Accuracy 0.8486\n",
            "Epoch 39 Batch 200 Loss 0.5187 Accuracy 0.8490\n",
            "Epoch 39 Batch 250 Loss 0.5200 Accuracy 0.8485\n",
            "Epoch 39 Batch 300 Loss 0.5232 Accuracy 0.8475\n",
            "Epoch 39 Batch 350 Loss 0.5262 Accuracy 0.8467\n",
            "Epoch 39 Batch 400 Loss 0.5276 Accuracy 0.8463\n",
            "Epoch 39 Batch 450 Loss 0.5285 Accuracy 0.8461\n",
            "Epoch 39 Batch 500 Loss 0.5300 Accuracy 0.8456\n",
            "Epoch 39 Batch 550 Loss 0.5313 Accuracy 0.8453\n",
            "Epoch 39 Batch 600 Loss 0.5326 Accuracy 0.8447\n",
            "Epoch 39 Batch 650 Loss 0.5340 Accuracy 0.8443\n",
            "Epoch 39 Batch 700 Loss 0.5349 Accuracy 0.8443\n",
            "Epoch 39 Batch 750 Loss 0.5368 Accuracy 0.8438\n",
            "Epoch 39 Batch 800 Loss 0.5380 Accuracy 0.8436\n",
            "Epoch 39 Batch 850 Loss 0.5386 Accuracy 0.8434\n",
            "Epoch 39 Batch 900 Loss 0.5401 Accuracy 0.8431\n",
            "Epoch 39 Batch 950 Loss 0.5412 Accuracy 0.8429\n",
            "Epoch 39 Batch 1000 Loss 0.5428 Accuracy 0.8425\n",
            "Epoch 39 Batch 1050 Loss 0.5441 Accuracy 0.8422\n",
            "Epoch 39 Batch 1100 Loss 0.5449 Accuracy 0.8420\n",
            "Epoch 39 Batch 1150 Loss 0.5459 Accuracy 0.8417\n",
            "Epoch 39 Batch 1200 Loss 0.5475 Accuracy 0.8412\n",
            "Epoch 39 Loss 0.5483 Accuracy 0.8410\n",
            "Time taken for 1 epoch: 116.65 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.5656 Accuracy 0.8349\n",
            "Epoch 40 Batch 50 Loss 0.4992 Accuracy 0.8546\n",
            "Epoch 40 Batch 100 Loss 0.4969 Accuracy 0.8541\n",
            "Epoch 40 Batch 150 Loss 0.4975 Accuracy 0.8539\n",
            "Epoch 40 Batch 200 Loss 0.5028 Accuracy 0.8525\n",
            "Epoch 40 Batch 250 Loss 0.5047 Accuracy 0.8520\n",
            "Epoch 40 Batch 300 Loss 0.5091 Accuracy 0.8510\n",
            "Epoch 40 Batch 350 Loss 0.5125 Accuracy 0.8500\n",
            "Epoch 40 Batch 400 Loss 0.5147 Accuracy 0.8493\n",
            "Epoch 40 Batch 450 Loss 0.5178 Accuracy 0.8486\n",
            "Epoch 40 Batch 500 Loss 0.5202 Accuracy 0.8479\n",
            "Epoch 40 Batch 550 Loss 0.5222 Accuracy 0.8476\n",
            "Epoch 40 Batch 600 Loss 0.5243 Accuracy 0.8471\n",
            "Epoch 40 Batch 650 Loss 0.5255 Accuracy 0.8467\n",
            "Epoch 40 Batch 700 Loss 0.5268 Accuracy 0.8464\n",
            "Epoch 40 Batch 750 Loss 0.5273 Accuracy 0.8463\n",
            "Epoch 40 Batch 800 Loss 0.5282 Accuracy 0.8462\n",
            "Epoch 40 Batch 850 Loss 0.5287 Accuracy 0.8461\n",
            "Epoch 40 Batch 900 Loss 0.5296 Accuracy 0.8458\n",
            "Epoch 40 Batch 950 Loss 0.5309 Accuracy 0.8454\n",
            "Epoch 40 Batch 1000 Loss 0.5321 Accuracy 0.8451\n",
            "Epoch 40 Batch 1050 Loss 0.5331 Accuracy 0.8448\n",
            "Epoch 40 Batch 1100 Loss 0.5344 Accuracy 0.8447\n",
            "Epoch 40 Batch 1150 Loss 0.5359 Accuracy 0.8443\n",
            "Epoch 40 Batch 1200 Loss 0.5374 Accuracy 0.8439\n",
            "Saving checkpoint for epoch 40 at ./checkpoints/train/ckpt-8\n",
            "Epoch 40 Loss 0.5386 Accuracy 0.8437\n",
            "Time taken for 1 epoch: 117.97 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.4018 Accuracy 0.8835\n",
            "Epoch 41 Batch 50 Loss 0.5056 Accuracy 0.8514\n",
            "Epoch 41 Batch 100 Loss 0.5021 Accuracy 0.8523\n",
            "Epoch 41 Batch 150 Loss 0.5007 Accuracy 0.8530\n",
            "Epoch 41 Batch 200 Loss 0.5006 Accuracy 0.8527\n",
            "Epoch 41 Batch 250 Loss 0.5021 Accuracy 0.8523\n",
            "Epoch 41 Batch 300 Loss 0.5045 Accuracy 0.8521\n",
            "Epoch 41 Batch 350 Loss 0.5077 Accuracy 0.8513\n",
            "Epoch 41 Batch 400 Loss 0.5102 Accuracy 0.8508\n",
            "Epoch 41 Batch 450 Loss 0.5095 Accuracy 0.8511\n",
            "Epoch 41 Batch 500 Loss 0.5107 Accuracy 0.8509\n",
            "Epoch 41 Batch 550 Loss 0.5121 Accuracy 0.8505\n",
            "Epoch 41 Batch 600 Loss 0.5131 Accuracy 0.8503\n",
            "Epoch 41 Batch 650 Loss 0.5145 Accuracy 0.8498\n",
            "Epoch 41 Batch 700 Loss 0.5157 Accuracy 0.8495\n",
            "Epoch 41 Batch 750 Loss 0.5164 Accuracy 0.8492\n",
            "Epoch 41 Batch 800 Loss 0.5177 Accuracy 0.8489\n",
            "Epoch 41 Batch 850 Loss 0.5186 Accuracy 0.8487\n",
            "Epoch 41 Batch 900 Loss 0.5196 Accuracy 0.8483\n",
            "Epoch 41 Batch 950 Loss 0.5204 Accuracy 0.8481\n",
            "Epoch 41 Batch 1000 Loss 0.5216 Accuracy 0.8477\n",
            "Epoch 41 Batch 1050 Loss 0.5224 Accuracy 0.8474\n",
            "Epoch 41 Batch 1100 Loss 0.5240 Accuracy 0.8470\n",
            "Epoch 41 Batch 1150 Loss 0.5255 Accuracy 0.8466\n",
            "Epoch 41 Batch 1200 Loss 0.5268 Accuracy 0.8462\n",
            "Epoch 41 Loss 0.5279 Accuracy 0.8460\n",
            "Time taken for 1 epoch: 116.62 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.4674 Accuracy 0.8653\n",
            "Epoch 42 Batch 50 Loss 0.4839 Accuracy 0.8587\n",
            "Epoch 42 Batch 100 Loss 0.4885 Accuracy 0.8568\n",
            "Epoch 42 Batch 150 Loss 0.4874 Accuracy 0.8562\n",
            "Epoch 42 Batch 200 Loss 0.4918 Accuracy 0.8554\n",
            "Epoch 42 Batch 250 Loss 0.4935 Accuracy 0.8553\n",
            "Epoch 42 Batch 300 Loss 0.4956 Accuracy 0.8544\n",
            "Epoch 42 Batch 350 Loss 0.4961 Accuracy 0.8543\n",
            "Epoch 42 Batch 400 Loss 0.4968 Accuracy 0.8540\n",
            "Epoch 42 Batch 450 Loss 0.4986 Accuracy 0.8534\n",
            "Epoch 42 Batch 500 Loss 0.5005 Accuracy 0.8529\n",
            "Epoch 42 Batch 550 Loss 0.5019 Accuracy 0.8525\n",
            "Epoch 42 Batch 600 Loss 0.5032 Accuracy 0.8524\n",
            "Epoch 42 Batch 650 Loss 0.5036 Accuracy 0.8522\n",
            "Epoch 42 Batch 700 Loss 0.5052 Accuracy 0.8516\n",
            "Epoch 42 Batch 750 Loss 0.5063 Accuracy 0.8513\n",
            "Epoch 42 Batch 800 Loss 0.5067 Accuracy 0.8513\n",
            "Epoch 42 Batch 850 Loss 0.5075 Accuracy 0.8511\n",
            "Epoch 42 Batch 900 Loss 0.5090 Accuracy 0.8507\n",
            "Epoch 42 Batch 950 Loss 0.5103 Accuracy 0.8503\n",
            "Epoch 42 Batch 1000 Loss 0.5118 Accuracy 0.8499\n",
            "Epoch 42 Batch 1050 Loss 0.5124 Accuracy 0.8498\n",
            "Epoch 42 Batch 1100 Loss 0.5128 Accuracy 0.8497\n",
            "Epoch 42 Batch 1150 Loss 0.5136 Accuracy 0.8495\n",
            "Epoch 42 Batch 1200 Loss 0.5148 Accuracy 0.8492\n",
            "Epoch 42 Loss 0.5157 Accuracy 0.8489\n",
            "Time taken for 1 epoch: 116.45 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.3981 Accuracy 0.8798\n",
            "Epoch 43 Batch 50 Loss 0.4716 Accuracy 0.8595\n",
            "Epoch 43 Batch 100 Loss 0.4789 Accuracy 0.8583\n",
            "Epoch 43 Batch 150 Loss 0.4819 Accuracy 0.8576\n",
            "Epoch 43 Batch 200 Loss 0.4830 Accuracy 0.8568\n",
            "Epoch 43 Batch 250 Loss 0.4823 Accuracy 0.8570\n",
            "Epoch 43 Batch 300 Loss 0.4821 Accuracy 0.8569\n",
            "Epoch 43 Batch 350 Loss 0.4854 Accuracy 0.8564\n",
            "Epoch 43 Batch 400 Loss 0.4876 Accuracy 0.8560\n",
            "Epoch 43 Batch 450 Loss 0.4896 Accuracy 0.8556\n",
            "Epoch 43 Batch 500 Loss 0.4920 Accuracy 0.8550\n",
            "Epoch 43 Batch 550 Loss 0.4937 Accuracy 0.8546\n",
            "Epoch 43 Batch 600 Loss 0.4956 Accuracy 0.8540\n",
            "Epoch 43 Batch 650 Loss 0.4961 Accuracy 0.8541\n",
            "Epoch 43 Batch 700 Loss 0.4962 Accuracy 0.8540\n",
            "Epoch 43 Batch 750 Loss 0.4971 Accuracy 0.8536\n",
            "Epoch 43 Batch 800 Loss 0.4972 Accuracy 0.8537\n",
            "Epoch 43 Batch 850 Loss 0.4978 Accuracy 0.8534\n",
            "Epoch 43 Batch 900 Loss 0.4994 Accuracy 0.8531\n",
            "Epoch 43 Batch 950 Loss 0.5004 Accuracy 0.8529\n",
            "Epoch 43 Batch 1000 Loss 0.5015 Accuracy 0.8526\n",
            "Epoch 43 Batch 1050 Loss 0.5023 Accuracy 0.8524\n",
            "Epoch 43 Batch 1100 Loss 0.5032 Accuracy 0.8522\n",
            "Epoch 43 Batch 1150 Loss 0.5042 Accuracy 0.8520\n",
            "Epoch 43 Batch 1200 Loss 0.5062 Accuracy 0.8514\n",
            "Epoch 43 Loss 0.5073 Accuracy 0.8511\n",
            "Time taken for 1 epoch: 116.66 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.4811 Accuracy 0.8652\n",
            "Epoch 44 Batch 50 Loss 0.4830 Accuracy 0.8561\n",
            "Epoch 44 Batch 100 Loss 0.4722 Accuracy 0.8590\n",
            "Epoch 44 Batch 150 Loss 0.4729 Accuracy 0.8590\n",
            "Epoch 44 Batch 200 Loss 0.4744 Accuracy 0.8589\n",
            "Epoch 44 Batch 250 Loss 0.4744 Accuracy 0.8593\n",
            "Epoch 44 Batch 300 Loss 0.4768 Accuracy 0.8584\n",
            "Epoch 44 Batch 350 Loss 0.4771 Accuracy 0.8585\n",
            "Epoch 44 Batch 400 Loss 0.4792 Accuracy 0.8580\n",
            "Epoch 44 Batch 450 Loss 0.4818 Accuracy 0.8575\n",
            "Epoch 44 Batch 500 Loss 0.4838 Accuracy 0.8570\n",
            "Epoch 44 Batch 550 Loss 0.4839 Accuracy 0.8570\n",
            "Epoch 44 Batch 600 Loss 0.4854 Accuracy 0.8565\n",
            "Epoch 44 Batch 650 Loss 0.4856 Accuracy 0.8565\n",
            "Epoch 44 Batch 700 Loss 0.4859 Accuracy 0.8564\n",
            "Epoch 44 Batch 750 Loss 0.4866 Accuracy 0.8562\n",
            "Epoch 44 Batch 800 Loss 0.4863 Accuracy 0.8563\n",
            "Epoch 44 Batch 850 Loss 0.4867 Accuracy 0.8562\n",
            "Epoch 44 Batch 900 Loss 0.4882 Accuracy 0.8559\n",
            "Epoch 44 Batch 950 Loss 0.4889 Accuracy 0.8557\n",
            "Epoch 44 Batch 1000 Loss 0.4902 Accuracy 0.8554\n",
            "Epoch 44 Batch 1050 Loss 0.4916 Accuracy 0.8551\n",
            "Epoch 44 Batch 1100 Loss 0.4928 Accuracy 0.8549\n",
            "Epoch 44 Batch 1150 Loss 0.4936 Accuracy 0.8547\n",
            "Epoch 44 Batch 1200 Loss 0.4947 Accuracy 0.8544\n",
            "Epoch 44 Loss 0.4956 Accuracy 0.8541\n",
            "Time taken for 1 epoch: 116.81 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.3958 Accuracy 0.8811\n",
            "Epoch 45 Batch 50 Loss 0.4614 Accuracy 0.8643\n",
            "Epoch 45 Batch 100 Loss 0.4572 Accuracy 0.8654\n",
            "Epoch 45 Batch 150 Loss 0.4564 Accuracy 0.8649\n",
            "Epoch 45 Batch 200 Loss 0.4608 Accuracy 0.8635\n",
            "Epoch 45 Batch 250 Loss 0.4629 Accuracy 0.8629\n",
            "Epoch 45 Batch 300 Loss 0.4634 Accuracy 0.8627\n",
            "Epoch 45 Batch 350 Loss 0.4673 Accuracy 0.8618\n",
            "Epoch 45 Batch 400 Loss 0.4694 Accuracy 0.8611\n",
            "Epoch 45 Batch 450 Loss 0.4708 Accuracy 0.8605\n",
            "Epoch 45 Batch 500 Loss 0.4728 Accuracy 0.8600\n",
            "Epoch 45 Batch 550 Loss 0.4749 Accuracy 0.8596\n",
            "Epoch 45 Batch 600 Loss 0.4754 Accuracy 0.8593\n",
            "Epoch 45 Batch 650 Loss 0.4766 Accuracy 0.8590\n",
            "Epoch 45 Batch 700 Loss 0.4778 Accuracy 0.8586\n",
            "Epoch 45 Batch 750 Loss 0.4785 Accuracy 0.8585\n",
            "Epoch 45 Batch 800 Loss 0.4794 Accuracy 0.8584\n",
            "Epoch 45 Batch 850 Loss 0.4799 Accuracy 0.8582\n",
            "Epoch 45 Batch 900 Loss 0.4810 Accuracy 0.8579\n",
            "Epoch 45 Batch 950 Loss 0.4819 Accuracy 0.8576\n",
            "Epoch 45 Batch 1000 Loss 0.4831 Accuracy 0.8572\n",
            "Epoch 45 Batch 1050 Loss 0.4839 Accuracy 0.8570\n",
            "Epoch 45 Batch 1100 Loss 0.4841 Accuracy 0.8569\n",
            "Epoch 45 Batch 1150 Loss 0.4853 Accuracy 0.8566\n",
            "Epoch 45 Batch 1200 Loss 0.4869 Accuracy 0.8562\n",
            "Saving checkpoint for epoch 45 at ./checkpoints/train/ckpt-9\n",
            "Epoch 45 Loss 0.4884 Accuracy 0.8558\n",
            "Time taken for 1 epoch: 117.54 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.4605 Accuracy 0.8547\n",
            "Epoch 46 Batch 50 Loss 0.4470 Accuracy 0.8677\n",
            "Epoch 46 Batch 100 Loss 0.4452 Accuracy 0.8674\n",
            "Epoch 46 Batch 150 Loss 0.4492 Accuracy 0.8669\n",
            "Epoch 46 Batch 200 Loss 0.4505 Accuracy 0.8667\n",
            "Epoch 46 Batch 250 Loss 0.4507 Accuracy 0.8669\n",
            "Epoch 46 Batch 300 Loss 0.4526 Accuracy 0.8663\n",
            "Epoch 46 Batch 350 Loss 0.4537 Accuracy 0.8659\n",
            "Epoch 46 Batch 400 Loss 0.4574 Accuracy 0.8648\n",
            "Epoch 46 Batch 450 Loss 0.4601 Accuracy 0.8641\n",
            "Epoch 46 Batch 500 Loss 0.4620 Accuracy 0.8634\n",
            "Epoch 46 Batch 550 Loss 0.4639 Accuracy 0.8629\n",
            "Epoch 46 Batch 600 Loss 0.4648 Accuracy 0.8626\n",
            "Epoch 46 Batch 650 Loss 0.4663 Accuracy 0.8622\n",
            "Epoch 46 Batch 700 Loss 0.4683 Accuracy 0.8615\n",
            "Epoch 46 Batch 750 Loss 0.4690 Accuracy 0.8614\n",
            "Epoch 46 Batch 800 Loss 0.4693 Accuracy 0.8612\n",
            "Epoch 46 Batch 850 Loss 0.4706 Accuracy 0.8607\n",
            "Epoch 46 Batch 900 Loss 0.4719 Accuracy 0.8604\n",
            "Epoch 46 Batch 950 Loss 0.4725 Accuracy 0.8603\n",
            "Epoch 46 Batch 1000 Loss 0.4733 Accuracy 0.8601\n",
            "Epoch 46 Batch 1050 Loss 0.4740 Accuracy 0.8598\n",
            "Epoch 46 Batch 1100 Loss 0.4755 Accuracy 0.8593\n",
            "Epoch 46 Batch 1150 Loss 0.4768 Accuracy 0.8589\n",
            "Epoch 46 Batch 1200 Loss 0.4779 Accuracy 0.8587\n",
            "Epoch 46 Loss 0.4788 Accuracy 0.8584\n",
            "Time taken for 1 epoch: 116.65 secs\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.4988 Accuracy 0.8557\n",
            "Epoch 47 Batch 50 Loss 0.4471 Accuracy 0.8661\n",
            "Epoch 47 Batch 100 Loss 0.4427 Accuracy 0.8674\n",
            "Epoch 47 Batch 150 Loss 0.4410 Accuracy 0.8681\n",
            "Epoch 47 Batch 200 Loss 0.4433 Accuracy 0.8674\n",
            "Epoch 47 Batch 250 Loss 0.4439 Accuracy 0.8672\n",
            "Epoch 47 Batch 300 Loss 0.4443 Accuracy 0.8669\n",
            "Epoch 47 Batch 350 Loss 0.4468 Accuracy 0.8661\n",
            "Epoch 47 Batch 400 Loss 0.4490 Accuracy 0.8657\n",
            "Epoch 47 Batch 450 Loss 0.4509 Accuracy 0.8652\n",
            "Epoch 47 Batch 500 Loss 0.4522 Accuracy 0.8648\n",
            "Epoch 47 Batch 550 Loss 0.4535 Accuracy 0.8645\n",
            "Epoch 47 Batch 600 Loss 0.4560 Accuracy 0.8639\n",
            "Epoch 47 Batch 650 Loss 0.4575 Accuracy 0.8635\n",
            "Epoch 47 Batch 700 Loss 0.4589 Accuracy 0.8631\n",
            "Epoch 47 Batch 750 Loss 0.4595 Accuracy 0.8630\n",
            "Epoch 47 Batch 800 Loss 0.4603 Accuracy 0.8628\n",
            "Epoch 47 Batch 850 Loss 0.4615 Accuracy 0.8627\n",
            "Epoch 47 Batch 900 Loss 0.4626 Accuracy 0.8624\n",
            "Epoch 47 Batch 950 Loss 0.4637 Accuracy 0.8621\n",
            "Epoch 47 Batch 1000 Loss 0.4648 Accuracy 0.8617\n",
            "Epoch 47 Batch 1050 Loss 0.4654 Accuracy 0.8616\n",
            "Epoch 47 Batch 1100 Loss 0.4664 Accuracy 0.8613\n",
            "Epoch 47 Batch 1150 Loss 0.4675 Accuracy 0.8610\n",
            "Epoch 47 Batch 1200 Loss 0.4686 Accuracy 0.8608\n",
            "Epoch 47 Loss 0.4702 Accuracy 0.8603\n",
            "Time taken for 1 epoch: 116.66 secs\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.4090 Accuracy 0.8799\n",
            "Epoch 48 Batch 50 Loss 0.4272 Accuracy 0.8717\n",
            "Epoch 48 Batch 100 Loss 0.4298 Accuracy 0.8711\n",
            "Epoch 48 Batch 150 Loss 0.4327 Accuracy 0.8707\n",
            "Epoch 48 Batch 200 Loss 0.4364 Accuracy 0.8700\n",
            "Epoch 48 Batch 250 Loss 0.4377 Accuracy 0.8696\n",
            "Epoch 48 Batch 300 Loss 0.4386 Accuracy 0.8693\n",
            "Epoch 48 Batch 350 Loss 0.4415 Accuracy 0.8685\n",
            "Epoch 48 Batch 400 Loss 0.4439 Accuracy 0.8679\n",
            "Epoch 48 Batch 450 Loss 0.4465 Accuracy 0.8670\n",
            "Epoch 48 Batch 500 Loss 0.4483 Accuracy 0.8664\n",
            "Epoch 48 Batch 550 Loss 0.4491 Accuracy 0.8660\n",
            "Epoch 48 Batch 600 Loss 0.4491 Accuracy 0.8659\n",
            "Epoch 48 Batch 650 Loss 0.4507 Accuracy 0.8655\n",
            "Epoch 48 Batch 700 Loss 0.4512 Accuracy 0.8654\n",
            "Epoch 48 Batch 750 Loss 0.4525 Accuracy 0.8651\n",
            "Epoch 48 Batch 800 Loss 0.4533 Accuracy 0.8648\n",
            "Epoch 48 Batch 850 Loss 0.4536 Accuracy 0.8648\n",
            "Epoch 48 Batch 900 Loss 0.4543 Accuracy 0.8647\n",
            "Epoch 48 Batch 950 Loss 0.4557 Accuracy 0.8644\n",
            "Epoch 48 Batch 1000 Loss 0.4568 Accuracy 0.8641\n",
            "Epoch 48 Batch 1050 Loss 0.4575 Accuracy 0.8639\n",
            "Epoch 48 Batch 1100 Loss 0.4584 Accuracy 0.8637\n",
            "Epoch 48 Batch 1150 Loss 0.4593 Accuracy 0.8634\n",
            "Epoch 48 Batch 1200 Loss 0.4614 Accuracy 0.8628\n",
            "Epoch 48 Loss 0.4624 Accuracy 0.8625\n",
            "Time taken for 1 epoch: 116.56 secs\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.3789 Accuracy 0.8899\n",
            "Epoch 49 Batch 50 Loss 0.4282 Accuracy 0.8740\n",
            "Epoch 49 Batch 100 Loss 0.4236 Accuracy 0.8736\n",
            "Epoch 49 Batch 150 Loss 0.4279 Accuracy 0.8730\n",
            "Epoch 49 Batch 200 Loss 0.4269 Accuracy 0.8733\n",
            "Epoch 49 Batch 250 Loss 0.4283 Accuracy 0.8726\n",
            "Epoch 49 Batch 300 Loss 0.4304 Accuracy 0.8723\n",
            "Epoch 49 Batch 350 Loss 0.4316 Accuracy 0.8717\n",
            "Epoch 49 Batch 400 Loss 0.4316 Accuracy 0.8717\n",
            "Epoch 49 Batch 450 Loss 0.4346 Accuracy 0.8709\n",
            "Epoch 49 Batch 500 Loss 0.4366 Accuracy 0.8705\n",
            "Epoch 49 Batch 550 Loss 0.4384 Accuracy 0.8699\n",
            "Epoch 49 Batch 600 Loss 0.4390 Accuracy 0.8697\n",
            "Epoch 49 Batch 650 Loss 0.4412 Accuracy 0.8690\n",
            "Epoch 49 Batch 700 Loss 0.4418 Accuracy 0.8688\n",
            "Epoch 49 Batch 750 Loss 0.4436 Accuracy 0.8681\n",
            "Epoch 49 Batch 800 Loss 0.4445 Accuracy 0.8679\n",
            "Epoch 49 Batch 850 Loss 0.4452 Accuracy 0.8678\n",
            "Epoch 49 Batch 900 Loss 0.4466 Accuracy 0.8674\n",
            "Epoch 49 Batch 950 Loss 0.4474 Accuracy 0.8671\n",
            "Epoch 49 Batch 1000 Loss 0.4484 Accuracy 0.8668\n",
            "Epoch 49 Batch 1050 Loss 0.4489 Accuracy 0.8666\n",
            "Epoch 49 Batch 1100 Loss 0.4500 Accuracy 0.8664\n",
            "Epoch 49 Batch 1150 Loss 0.4515 Accuracy 0.8659\n",
            "Epoch 49 Batch 1200 Loss 0.4522 Accuracy 0.8657\n",
            "Epoch 49 Loss 0.4529 Accuracy 0.8655\n",
            "Time taken for 1 epoch: 116.54 secs\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.4235 Accuracy 0.8689\n",
            "Epoch 50 Batch 50 Loss 0.4195 Accuracy 0.8752\n",
            "Epoch 50 Batch 100 Loss 0.4187 Accuracy 0.8748\n",
            "Epoch 50 Batch 150 Loss 0.4237 Accuracy 0.8733\n",
            "Epoch 50 Batch 200 Loss 0.4222 Accuracy 0.8738\n",
            "Epoch 50 Batch 250 Loss 0.4266 Accuracy 0.8727\n",
            "Epoch 50 Batch 300 Loss 0.4275 Accuracy 0.8724\n",
            "Epoch 50 Batch 350 Loss 0.4276 Accuracy 0.8723\n",
            "Epoch 50 Batch 400 Loss 0.4288 Accuracy 0.8719\n",
            "Epoch 50 Batch 450 Loss 0.4308 Accuracy 0.8713\n",
            "Epoch 50 Batch 500 Loss 0.4329 Accuracy 0.8708\n",
            "Epoch 50 Batch 550 Loss 0.4342 Accuracy 0.8704\n",
            "Epoch 50 Batch 600 Loss 0.4352 Accuracy 0.8702\n",
            "Epoch 50 Batch 650 Loss 0.4364 Accuracy 0.8699\n",
            "Epoch 50 Batch 700 Loss 0.4375 Accuracy 0.8696\n",
            "Epoch 50 Batch 750 Loss 0.4385 Accuracy 0.8692\n",
            "Epoch 50 Batch 800 Loss 0.4384 Accuracy 0.8692\n",
            "Epoch 50 Batch 850 Loss 0.4400 Accuracy 0.8688\n",
            "Epoch 50 Batch 900 Loss 0.4408 Accuracy 0.8686\n",
            "Epoch 50 Batch 950 Loss 0.4415 Accuracy 0.8683\n",
            "Epoch 50 Batch 1000 Loss 0.4417 Accuracy 0.8682\n",
            "Epoch 50 Batch 1050 Loss 0.4425 Accuracy 0.8680\n",
            "Epoch 50 Batch 1100 Loss 0.4436 Accuracy 0.8677\n",
            "Epoch 50 Batch 1150 Loss 0.4438 Accuracy 0.8677\n",
            "Epoch 50 Batch 1200 Loss 0.4446 Accuracy 0.8674\n",
            "Saving checkpoint for epoch 50 at ./checkpoints/train/ckpt-10\n",
            "Epoch 50 Loss 0.4454 Accuracy 0.8672\n",
            "Time taken for 1 epoch: 117.21 secs\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.4221 Accuracy 0.8707\n",
            "Epoch 51 Batch 50 Loss 0.4128 Accuracy 0.8760\n",
            "Epoch 51 Batch 100 Loss 0.4109 Accuracy 0.8770\n",
            "Epoch 51 Batch 150 Loss 0.4142 Accuracy 0.8761\n",
            "Epoch 51 Batch 200 Loss 0.4165 Accuracy 0.8753\n",
            "Epoch 51 Batch 250 Loss 0.4181 Accuracy 0.8751\n",
            "Epoch 51 Batch 300 Loss 0.4173 Accuracy 0.8752\n",
            "Epoch 51 Batch 350 Loss 0.4214 Accuracy 0.8741\n",
            "Epoch 51 Batch 400 Loss 0.4232 Accuracy 0.8734\n",
            "Epoch 51 Batch 450 Loss 0.4229 Accuracy 0.8734\n",
            "Epoch 51 Batch 500 Loss 0.4249 Accuracy 0.8727\n",
            "Epoch 51 Batch 550 Loss 0.4267 Accuracy 0.8721\n",
            "Epoch 51 Batch 600 Loss 0.4271 Accuracy 0.8720\n",
            "Epoch 51 Batch 650 Loss 0.4274 Accuracy 0.8720\n",
            "Epoch 51 Batch 700 Loss 0.4285 Accuracy 0.8717\n",
            "Epoch 51 Batch 750 Loss 0.4297 Accuracy 0.8716\n",
            "Epoch 51 Batch 800 Loss 0.4309 Accuracy 0.8713\n",
            "Epoch 51 Batch 850 Loss 0.4313 Accuracy 0.8712\n",
            "Epoch 51 Batch 900 Loss 0.4320 Accuracy 0.8710\n",
            "Epoch 51 Batch 950 Loss 0.4321 Accuracy 0.8709\n",
            "Epoch 51 Batch 1000 Loss 0.4331 Accuracy 0.8706\n",
            "Epoch 51 Batch 1050 Loss 0.4341 Accuracy 0.8703\n",
            "Epoch 51 Batch 1100 Loss 0.4354 Accuracy 0.8699\n",
            "Epoch 51 Batch 1150 Loss 0.4365 Accuracy 0.8695\n",
            "Epoch 51 Batch 1200 Loss 0.4379 Accuracy 0.8691\n",
            "Epoch 51 Loss 0.4387 Accuracy 0.8689\n",
            "Time taken for 1 epoch: 116.70 secs\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.4611 Accuracy 0.8601\n",
            "Epoch 52 Batch 50 Loss 0.4080 Accuracy 0.8772\n",
            "Epoch 52 Batch 100 Loss 0.4092 Accuracy 0.8765\n",
            "Epoch 52 Batch 150 Loss 0.4070 Accuracy 0.8777\n",
            "Epoch 52 Batch 200 Loss 0.4063 Accuracy 0.8775\n",
            "Epoch 52 Batch 250 Loss 0.4080 Accuracy 0.8772\n",
            "Epoch 52 Batch 300 Loss 0.4099 Accuracy 0.8762\n",
            "Epoch 52 Batch 350 Loss 0.4121 Accuracy 0.8757\n",
            "Epoch 52 Batch 400 Loss 0.4122 Accuracy 0.8756\n",
            "Epoch 52 Batch 450 Loss 0.4144 Accuracy 0.8750\n",
            "Epoch 52 Batch 500 Loss 0.4182 Accuracy 0.8740\n",
            "Epoch 52 Batch 550 Loss 0.4192 Accuracy 0.8738\n",
            "Epoch 52 Batch 600 Loss 0.4206 Accuracy 0.8734\n",
            "Epoch 52 Batch 650 Loss 0.4210 Accuracy 0.8734\n",
            "Epoch 52 Batch 700 Loss 0.4226 Accuracy 0.8731\n",
            "Epoch 52 Batch 750 Loss 0.4226 Accuracy 0.8731\n",
            "Epoch 52 Batch 800 Loss 0.4233 Accuracy 0.8730\n",
            "Epoch 52 Batch 850 Loss 0.4237 Accuracy 0.8728\n",
            "Epoch 52 Batch 900 Loss 0.4247 Accuracy 0.8725\n",
            "Epoch 52 Batch 950 Loss 0.4257 Accuracy 0.8722\n",
            "Epoch 52 Batch 1000 Loss 0.4263 Accuracy 0.8720\n",
            "Epoch 52 Batch 1050 Loss 0.4275 Accuracy 0.8717\n",
            "Epoch 52 Batch 1100 Loss 0.4286 Accuracy 0.8715\n",
            "Epoch 52 Batch 1150 Loss 0.4300 Accuracy 0.8712\n",
            "Epoch 52 Batch 1200 Loss 0.4308 Accuracy 0.8711\n",
            "Epoch 52 Loss 0.4323 Accuracy 0.8706\n",
            "Time taken for 1 epoch: 116.72 secs\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.3533 Accuracy 0.8999\n",
            "Epoch 53 Batch 50 Loss 0.3894 Accuracy 0.8826\n",
            "Epoch 53 Batch 100 Loss 0.3965 Accuracy 0.8805\n",
            "Epoch 53 Batch 150 Loss 0.4010 Accuracy 0.8795\n",
            "Epoch 53 Batch 200 Loss 0.4026 Accuracy 0.8791\n",
            "Epoch 53 Batch 250 Loss 0.4048 Accuracy 0.8781\n",
            "Epoch 53 Batch 300 Loss 0.4060 Accuracy 0.8777\n",
            "Epoch 53 Batch 350 Loss 0.4072 Accuracy 0.8773\n",
            "Epoch 53 Batch 400 Loss 0.4076 Accuracy 0.8773\n",
            "Epoch 53 Batch 450 Loss 0.4088 Accuracy 0.8769\n",
            "Epoch 53 Batch 500 Loss 0.4088 Accuracy 0.8770\n",
            "Epoch 53 Batch 550 Loss 0.4112 Accuracy 0.8764\n",
            "Epoch 53 Batch 600 Loss 0.4129 Accuracy 0.8759\n",
            "Epoch 53 Batch 650 Loss 0.4131 Accuracy 0.8758\n",
            "Epoch 53 Batch 700 Loss 0.4144 Accuracy 0.8754\n",
            "Epoch 53 Batch 750 Loss 0.4157 Accuracy 0.8750\n",
            "Epoch 53 Batch 800 Loss 0.4165 Accuracy 0.8749\n",
            "Epoch 53 Batch 850 Loss 0.4176 Accuracy 0.8746\n",
            "Epoch 53 Batch 900 Loss 0.4187 Accuracy 0.8743\n",
            "Epoch 53 Batch 950 Loss 0.4190 Accuracy 0.8742\n",
            "Epoch 53 Batch 1000 Loss 0.4196 Accuracy 0.8740\n",
            "Epoch 53 Batch 1050 Loss 0.4205 Accuracy 0.8737\n",
            "Epoch 53 Batch 1100 Loss 0.4215 Accuracy 0.8734\n",
            "Epoch 53 Batch 1150 Loss 0.4229 Accuracy 0.8730\n",
            "Epoch 53 Batch 1200 Loss 0.4242 Accuracy 0.8727\n",
            "Epoch 53 Loss 0.4250 Accuracy 0.8724\n",
            "Time taken for 1 epoch: 116.21 secs\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.3206 Accuracy 0.8955\n",
            "Epoch 54 Batch 50 Loss 0.4069 Accuracy 0.8768\n",
            "Epoch 54 Batch 100 Loss 0.4048 Accuracy 0.8772\n",
            "Epoch 54 Batch 150 Loss 0.4002 Accuracy 0.8784\n",
            "Epoch 54 Batch 200 Loss 0.4004 Accuracy 0.8785\n",
            "Epoch 54 Batch 250 Loss 0.4014 Accuracy 0.8791\n",
            "Epoch 54 Batch 300 Loss 0.4011 Accuracy 0.8790\n",
            "Epoch 54 Batch 350 Loss 0.4016 Accuracy 0.8791\n",
            "Epoch 54 Batch 400 Loss 0.4024 Accuracy 0.8791\n",
            "Epoch 54 Batch 450 Loss 0.4044 Accuracy 0.8786\n",
            "Epoch 54 Batch 500 Loss 0.4049 Accuracy 0.8783\n",
            "Epoch 54 Batch 550 Loss 0.4058 Accuracy 0.8781\n",
            "Epoch 54 Batch 600 Loss 0.4068 Accuracy 0.8779\n",
            "Epoch 54 Batch 650 Loss 0.4079 Accuracy 0.8775\n",
            "Epoch 54 Batch 700 Loss 0.4087 Accuracy 0.8772\n",
            "Epoch 54 Batch 750 Loss 0.4090 Accuracy 0.8772\n",
            "Epoch 54 Batch 800 Loss 0.4100 Accuracy 0.8770\n",
            "Epoch 54 Batch 850 Loss 0.4110 Accuracy 0.8766\n",
            "Epoch 54 Batch 900 Loss 0.4112 Accuracy 0.8766\n",
            "Epoch 54 Batch 950 Loss 0.4121 Accuracy 0.8763\n",
            "Epoch 54 Batch 1000 Loss 0.4132 Accuracy 0.8760\n",
            "Epoch 54 Batch 1050 Loss 0.4143 Accuracy 0.8757\n",
            "Epoch 54 Batch 1100 Loss 0.4155 Accuracy 0.8752\n",
            "Epoch 54 Batch 1150 Loss 0.4168 Accuracy 0.8749\n",
            "Epoch 54 Batch 1200 Loss 0.4183 Accuracy 0.8744\n",
            "Epoch 54 Loss 0.4193 Accuracy 0.8741\n",
            "Time taken for 1 epoch: 116.63 secs\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.4570 Accuracy 0.8750\n",
            "Epoch 55 Batch 50 Loss 0.3864 Accuracy 0.8836\n",
            "Epoch 55 Batch 100 Loss 0.3840 Accuracy 0.8836\n",
            "Epoch 55 Batch 150 Loss 0.3865 Accuracy 0.8834\n",
            "Epoch 55 Batch 200 Loss 0.3888 Accuracy 0.8823\n",
            "Epoch 55 Batch 250 Loss 0.3910 Accuracy 0.8819\n",
            "Epoch 55 Batch 300 Loss 0.3931 Accuracy 0.8806\n",
            "Epoch 55 Batch 350 Loss 0.3953 Accuracy 0.8801\n",
            "Epoch 55 Batch 400 Loss 0.3977 Accuracy 0.8795\n",
            "Epoch 55 Batch 450 Loss 0.3995 Accuracy 0.8791\n",
            "Epoch 55 Batch 500 Loss 0.4006 Accuracy 0.8787\n",
            "Epoch 55 Batch 550 Loss 0.4006 Accuracy 0.8788\n",
            "Epoch 55 Batch 600 Loss 0.4021 Accuracy 0.8784\n",
            "Epoch 55 Batch 650 Loss 0.4031 Accuracy 0.8781\n",
            "Epoch 55 Batch 700 Loss 0.4037 Accuracy 0.8779\n",
            "Epoch 55 Batch 750 Loss 0.4049 Accuracy 0.8777\n",
            "Epoch 55 Batch 800 Loss 0.4055 Accuracy 0.8775\n",
            "Epoch 55 Batch 850 Loss 0.4055 Accuracy 0.8775\n",
            "Epoch 55 Batch 900 Loss 0.4059 Accuracy 0.8773\n",
            "Epoch 55 Batch 950 Loss 0.4064 Accuracy 0.8772\n",
            "Epoch 55 Batch 1000 Loss 0.4076 Accuracy 0.8770\n",
            "Epoch 55 Batch 1050 Loss 0.4084 Accuracy 0.8768\n",
            "Epoch 55 Batch 1100 Loss 0.4093 Accuracy 0.8766\n",
            "Epoch 55 Batch 1150 Loss 0.4107 Accuracy 0.8761\n",
            "Epoch 55 Batch 1200 Loss 0.4121 Accuracy 0.8757\n",
            "Saving checkpoint for epoch 55 at ./checkpoints/train/ckpt-11\n",
            "Epoch 55 Loss 0.4134 Accuracy 0.8754\n",
            "Time taken for 1 epoch: 117.36 secs\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.4111 Accuracy 0.8704\n",
            "Epoch 56 Batch 50 Loss 0.3734 Accuracy 0.8878\n",
            "Epoch 56 Batch 100 Loss 0.3780 Accuracy 0.8857\n",
            "Epoch 56 Batch 150 Loss 0.3805 Accuracy 0.8852\n",
            "Epoch 56 Batch 200 Loss 0.3832 Accuracy 0.8843\n",
            "Epoch 56 Batch 250 Loss 0.3831 Accuracy 0.8843\n",
            "Epoch 56 Batch 300 Loss 0.3845 Accuracy 0.8839\n",
            "Epoch 56 Batch 350 Loss 0.3878 Accuracy 0.8829\n",
            "Epoch 56 Batch 400 Loss 0.3900 Accuracy 0.8824\n",
            "Epoch 56 Batch 450 Loss 0.3916 Accuracy 0.8818\n",
            "Epoch 56 Batch 500 Loss 0.3931 Accuracy 0.8815\n",
            "Epoch 56 Batch 550 Loss 0.3938 Accuracy 0.8813\n",
            "Epoch 56 Batch 600 Loss 0.3946 Accuracy 0.8812\n",
            "Epoch 56 Batch 650 Loss 0.3961 Accuracy 0.8808\n",
            "Epoch 56 Batch 700 Loss 0.3971 Accuracy 0.8806\n",
            "Epoch 56 Batch 750 Loss 0.3985 Accuracy 0.8802\n",
            "Epoch 56 Batch 800 Loss 0.3995 Accuracy 0.8799\n",
            "Epoch 56 Batch 850 Loss 0.3994 Accuracy 0.8798\n",
            "Epoch 56 Batch 900 Loss 0.3999 Accuracy 0.8797\n",
            "Epoch 56 Batch 950 Loss 0.4008 Accuracy 0.8795\n",
            "Epoch 56 Batch 1000 Loss 0.4019 Accuracy 0.8792\n",
            "Epoch 56 Batch 1050 Loss 0.4027 Accuracy 0.8789\n",
            "Epoch 56 Batch 1100 Loss 0.4036 Accuracy 0.8786\n",
            "Epoch 56 Batch 1150 Loss 0.4045 Accuracy 0.8784\n",
            "Epoch 56 Batch 1200 Loss 0.4054 Accuracy 0.8781\n",
            "Epoch 56 Loss 0.4063 Accuracy 0.8778\n",
            "Time taken for 1 epoch: 116.69 secs\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.3623 Accuracy 0.8824\n",
            "Epoch 57 Batch 50 Loss 0.3767 Accuracy 0.8857\n",
            "Epoch 57 Batch 100 Loss 0.3713 Accuracy 0.8866\n",
            "Epoch 57 Batch 150 Loss 0.3738 Accuracy 0.8868\n",
            "Epoch 57 Batch 200 Loss 0.3748 Accuracy 0.8867\n",
            "Epoch 57 Batch 250 Loss 0.3782 Accuracy 0.8859\n",
            "Epoch 57 Batch 300 Loss 0.3775 Accuracy 0.8859\n",
            "Epoch 57 Batch 350 Loss 0.3790 Accuracy 0.8855\n",
            "Epoch 57 Batch 400 Loss 0.3814 Accuracy 0.8847\n",
            "Epoch 57 Batch 450 Loss 0.3821 Accuracy 0.8847\n",
            "Epoch 57 Batch 500 Loss 0.3834 Accuracy 0.8842\n",
            "Epoch 57 Batch 550 Loss 0.3850 Accuracy 0.8838\n",
            "Epoch 57 Batch 600 Loss 0.3864 Accuracy 0.8833\n",
            "Epoch 57 Batch 650 Loss 0.3881 Accuracy 0.8828\n",
            "Epoch 57 Batch 700 Loss 0.3894 Accuracy 0.8824\n",
            "Epoch 57 Batch 750 Loss 0.3901 Accuracy 0.8821\n",
            "Epoch 57 Batch 800 Loss 0.3907 Accuracy 0.8820\n",
            "Epoch 57 Batch 850 Loss 0.3913 Accuracy 0.8818\n",
            "Epoch 57 Batch 900 Loss 0.3925 Accuracy 0.8814\n",
            "Epoch 57 Batch 950 Loss 0.3933 Accuracy 0.8812\n",
            "Epoch 57 Batch 1000 Loss 0.3942 Accuracy 0.8810\n",
            "Epoch 57 Batch 1050 Loss 0.3955 Accuracy 0.8807\n",
            "Epoch 57 Batch 1100 Loss 0.3963 Accuracy 0.8803\n",
            "Epoch 57 Batch 1150 Loss 0.3973 Accuracy 0.8801\n",
            "Epoch 57 Batch 1200 Loss 0.3986 Accuracy 0.8798\n",
            "Epoch 57 Loss 0.3999 Accuracy 0.8794\n",
            "Time taken for 1 epoch: 116.86 secs\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.3863 Accuracy 0.8779\n",
            "Epoch 58 Batch 50 Loss 0.3713 Accuracy 0.8877\n",
            "Epoch 58 Batch 100 Loss 0.3750 Accuracy 0.8868\n",
            "Epoch 58 Batch 150 Loss 0.3717 Accuracy 0.8878\n",
            "Epoch 58 Batch 200 Loss 0.3717 Accuracy 0.8876\n",
            "Epoch 58 Batch 250 Loss 0.3745 Accuracy 0.8867\n",
            "Epoch 58 Batch 300 Loss 0.3766 Accuracy 0.8859\n",
            "Epoch 58 Batch 350 Loss 0.3777 Accuracy 0.8853\n",
            "Epoch 58 Batch 400 Loss 0.3781 Accuracy 0.8853\n",
            "Epoch 58 Batch 450 Loss 0.3795 Accuracy 0.8851\n",
            "Epoch 58 Batch 500 Loss 0.3805 Accuracy 0.8848\n",
            "Epoch 58 Batch 550 Loss 0.3813 Accuracy 0.8846\n",
            "Epoch 58 Batch 600 Loss 0.3832 Accuracy 0.8842\n",
            "Epoch 58 Batch 650 Loss 0.3845 Accuracy 0.8840\n",
            "Epoch 58 Batch 700 Loss 0.3846 Accuracy 0.8839\n",
            "Epoch 58 Batch 750 Loss 0.3855 Accuracy 0.8837\n",
            "Epoch 58 Batch 800 Loss 0.3869 Accuracy 0.8833\n",
            "Epoch 58 Batch 850 Loss 0.3877 Accuracy 0.8831\n",
            "Epoch 58 Batch 900 Loss 0.3880 Accuracy 0.8830\n",
            "Epoch 58 Batch 950 Loss 0.3889 Accuracy 0.8827\n",
            "Epoch 58 Batch 1000 Loss 0.3895 Accuracy 0.8825\n",
            "Epoch 58 Batch 1050 Loss 0.3902 Accuracy 0.8823\n",
            "Epoch 58 Batch 1100 Loss 0.3913 Accuracy 0.8820\n",
            "Epoch 58 Batch 1150 Loss 0.3925 Accuracy 0.8817\n",
            "Epoch 58 Batch 1200 Loss 0.3941 Accuracy 0.8813\n",
            "Epoch 58 Loss 0.3952 Accuracy 0.8810\n",
            "Time taken for 1 epoch: 116.59 secs\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.4164 Accuracy 0.8810\n",
            "Epoch 59 Batch 50 Loss 0.3592 Accuracy 0.8917\n",
            "Epoch 59 Batch 100 Loss 0.3607 Accuracy 0.8915\n",
            "Epoch 59 Batch 150 Loss 0.3664 Accuracy 0.8902\n",
            "Epoch 59 Batch 200 Loss 0.3696 Accuracy 0.8893\n",
            "Epoch 59 Batch 250 Loss 0.3692 Accuracy 0.8892\n",
            "Epoch 59 Batch 300 Loss 0.3699 Accuracy 0.8887\n",
            "Epoch 59 Batch 350 Loss 0.3710 Accuracy 0.8883\n",
            "Epoch 59 Batch 400 Loss 0.3738 Accuracy 0.8873\n",
            "Epoch 59 Batch 450 Loss 0.3753 Accuracy 0.8869\n",
            "Epoch 59 Batch 500 Loss 0.3768 Accuracy 0.8866\n",
            "Epoch 59 Batch 550 Loss 0.3779 Accuracy 0.8861\n",
            "Epoch 59 Batch 600 Loss 0.3787 Accuracy 0.8859\n",
            "Epoch 59 Batch 650 Loss 0.3799 Accuracy 0.8855\n",
            "Epoch 59 Batch 700 Loss 0.3807 Accuracy 0.8853\n",
            "Epoch 59 Batch 750 Loss 0.3821 Accuracy 0.8848\n",
            "Epoch 59 Batch 800 Loss 0.3823 Accuracy 0.8847\n",
            "Epoch 59 Batch 850 Loss 0.3827 Accuracy 0.8845\n",
            "Epoch 59 Batch 900 Loss 0.3832 Accuracy 0.8843\n",
            "Epoch 59 Batch 950 Loss 0.3841 Accuracy 0.8840\n",
            "Epoch 59 Batch 1000 Loss 0.3851 Accuracy 0.8837\n",
            "Epoch 59 Batch 1050 Loss 0.3857 Accuracy 0.8835\n",
            "Epoch 59 Batch 1100 Loss 0.3869 Accuracy 0.8832\n",
            "Epoch 59 Batch 1150 Loss 0.3877 Accuracy 0.8830\n",
            "Epoch 59 Batch 1200 Loss 0.3891 Accuracy 0.8825\n",
            "Epoch 59 Loss 0.3901 Accuracy 0.8823\n",
            "Time taken for 1 epoch: 116.84 secs\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.3654 Accuracy 0.9022\n",
            "Epoch 60 Batch 50 Loss 0.3533 Accuracy 0.8914\n",
            "Epoch 60 Batch 100 Loss 0.3555 Accuracy 0.8912\n",
            "Epoch 60 Batch 150 Loss 0.3589 Accuracy 0.8907\n",
            "Epoch 60 Batch 200 Loss 0.3596 Accuracy 0.8904\n",
            "Epoch 60 Batch 250 Loss 0.3610 Accuracy 0.8895\n",
            "Epoch 60 Batch 300 Loss 0.3622 Accuracy 0.8890\n",
            "Epoch 60 Batch 350 Loss 0.3641 Accuracy 0.8888\n",
            "Epoch 60 Batch 400 Loss 0.3663 Accuracy 0.8882\n",
            "Epoch 60 Batch 450 Loss 0.3684 Accuracy 0.8877\n",
            "Epoch 60 Batch 500 Loss 0.3700 Accuracy 0.8874\n",
            "Epoch 60 Batch 550 Loss 0.3712 Accuracy 0.8871\n",
            "Epoch 60 Batch 600 Loss 0.3717 Accuracy 0.8870\n",
            "Epoch 60 Batch 650 Loss 0.3731 Accuracy 0.8866\n",
            "Epoch 60 Batch 700 Loss 0.3739 Accuracy 0.8865\n",
            "Epoch 60 Batch 750 Loss 0.3745 Accuracy 0.8864\n",
            "Epoch 60 Batch 800 Loss 0.3754 Accuracy 0.8861\n",
            "Epoch 60 Batch 850 Loss 0.3761 Accuracy 0.8860\n",
            "Epoch 60 Batch 900 Loss 0.3769 Accuracy 0.8858\n",
            "Epoch 60 Batch 950 Loss 0.3774 Accuracy 0.8856\n",
            "Epoch 60 Batch 1000 Loss 0.3782 Accuracy 0.8853\n",
            "Epoch 60 Batch 1050 Loss 0.3795 Accuracy 0.8850\n",
            "Epoch 60 Batch 1100 Loss 0.3801 Accuracy 0.8848\n",
            "Epoch 60 Batch 1150 Loss 0.3810 Accuracy 0.8846\n",
            "Epoch 60 Batch 1200 Loss 0.3822 Accuracy 0.8842\n",
            "Saving checkpoint for epoch 60 at ./checkpoints/train/ckpt-12\n",
            "Epoch 60 Loss 0.3829 Accuracy 0.8840\n",
            "Time taken for 1 epoch: 117.73 secs\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.3431 Accuracy 0.8844\n",
            "Epoch 61 Batch 50 Loss 0.3521 Accuracy 0.8924\n",
            "Epoch 61 Batch 100 Loss 0.3517 Accuracy 0.8930\n",
            "Epoch 61 Batch 150 Loss 0.3546 Accuracy 0.8923\n",
            "Epoch 61 Batch 200 Loss 0.3568 Accuracy 0.8917\n",
            "Epoch 61 Batch 250 Loss 0.3569 Accuracy 0.8917\n",
            "Epoch 61 Batch 300 Loss 0.3590 Accuracy 0.8911\n",
            "Epoch 61 Batch 350 Loss 0.3607 Accuracy 0.8905\n",
            "Epoch 61 Batch 400 Loss 0.3632 Accuracy 0.8896\n",
            "Epoch 61 Batch 450 Loss 0.3646 Accuracy 0.8892\n",
            "Epoch 61 Batch 500 Loss 0.3656 Accuracy 0.8889\n",
            "Epoch 61 Batch 550 Loss 0.3668 Accuracy 0.8886\n",
            "Epoch 61 Batch 600 Loss 0.3679 Accuracy 0.8883\n",
            "Epoch 61 Batch 650 Loss 0.3689 Accuracy 0.8880\n",
            "Epoch 61 Batch 700 Loss 0.3695 Accuracy 0.8878\n",
            "Epoch 61 Batch 750 Loss 0.3701 Accuracy 0.8877\n",
            "Epoch 61 Batch 800 Loss 0.3714 Accuracy 0.8873\n",
            "Epoch 61 Batch 850 Loss 0.3715 Accuracy 0.8873\n",
            "Epoch 61 Batch 900 Loss 0.3724 Accuracy 0.8871\n",
            "Epoch 61 Batch 950 Loss 0.3731 Accuracy 0.8869\n",
            "Epoch 61 Batch 1000 Loss 0.3741 Accuracy 0.8866\n",
            "Epoch 61 Batch 1050 Loss 0.3751 Accuracy 0.8863\n",
            "Epoch 61 Batch 1100 Loss 0.3752 Accuracy 0.8863\n",
            "Epoch 61 Batch 1150 Loss 0.3759 Accuracy 0.8861\n",
            "Epoch 61 Batch 1200 Loss 0.3767 Accuracy 0.8859\n",
            "Epoch 61 Loss 0.3779 Accuracy 0.8855\n",
            "Time taken for 1 epoch: 116.68 secs\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.4471 Accuracy 0.8794\n",
            "Epoch 62 Batch 50 Loss 0.3515 Accuracy 0.8928\n",
            "Epoch 62 Batch 100 Loss 0.3507 Accuracy 0.8929\n",
            "Epoch 62 Batch 150 Loss 0.3493 Accuracy 0.8934\n",
            "Epoch 62 Batch 200 Loss 0.3529 Accuracy 0.8923\n",
            "Epoch 62 Batch 250 Loss 0.3552 Accuracy 0.8917\n",
            "Epoch 62 Batch 300 Loss 0.3561 Accuracy 0.8915\n",
            "Epoch 62 Batch 350 Loss 0.3578 Accuracy 0.8910\n",
            "Epoch 62 Batch 400 Loss 0.3593 Accuracy 0.8906\n",
            "Epoch 62 Batch 450 Loss 0.3605 Accuracy 0.8905\n",
            "Epoch 62 Batch 500 Loss 0.3621 Accuracy 0.8900\n",
            "Epoch 62 Batch 550 Loss 0.3636 Accuracy 0.8895\n",
            "Epoch 62 Batch 600 Loss 0.3638 Accuracy 0.8893\n",
            "Epoch 62 Batch 650 Loss 0.3642 Accuracy 0.8892\n",
            "Epoch 62 Batch 700 Loss 0.3654 Accuracy 0.8889\n",
            "Epoch 62 Batch 750 Loss 0.3658 Accuracy 0.8887\n",
            "Epoch 62 Batch 800 Loss 0.3664 Accuracy 0.8885\n",
            "Epoch 62 Batch 850 Loss 0.3667 Accuracy 0.8884\n",
            "Epoch 62 Batch 900 Loss 0.3672 Accuracy 0.8882\n",
            "Epoch 62 Batch 950 Loss 0.3685 Accuracy 0.8879\n",
            "Epoch 62 Batch 1000 Loss 0.3688 Accuracy 0.8877\n",
            "Epoch 62 Batch 1050 Loss 0.3698 Accuracy 0.8874\n",
            "Epoch 62 Batch 1100 Loss 0.3705 Accuracy 0.8873\n",
            "Epoch 62 Batch 1150 Loss 0.3714 Accuracy 0.8870\n",
            "Epoch 62 Batch 1200 Loss 0.3727 Accuracy 0.8867\n",
            "Epoch 62 Loss 0.3735 Accuracy 0.8865\n",
            "Time taken for 1 epoch: 116.50 secs\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.4374 Accuracy 0.8639\n",
            "Epoch 63 Batch 50 Loss 0.3405 Accuracy 0.8963\n",
            "Epoch 63 Batch 100 Loss 0.3426 Accuracy 0.8955\n",
            "Epoch 63 Batch 150 Loss 0.3453 Accuracy 0.8953\n",
            "Epoch 63 Batch 200 Loss 0.3463 Accuracy 0.8941\n",
            "Epoch 63 Batch 250 Loss 0.3466 Accuracy 0.8940\n",
            "Epoch 63 Batch 300 Loss 0.3498 Accuracy 0.8936\n",
            "Epoch 63 Batch 350 Loss 0.3512 Accuracy 0.8931\n",
            "Epoch 63 Batch 400 Loss 0.3527 Accuracy 0.8929\n",
            "Epoch 63 Batch 450 Loss 0.3540 Accuracy 0.8925\n",
            "Epoch 63 Batch 500 Loss 0.3547 Accuracy 0.8923\n",
            "Epoch 63 Batch 550 Loss 0.3562 Accuracy 0.8919\n",
            "Epoch 63 Batch 600 Loss 0.3574 Accuracy 0.8916\n",
            "Epoch 63 Batch 650 Loss 0.3589 Accuracy 0.8911\n",
            "Epoch 63 Batch 700 Loss 0.3599 Accuracy 0.8908\n",
            "Epoch 63 Batch 750 Loss 0.3604 Accuracy 0.8906\n",
            "Epoch 63 Batch 800 Loss 0.3612 Accuracy 0.8904\n",
            "Epoch 63 Batch 850 Loss 0.3617 Accuracy 0.8903\n",
            "Epoch 63 Batch 900 Loss 0.3619 Accuracy 0.8903\n",
            "Epoch 63 Batch 950 Loss 0.3630 Accuracy 0.8900\n",
            "Epoch 63 Batch 1000 Loss 0.3637 Accuracy 0.8898\n",
            "Epoch 63 Batch 1050 Loss 0.3644 Accuracy 0.8896\n",
            "Epoch 63 Batch 1100 Loss 0.3655 Accuracy 0.8892\n",
            "Epoch 63 Batch 1150 Loss 0.3665 Accuracy 0.8890\n",
            "Epoch 63 Batch 1200 Loss 0.3672 Accuracy 0.8888\n",
            "Epoch 63 Loss 0.3683 Accuracy 0.8885\n",
            "Time taken for 1 epoch: 116.77 secs\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.3150 Accuracy 0.9011\n",
            "Epoch 64 Batch 50 Loss 0.3467 Accuracy 0.8955\n",
            "Epoch 64 Batch 100 Loss 0.3422 Accuracy 0.8966\n",
            "Epoch 64 Batch 150 Loss 0.3385 Accuracy 0.8966\n",
            "Epoch 64 Batch 200 Loss 0.3419 Accuracy 0.8953\n",
            "Epoch 64 Batch 250 Loss 0.3438 Accuracy 0.8951\n",
            "Epoch 64 Batch 300 Loss 0.3435 Accuracy 0.8953\n",
            "Epoch 64 Batch 350 Loss 0.3460 Accuracy 0.8945\n",
            "Epoch 64 Batch 400 Loss 0.3479 Accuracy 0.8940\n",
            "Epoch 64 Batch 450 Loss 0.3486 Accuracy 0.8939\n",
            "Epoch 64 Batch 500 Loss 0.3502 Accuracy 0.8935\n",
            "Epoch 64 Batch 550 Loss 0.3512 Accuracy 0.8931\n",
            "Epoch 64 Batch 600 Loss 0.3524 Accuracy 0.8927\n",
            "Epoch 64 Batch 650 Loss 0.3533 Accuracy 0.8924\n",
            "Epoch 64 Batch 700 Loss 0.3543 Accuracy 0.8920\n",
            "Epoch 64 Batch 750 Loss 0.3553 Accuracy 0.8916\n",
            "Epoch 64 Batch 800 Loss 0.3557 Accuracy 0.8916\n",
            "Epoch 64 Batch 850 Loss 0.3569 Accuracy 0.8912\n",
            "Epoch 64 Batch 900 Loss 0.3576 Accuracy 0.8911\n",
            "Epoch 64 Batch 950 Loss 0.3585 Accuracy 0.8909\n",
            "Epoch 64 Batch 1000 Loss 0.3594 Accuracy 0.8906\n",
            "Epoch 64 Batch 1050 Loss 0.3605 Accuracy 0.8902\n",
            "Epoch 64 Batch 1100 Loss 0.3611 Accuracy 0.8901\n",
            "Epoch 64 Batch 1150 Loss 0.3618 Accuracy 0.8899\n",
            "Epoch 64 Batch 1200 Loss 0.3628 Accuracy 0.8896\n",
            "Epoch 64 Loss 0.3639 Accuracy 0.8893\n",
            "Time taken for 1 epoch: 116.61 secs\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.3645 Accuracy 0.8901\n",
            "Epoch 65 Batch 50 Loss 0.3394 Accuracy 0.8983\n",
            "Epoch 65 Batch 100 Loss 0.3396 Accuracy 0.8983\n",
            "Epoch 65 Batch 150 Loss 0.3385 Accuracy 0.8979\n",
            "Epoch 65 Batch 200 Loss 0.3394 Accuracy 0.8974\n",
            "Epoch 65 Batch 250 Loss 0.3397 Accuracy 0.8967\n",
            "Epoch 65 Batch 300 Loss 0.3414 Accuracy 0.8963\n",
            "Epoch 65 Batch 350 Loss 0.3421 Accuracy 0.8960\n",
            "Epoch 65 Batch 400 Loss 0.3436 Accuracy 0.8957\n",
            "Epoch 65 Batch 450 Loss 0.3449 Accuracy 0.8954\n",
            "Epoch 65 Batch 500 Loss 0.3464 Accuracy 0.8949\n",
            "Epoch 65 Batch 550 Loss 0.3466 Accuracy 0.8948\n",
            "Epoch 65 Batch 600 Loss 0.3483 Accuracy 0.8944\n",
            "Epoch 65 Batch 650 Loss 0.3491 Accuracy 0.8941\n",
            "Epoch 65 Batch 700 Loss 0.3501 Accuracy 0.8938\n",
            "Epoch 65 Batch 750 Loss 0.3508 Accuracy 0.8935\n",
            "Epoch 65 Batch 800 Loss 0.3514 Accuracy 0.8933\n",
            "Epoch 65 Batch 850 Loss 0.3523 Accuracy 0.8930\n",
            "Epoch 65 Batch 900 Loss 0.3530 Accuracy 0.8928\n",
            "Epoch 65 Batch 950 Loss 0.3530 Accuracy 0.8928\n",
            "Epoch 65 Batch 1000 Loss 0.3540 Accuracy 0.8925\n",
            "Epoch 65 Batch 1050 Loss 0.3548 Accuracy 0.8923\n",
            "Epoch 65 Batch 1100 Loss 0.3553 Accuracy 0.8921\n",
            "Epoch 65 Batch 1150 Loss 0.3567 Accuracy 0.8917\n",
            "Epoch 65 Batch 1200 Loss 0.3579 Accuracy 0.8914\n",
            "Saving checkpoint for epoch 65 at ./checkpoints/train/ckpt-13\n",
            "Epoch 65 Loss 0.3584 Accuracy 0.8912\n",
            "Time taken for 1 epoch: 117.57 secs\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.3013 Accuracy 0.8967\n",
            "Epoch 66 Batch 50 Loss 0.3271 Accuracy 0.8987\n",
            "Epoch 66 Batch 100 Loss 0.3278 Accuracy 0.8991\n",
            "Epoch 66 Batch 150 Loss 0.3304 Accuracy 0.8982\n",
            "Epoch 66 Batch 200 Loss 0.3330 Accuracy 0.8980\n",
            "Epoch 66 Batch 250 Loss 0.3347 Accuracy 0.8972\n",
            "Epoch 66 Batch 300 Loss 0.3357 Accuracy 0.8967\n",
            "Epoch 66 Batch 350 Loss 0.3367 Accuracy 0.8964\n",
            "Epoch 66 Batch 400 Loss 0.3392 Accuracy 0.8956\n",
            "Epoch 66 Batch 450 Loss 0.3401 Accuracy 0.8953\n",
            "Epoch 66 Batch 500 Loss 0.3424 Accuracy 0.8948\n",
            "Epoch 66 Batch 550 Loss 0.3428 Accuracy 0.8948\n",
            "Epoch 66 Batch 600 Loss 0.3435 Accuracy 0.8948\n",
            "Epoch 66 Batch 650 Loss 0.3449 Accuracy 0.8945\n",
            "Epoch 66 Batch 700 Loss 0.3457 Accuracy 0.8944\n",
            "Epoch 66 Batch 750 Loss 0.3467 Accuracy 0.8941\n",
            "Epoch 66 Batch 800 Loss 0.3474 Accuracy 0.8939\n",
            "Epoch 66 Batch 850 Loss 0.3478 Accuracy 0.8939\n",
            "Epoch 66 Batch 900 Loss 0.3487 Accuracy 0.8937\n",
            "Epoch 66 Batch 950 Loss 0.3494 Accuracy 0.8935\n",
            "Epoch 66 Batch 1000 Loss 0.3496 Accuracy 0.8934\n",
            "Epoch 66 Batch 1050 Loss 0.3503 Accuracy 0.8932\n",
            "Epoch 66 Batch 1100 Loss 0.3513 Accuracy 0.8929\n",
            "Epoch 66 Batch 1150 Loss 0.3521 Accuracy 0.8927\n",
            "Epoch 66 Batch 1200 Loss 0.3527 Accuracy 0.8925\n",
            "Epoch 66 Loss 0.3538 Accuracy 0.8921\n",
            "Time taken for 1 epoch: 116.67 secs\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.3608 Accuracy 0.8757\n",
            "Epoch 67 Batch 50 Loss 0.3353 Accuracy 0.8972\n",
            "Epoch 67 Batch 100 Loss 0.3322 Accuracy 0.8978\n",
            "Epoch 67 Batch 150 Loss 0.3324 Accuracy 0.8987\n",
            "Epoch 67 Batch 200 Loss 0.3310 Accuracy 0.8995\n",
            "Epoch 67 Batch 250 Loss 0.3323 Accuracy 0.8992\n",
            "Epoch 67 Batch 300 Loss 0.3317 Accuracy 0.8994\n",
            "Epoch 67 Batch 350 Loss 0.3329 Accuracy 0.8989\n",
            "Epoch 67 Batch 400 Loss 0.3342 Accuracy 0.8985\n",
            "Epoch 67 Batch 450 Loss 0.3360 Accuracy 0.8977\n",
            "Epoch 67 Batch 500 Loss 0.3372 Accuracy 0.8974\n",
            "Epoch 67 Batch 550 Loss 0.3383 Accuracy 0.8970\n",
            "Epoch 67 Batch 600 Loss 0.3396 Accuracy 0.8965\n",
            "Epoch 67 Batch 650 Loss 0.3413 Accuracy 0.8960\n",
            "Epoch 67 Batch 700 Loss 0.3417 Accuracy 0.8959\n",
            "Epoch 67 Batch 750 Loss 0.3424 Accuracy 0.8957\n",
            "Epoch 67 Batch 800 Loss 0.3432 Accuracy 0.8954\n",
            "Epoch 67 Batch 850 Loss 0.3441 Accuracy 0.8951\n",
            "Epoch 67 Batch 900 Loss 0.3452 Accuracy 0.8948\n",
            "Epoch 67 Batch 950 Loss 0.3464 Accuracy 0.8944\n",
            "Epoch 67 Batch 1000 Loss 0.3471 Accuracy 0.8943\n",
            "Epoch 67 Batch 1050 Loss 0.3479 Accuracy 0.8941\n",
            "Epoch 67 Batch 1100 Loss 0.3487 Accuracy 0.8938\n",
            "Epoch 67 Batch 1150 Loss 0.3493 Accuracy 0.8937\n",
            "Epoch 67 Batch 1200 Loss 0.3499 Accuracy 0.8935\n",
            "Epoch 67 Loss 0.3508 Accuracy 0.8932\n",
            "Time taken for 1 epoch: 116.70 secs\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.3267 Accuracy 0.9024\n",
            "Epoch 68 Batch 50 Loss 0.3185 Accuracy 0.9019\n",
            "Epoch 68 Batch 100 Loss 0.3221 Accuracy 0.9004\n",
            "Epoch 68 Batch 150 Loss 0.3246 Accuracy 0.9001\n",
            "Epoch 68 Batch 200 Loss 0.3263 Accuracy 0.8998\n",
            "Epoch 68 Batch 250 Loss 0.3297 Accuracy 0.8986\n",
            "Epoch 68 Batch 300 Loss 0.3312 Accuracy 0.8983\n",
            "Epoch 68 Batch 350 Loss 0.3322 Accuracy 0.8981\n",
            "Epoch 68 Batch 400 Loss 0.3327 Accuracy 0.8981\n",
            "Epoch 68 Batch 450 Loss 0.3338 Accuracy 0.8978\n",
            "Epoch 68 Batch 500 Loss 0.3353 Accuracy 0.8974\n",
            "Epoch 68 Batch 550 Loss 0.3363 Accuracy 0.8971\n",
            "Epoch 68 Batch 600 Loss 0.3367 Accuracy 0.8969\n",
            "Epoch 68 Batch 650 Loss 0.3376 Accuracy 0.8968\n",
            "Epoch 68 Batch 700 Loss 0.3389 Accuracy 0.8963\n",
            "Epoch 68 Batch 750 Loss 0.3393 Accuracy 0.8963\n",
            "Epoch 68 Batch 800 Loss 0.3402 Accuracy 0.8960\n",
            "Epoch 68 Batch 850 Loss 0.3404 Accuracy 0.8960\n",
            "Epoch 68 Batch 900 Loss 0.3412 Accuracy 0.8959\n",
            "Epoch 68 Batch 950 Loss 0.3421 Accuracy 0.8955\n",
            "Epoch 68 Batch 1000 Loss 0.3424 Accuracy 0.8955\n",
            "Epoch 68 Batch 1050 Loss 0.3436 Accuracy 0.8951\n",
            "Epoch 68 Batch 1100 Loss 0.3441 Accuracy 0.8950\n",
            "Epoch 68 Batch 1150 Loss 0.3447 Accuracy 0.8948\n",
            "Epoch 68 Batch 1200 Loss 0.3458 Accuracy 0.8945\n",
            "Epoch 68 Loss 0.3467 Accuracy 0.8943\n",
            "Time taken for 1 epoch: 128.55 secs\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.3625 Accuracy 0.8968\n",
            "Epoch 69 Batch 50 Loss 0.3286 Accuracy 0.9013\n",
            "Epoch 69 Batch 100 Loss 0.3266 Accuracy 0.9012\n",
            "Epoch 69 Batch 150 Loss 0.3258 Accuracy 0.9011\n",
            "Epoch 69 Batch 200 Loss 0.3242 Accuracy 0.9013\n",
            "Epoch 69 Batch 250 Loss 0.3227 Accuracy 0.9019\n",
            "Epoch 69 Batch 300 Loss 0.3243 Accuracy 0.9015\n",
            "Epoch 69 Batch 350 Loss 0.3261 Accuracy 0.9010\n",
            "Epoch 69 Batch 400 Loss 0.3275 Accuracy 0.9004\n",
            "Epoch 69 Batch 450 Loss 0.3283 Accuracy 0.9002\n",
            "Epoch 69 Batch 500 Loss 0.3285 Accuracy 0.9001\n",
            "Epoch 69 Batch 550 Loss 0.3297 Accuracy 0.8998\n",
            "Epoch 69 Batch 600 Loss 0.3304 Accuracy 0.8997\n",
            "Epoch 69 Batch 650 Loss 0.3306 Accuracy 0.8995\n",
            "Epoch 69 Batch 700 Loss 0.3314 Accuracy 0.8992\n",
            "Epoch 69 Batch 750 Loss 0.3323 Accuracy 0.8989\n",
            "Epoch 69 Batch 800 Loss 0.3328 Accuracy 0.8987\n",
            "Epoch 69 Batch 850 Loss 0.3330 Accuracy 0.8985\n",
            "Epoch 69 Batch 900 Loss 0.3346 Accuracy 0.8981\n",
            "Epoch 69 Batch 950 Loss 0.3350 Accuracy 0.8979\n",
            "Epoch 69 Batch 1000 Loss 0.3362 Accuracy 0.8975\n",
            "Epoch 69 Batch 1050 Loss 0.3372 Accuracy 0.8972\n",
            "Epoch 69 Batch 1100 Loss 0.3378 Accuracy 0.8970\n",
            "Epoch 69 Batch 1150 Loss 0.3391 Accuracy 0.8967\n",
            "Epoch 69 Batch 1200 Loss 0.3401 Accuracy 0.8964\n",
            "Epoch 69 Loss 0.3408 Accuracy 0.8962\n",
            "Time taken for 1 epoch: 130.86 secs\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.2729 Accuracy 0.9208\n",
            "Epoch 70 Batch 50 Loss 0.3238 Accuracy 0.9003\n",
            "Epoch 70 Batch 100 Loss 0.3167 Accuracy 0.9027\n",
            "Epoch 70 Batch 150 Loss 0.3183 Accuracy 0.9023\n",
            "Epoch 70 Batch 200 Loss 0.3173 Accuracy 0.9024\n",
            "Epoch 70 Batch 250 Loss 0.3183 Accuracy 0.9019\n",
            "Epoch 70 Batch 300 Loss 0.3198 Accuracy 0.9015\n",
            "Epoch 70 Batch 350 Loss 0.3216 Accuracy 0.9011\n",
            "Epoch 70 Batch 400 Loss 0.3235 Accuracy 0.9005\n",
            "Epoch 70 Batch 450 Loss 0.3255 Accuracy 0.8999\n",
            "Epoch 70 Batch 500 Loss 0.3263 Accuracy 0.8998\n",
            "Epoch 70 Batch 550 Loss 0.3271 Accuracy 0.8996\n",
            "Epoch 70 Batch 600 Loss 0.3272 Accuracy 0.8995\n",
            "Epoch 70 Batch 650 Loss 0.3284 Accuracy 0.8994\n",
            "Epoch 70 Batch 700 Loss 0.3287 Accuracy 0.8993\n",
            "Epoch 70 Batch 750 Loss 0.3297 Accuracy 0.8992\n",
            "Epoch 70 Batch 800 Loss 0.3306 Accuracy 0.8989\n",
            "Epoch 70 Batch 850 Loss 0.3316 Accuracy 0.8986\n",
            "Epoch 70 Batch 900 Loss 0.3318 Accuracy 0.8986\n",
            "Epoch 70 Batch 950 Loss 0.3325 Accuracy 0.8984\n",
            "Epoch 70 Batch 1000 Loss 0.3332 Accuracy 0.8982\n",
            "Epoch 70 Batch 1050 Loss 0.3336 Accuracy 0.8980\n",
            "Epoch 70 Batch 1100 Loss 0.3342 Accuracy 0.8979\n",
            "Epoch 70 Batch 1150 Loss 0.3351 Accuracy 0.8976\n",
            "Epoch 70 Batch 1200 Loss 0.3363 Accuracy 0.8973\n",
            "Saving checkpoint for epoch 70 at ./checkpoints/train/ckpt-14\n",
            "Epoch 70 Loss 0.3377 Accuracy 0.8969\n",
            "Time taken for 1 epoch: 136.54 secs\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.3212 Accuracy 0.9024\n",
            "Epoch 71 Batch 50 Loss 0.3141 Accuracy 0.9043\n",
            "Epoch 71 Batch 100 Loss 0.3105 Accuracy 0.9049\n",
            "Epoch 71 Batch 150 Loss 0.3121 Accuracy 0.9040\n",
            "Epoch 71 Batch 200 Loss 0.3126 Accuracy 0.9040\n",
            "Epoch 71 Batch 250 Loss 0.3147 Accuracy 0.9036\n",
            "Epoch 71 Batch 300 Loss 0.3166 Accuracy 0.9031\n",
            "Epoch 71 Batch 350 Loss 0.3177 Accuracy 0.9028\n",
            "Epoch 71 Batch 400 Loss 0.3189 Accuracy 0.9024\n",
            "Epoch 71 Batch 450 Loss 0.3195 Accuracy 0.9024\n",
            "Epoch 71 Batch 500 Loss 0.3200 Accuracy 0.9021\n",
            "Epoch 71 Batch 550 Loss 0.3214 Accuracy 0.9016\n",
            "Epoch 71 Batch 600 Loss 0.3226 Accuracy 0.9012\n",
            "Epoch 71 Batch 650 Loss 0.3239 Accuracy 0.9007\n",
            "Epoch 71 Batch 700 Loss 0.3251 Accuracy 0.9005\n",
            "Epoch 71 Batch 750 Loss 0.3258 Accuracy 0.9004\n",
            "Epoch 71 Batch 800 Loss 0.3264 Accuracy 0.9003\n",
            "Epoch 71 Batch 850 Loss 0.3269 Accuracy 0.9001\n",
            "Epoch 71 Batch 900 Loss 0.3274 Accuracy 0.8998\n",
            "Epoch 71 Batch 950 Loss 0.3279 Accuracy 0.8998\n",
            "Epoch 71 Batch 1000 Loss 0.3290 Accuracy 0.8994\n",
            "Epoch 71 Batch 1050 Loss 0.3297 Accuracy 0.8991\n",
            "Epoch 71 Batch 1100 Loss 0.3307 Accuracy 0.8989\n",
            "Epoch 71 Batch 1150 Loss 0.3319 Accuracy 0.8986\n",
            "Epoch 71 Batch 1200 Loss 0.3328 Accuracy 0.8983\n",
            "Epoch 71 Loss 0.3340 Accuracy 0.8980\n",
            "Time taken for 1 epoch: 123.75 secs\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.2796 Accuracy 0.9150\n",
            "Epoch 72 Batch 50 Loss 0.3049 Accuracy 0.9073\n",
            "Epoch 72 Batch 100 Loss 0.3115 Accuracy 0.9055\n",
            "Epoch 72 Batch 150 Loss 0.3092 Accuracy 0.9061\n",
            "Epoch 72 Batch 200 Loss 0.3137 Accuracy 0.9045\n",
            "Epoch 72 Batch 250 Loss 0.3149 Accuracy 0.9040\n",
            "Epoch 72 Batch 300 Loss 0.3170 Accuracy 0.9037\n",
            "Epoch 72 Batch 350 Loss 0.3171 Accuracy 0.9035\n",
            "Epoch 72 Batch 400 Loss 0.3187 Accuracy 0.9031\n",
            "Epoch 72 Batch 450 Loss 0.3201 Accuracy 0.9026\n",
            "Epoch 72 Batch 500 Loss 0.3214 Accuracy 0.9022\n",
            "Epoch 72 Batch 550 Loss 0.3221 Accuracy 0.9018\n",
            "Epoch 72 Batch 600 Loss 0.3224 Accuracy 0.9017\n",
            "Epoch 72 Batch 650 Loss 0.3229 Accuracy 0.9015\n",
            "Epoch 72 Batch 700 Loss 0.3238 Accuracy 0.9013\n",
            "Epoch 72 Batch 750 Loss 0.3247 Accuracy 0.9010\n",
            "Epoch 72 Batch 800 Loss 0.3246 Accuracy 0.9009\n",
            "Epoch 72 Batch 850 Loss 0.3253 Accuracy 0.9007\n",
            "Epoch 72 Batch 900 Loss 0.3260 Accuracy 0.9006\n",
            "Epoch 72 Batch 950 Loss 0.3261 Accuracy 0.9006\n",
            "Epoch 72 Batch 1000 Loss 0.3268 Accuracy 0.9005\n",
            "Epoch 72 Batch 1050 Loss 0.3274 Accuracy 0.9003\n",
            "Epoch 72 Batch 1100 Loss 0.3288 Accuracy 0.9000\n",
            "Epoch 72 Batch 1150 Loss 0.3298 Accuracy 0.8996\n",
            "Epoch 72 Batch 1200 Loss 0.3307 Accuracy 0.8993\n",
            "Epoch 72 Loss 0.3312 Accuracy 0.8992\n",
            "Time taken for 1 epoch: 145.20 secs\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.3164 Accuracy 0.9086\n",
            "Epoch 73 Batch 50 Loss 0.3035 Accuracy 0.9069\n",
            "Epoch 73 Batch 100 Loss 0.3071 Accuracy 0.9057\n",
            "Epoch 73 Batch 150 Loss 0.3093 Accuracy 0.9049\n",
            "Epoch 73 Batch 200 Loss 0.3089 Accuracy 0.9054\n",
            "Epoch 73 Batch 250 Loss 0.3093 Accuracy 0.9049\n",
            "Epoch 73 Batch 300 Loss 0.3101 Accuracy 0.9046\n",
            "Epoch 73 Batch 350 Loss 0.3120 Accuracy 0.9040\n",
            "Epoch 73 Batch 400 Loss 0.3133 Accuracy 0.9036\n",
            "Epoch 73 Batch 450 Loss 0.3144 Accuracy 0.9035\n",
            "Epoch 73 Batch 500 Loss 0.3148 Accuracy 0.9035\n",
            "Epoch 73 Batch 550 Loss 0.3160 Accuracy 0.9032\n",
            "Epoch 73 Batch 600 Loss 0.3161 Accuracy 0.9032\n",
            "Epoch 73 Batch 650 Loss 0.3171 Accuracy 0.9029\n",
            "Epoch 73 Batch 700 Loss 0.3181 Accuracy 0.9026\n",
            "Epoch 73 Batch 750 Loss 0.3185 Accuracy 0.9024\n",
            "Epoch 73 Batch 800 Loss 0.3198 Accuracy 0.9021\n",
            "Epoch 73 Batch 850 Loss 0.3203 Accuracy 0.9019\n",
            "Epoch 73 Batch 900 Loss 0.3212 Accuracy 0.9015\n",
            "Epoch 73 Batch 950 Loss 0.3215 Accuracy 0.9014\n",
            "Epoch 73 Batch 1000 Loss 0.3222 Accuracy 0.9013\n",
            "Epoch 73 Batch 1050 Loss 0.3228 Accuracy 0.9011\n",
            "Epoch 73 Batch 1100 Loss 0.3237 Accuracy 0.9009\n",
            "Epoch 73 Batch 1150 Loss 0.3245 Accuracy 0.9007\n",
            "Epoch 73 Batch 1200 Loss 0.3253 Accuracy 0.9003\n",
            "Epoch 73 Loss 0.3263 Accuracy 0.9000\n",
            "Time taken for 1 epoch: 145.20 secs\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.3271 Accuracy 0.9176\n",
            "Epoch 74 Batch 50 Loss 0.3073 Accuracy 0.9070\n",
            "Epoch 74 Batch 100 Loss 0.3015 Accuracy 0.9076\n",
            "Epoch 74 Batch 150 Loss 0.3059 Accuracy 0.9062\n",
            "Epoch 74 Batch 200 Loss 0.3073 Accuracy 0.9056\n",
            "Epoch 74 Batch 250 Loss 0.3066 Accuracy 0.9056\n",
            "Epoch 74 Batch 300 Loss 0.3092 Accuracy 0.9048\n",
            "Epoch 74 Batch 350 Loss 0.3103 Accuracy 0.9046\n",
            "Epoch 74 Batch 400 Loss 0.3114 Accuracy 0.9043\n",
            "Epoch 74 Batch 450 Loss 0.3118 Accuracy 0.9041\n",
            "Epoch 74 Batch 500 Loss 0.3127 Accuracy 0.9038\n",
            "Epoch 74 Batch 550 Loss 0.3137 Accuracy 0.9037\n",
            "Epoch 74 Batch 600 Loss 0.3151 Accuracy 0.9032\n",
            "Epoch 74 Batch 650 Loss 0.3157 Accuracy 0.9030\n",
            "Epoch 74 Batch 700 Loss 0.3164 Accuracy 0.9028\n",
            "Epoch 74 Batch 750 Loss 0.3165 Accuracy 0.9028\n",
            "Epoch 74 Batch 800 Loss 0.3167 Accuracy 0.9028\n",
            "Epoch 74 Batch 850 Loss 0.3175 Accuracy 0.9025\n",
            "Epoch 74 Batch 900 Loss 0.3185 Accuracy 0.9022\n",
            "Epoch 74 Batch 950 Loss 0.3191 Accuracy 0.9020\n",
            "Epoch 74 Batch 1000 Loss 0.3197 Accuracy 0.9019\n",
            "Epoch 74 Batch 1050 Loss 0.3206 Accuracy 0.9017\n",
            "Epoch 74 Batch 1100 Loss 0.3212 Accuracy 0.9015\n",
            "Epoch 74 Batch 1150 Loss 0.3219 Accuracy 0.9013\n",
            "Epoch 74 Batch 1200 Loss 0.3227 Accuracy 0.9011\n",
            "Epoch 74 Loss 0.3237 Accuracy 0.9008\n",
            "Time taken for 1 epoch: 124.03 secs\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.3055 Accuracy 0.9058\n",
            "Epoch 75 Batch 50 Loss 0.3016 Accuracy 0.9071\n",
            "Epoch 75 Batch 100 Loss 0.2961 Accuracy 0.9092\n",
            "Epoch 75 Batch 150 Loss 0.2965 Accuracy 0.9088\n",
            "Epoch 75 Batch 200 Loss 0.2982 Accuracy 0.9084\n",
            "Epoch 75 Batch 250 Loss 0.2991 Accuracy 0.9083\n",
            "Epoch 75 Batch 300 Loss 0.3016 Accuracy 0.9075\n",
            "Epoch 75 Batch 350 Loss 0.3025 Accuracy 0.9072\n",
            "Epoch 75 Batch 400 Loss 0.3036 Accuracy 0.9069\n",
            "Epoch 75 Batch 450 Loss 0.3059 Accuracy 0.9063\n",
            "Epoch 75 Batch 500 Loss 0.3063 Accuracy 0.9063\n",
            "Epoch 75 Batch 550 Loss 0.3081 Accuracy 0.9056\n",
            "Epoch 75 Batch 600 Loss 0.3094 Accuracy 0.9052\n",
            "Epoch 75 Batch 650 Loss 0.3101 Accuracy 0.9050\n",
            "Epoch 75 Batch 700 Loss 0.3106 Accuracy 0.9048\n",
            "Epoch 75 Batch 750 Loss 0.3114 Accuracy 0.9045\n",
            "Epoch 75 Batch 800 Loss 0.3128 Accuracy 0.9041\n",
            "Epoch 75 Batch 850 Loss 0.3137 Accuracy 0.9038\n",
            "Epoch 75 Batch 900 Loss 0.3138 Accuracy 0.9038\n",
            "Epoch 75 Batch 950 Loss 0.3148 Accuracy 0.9036\n",
            "Epoch 75 Batch 1000 Loss 0.3149 Accuracy 0.9036\n",
            "Epoch 75 Batch 1050 Loss 0.3160 Accuracy 0.9033\n",
            "Epoch 75 Batch 1100 Loss 0.3168 Accuracy 0.9030\n",
            "Epoch 75 Batch 1150 Loss 0.3178 Accuracy 0.9028\n",
            "Epoch 75 Batch 1200 Loss 0.3187 Accuracy 0.9026\n",
            "Saving checkpoint for epoch 75 at ./checkpoints/train/ckpt-15\n",
            "Epoch 75 Loss 0.3193 Accuracy 0.9024\n",
            "Time taken for 1 epoch: 133.38 secs\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.2966 Accuracy 0.9215\n",
            "Epoch 76 Batch 50 Loss 0.2989 Accuracy 0.9086\n",
            "Epoch 76 Batch 100 Loss 0.2971 Accuracy 0.9082\n",
            "Epoch 76 Batch 150 Loss 0.2980 Accuracy 0.9082\n",
            "Epoch 76 Batch 200 Loss 0.3002 Accuracy 0.9081\n",
            "Epoch 76 Batch 250 Loss 0.2990 Accuracy 0.9083\n",
            "Epoch 76 Batch 300 Loss 0.2999 Accuracy 0.9079\n",
            "Epoch 76 Batch 350 Loss 0.3011 Accuracy 0.9077\n",
            "Epoch 76 Batch 400 Loss 0.3017 Accuracy 0.9076\n",
            "Epoch 76 Batch 450 Loss 0.3040 Accuracy 0.9070\n",
            "Epoch 76 Batch 500 Loss 0.3046 Accuracy 0.9068\n",
            "Epoch 76 Batch 550 Loss 0.3052 Accuracy 0.9065\n",
            "Epoch 76 Batch 600 Loss 0.3063 Accuracy 0.9063\n",
            "Epoch 76 Batch 650 Loss 0.3072 Accuracy 0.9060\n",
            "Epoch 76 Batch 700 Loss 0.3085 Accuracy 0.9057\n",
            "Epoch 76 Batch 750 Loss 0.3095 Accuracy 0.9054\n",
            "Epoch 76 Batch 800 Loss 0.3107 Accuracy 0.9050\n",
            "Epoch 76 Batch 850 Loss 0.3111 Accuracy 0.9048\n",
            "Epoch 76 Batch 900 Loss 0.3124 Accuracy 0.9046\n",
            "Epoch 76 Batch 950 Loss 0.3131 Accuracy 0.9043\n",
            "Epoch 76 Batch 1000 Loss 0.3134 Accuracy 0.9040\n",
            "Epoch 76 Batch 1050 Loss 0.3141 Accuracy 0.9038\n",
            "Epoch 76 Batch 1100 Loss 0.3149 Accuracy 0.9036\n",
            "Epoch 76 Batch 1150 Loss 0.3159 Accuracy 0.9033\n",
            "Epoch 76 Batch 1200 Loss 0.3167 Accuracy 0.9032\n",
            "Epoch 76 Loss 0.3176 Accuracy 0.9029\n",
            "Time taken for 1 epoch: 116.69 secs\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.2868 Accuracy 0.9087\n",
            "Epoch 77 Batch 50 Loss 0.2887 Accuracy 0.9100\n",
            "Epoch 77 Batch 100 Loss 0.2879 Accuracy 0.9121\n",
            "Epoch 77 Batch 150 Loss 0.2902 Accuracy 0.9106\n",
            "Epoch 77 Batch 200 Loss 0.2902 Accuracy 0.9107\n",
            "Epoch 77 Batch 250 Loss 0.2921 Accuracy 0.9098\n",
            "Epoch 77 Batch 300 Loss 0.2939 Accuracy 0.9096\n",
            "Epoch 77 Batch 350 Loss 0.2947 Accuracy 0.9093\n",
            "Epoch 77 Batch 400 Loss 0.2967 Accuracy 0.9090\n",
            "Epoch 77 Batch 450 Loss 0.2978 Accuracy 0.9087\n",
            "Epoch 77 Batch 500 Loss 0.2984 Accuracy 0.9086\n",
            "Epoch 77 Batch 550 Loss 0.3008 Accuracy 0.9078\n",
            "Epoch 77 Batch 600 Loss 0.3014 Accuracy 0.9077\n",
            "Epoch 77 Batch 650 Loss 0.3023 Accuracy 0.9075\n",
            "Epoch 77 Batch 700 Loss 0.3034 Accuracy 0.9072\n",
            "Epoch 77 Batch 750 Loss 0.3043 Accuracy 0.9069\n",
            "Epoch 77 Batch 800 Loss 0.3046 Accuracy 0.9068\n",
            "Epoch 77 Batch 850 Loss 0.3058 Accuracy 0.9063\n",
            "Epoch 77 Batch 900 Loss 0.3065 Accuracy 0.9061\n",
            "Epoch 77 Batch 950 Loss 0.3075 Accuracy 0.9058\n",
            "Epoch 77 Batch 1000 Loss 0.3080 Accuracy 0.9055\n",
            "Epoch 77 Batch 1050 Loss 0.3092 Accuracy 0.9051\n",
            "Epoch 77 Batch 1100 Loss 0.3100 Accuracy 0.9049\n",
            "Epoch 77 Batch 1150 Loss 0.3113 Accuracy 0.9045\n",
            "Epoch 77 Batch 1200 Loss 0.3118 Accuracy 0.9044\n",
            "Epoch 77 Loss 0.3125 Accuracy 0.9042\n",
            "Time taken for 1 epoch: 117.10 secs\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.2841 Accuracy 0.9090\n",
            "Epoch 78 Batch 50 Loss 0.2948 Accuracy 0.9092\n",
            "Epoch 78 Batch 100 Loss 0.2926 Accuracy 0.9091\n",
            "Epoch 78 Batch 150 Loss 0.2932 Accuracy 0.9090\n",
            "Epoch 78 Batch 200 Loss 0.2929 Accuracy 0.9095\n",
            "Epoch 78 Batch 250 Loss 0.2929 Accuracy 0.9095\n",
            "Epoch 78 Batch 300 Loss 0.2956 Accuracy 0.9089\n",
            "Epoch 78 Batch 350 Loss 0.2964 Accuracy 0.9086\n",
            "Epoch 78 Batch 400 Loss 0.2977 Accuracy 0.9085\n",
            "Epoch 78 Batch 450 Loss 0.2979 Accuracy 0.9085\n",
            "Epoch 78 Batch 500 Loss 0.2987 Accuracy 0.9083\n",
            "Epoch 78 Batch 550 Loss 0.2996 Accuracy 0.9079\n",
            "Epoch 78 Batch 600 Loss 0.2998 Accuracy 0.9079\n",
            "Epoch 78 Batch 650 Loss 0.3005 Accuracy 0.9076\n",
            "Epoch 78 Batch 700 Loss 0.3021 Accuracy 0.9072\n",
            "Epoch 78 Batch 750 Loss 0.3027 Accuracy 0.9069\n",
            "Epoch 78 Batch 800 Loss 0.3037 Accuracy 0.9067\n",
            "Epoch 78 Batch 850 Loss 0.3043 Accuracy 0.9065\n",
            "Epoch 78 Batch 900 Loss 0.3050 Accuracy 0.9063\n",
            "Epoch 78 Batch 950 Loss 0.3061 Accuracy 0.9060\n",
            "Epoch 78 Batch 1000 Loss 0.3061 Accuracy 0.9059\n",
            "Epoch 78 Batch 1050 Loss 0.3068 Accuracy 0.9057\n",
            "Epoch 78 Batch 1100 Loss 0.3073 Accuracy 0.9056\n",
            "Epoch 78 Batch 1150 Loss 0.3080 Accuracy 0.9054\n",
            "Epoch 78 Batch 1200 Loss 0.3090 Accuracy 0.9052\n",
            "Epoch 78 Loss 0.3097 Accuracy 0.9050\n",
            "Time taken for 1 epoch: 116.42 secs\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.2652 Accuracy 0.9196\n",
            "Epoch 79 Batch 50 Loss 0.2860 Accuracy 0.9144\n",
            "Epoch 79 Batch 100 Loss 0.2855 Accuracy 0.9128\n",
            "Epoch 79 Batch 150 Loss 0.2866 Accuracy 0.9120\n",
            "Epoch 79 Batch 200 Loss 0.2873 Accuracy 0.9116\n",
            "Epoch 79 Batch 250 Loss 0.2889 Accuracy 0.9110\n",
            "Epoch 79 Batch 300 Loss 0.2912 Accuracy 0.9103\n",
            "Epoch 79 Batch 350 Loss 0.2919 Accuracy 0.9100\n",
            "Epoch 79 Batch 400 Loss 0.2930 Accuracy 0.9097\n",
            "Epoch 79 Batch 450 Loss 0.2946 Accuracy 0.9094\n",
            "Epoch 79 Batch 500 Loss 0.2953 Accuracy 0.9091\n",
            "Epoch 79 Batch 550 Loss 0.2962 Accuracy 0.9089\n",
            "Epoch 79 Batch 600 Loss 0.2968 Accuracy 0.9088\n",
            "Epoch 79 Batch 650 Loss 0.2979 Accuracy 0.9085\n",
            "Epoch 79 Batch 700 Loss 0.2993 Accuracy 0.9081\n",
            "Epoch 79 Batch 750 Loss 0.2998 Accuracy 0.9080\n",
            "Epoch 79 Batch 800 Loss 0.3011 Accuracy 0.9077\n",
            "Epoch 79 Batch 850 Loss 0.3018 Accuracy 0.9075\n",
            "Epoch 79 Batch 900 Loss 0.3026 Accuracy 0.9072\n",
            "Epoch 79 Batch 950 Loss 0.3032 Accuracy 0.9071\n",
            "Epoch 79 Batch 1000 Loss 0.3037 Accuracy 0.9070\n",
            "Epoch 79 Batch 1050 Loss 0.3043 Accuracy 0.9068\n",
            "Epoch 79 Batch 1100 Loss 0.3051 Accuracy 0.9066\n",
            "Epoch 79 Batch 1150 Loss 0.3057 Accuracy 0.9065\n",
            "Epoch 79 Batch 1200 Loss 0.3065 Accuracy 0.9063\n",
            "Epoch 79 Loss 0.3074 Accuracy 0.9060\n",
            "Time taken for 1 epoch: 117.04 secs\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.3324 Accuracy 0.9025\n",
            "Epoch 80 Batch 50 Loss 0.2944 Accuracy 0.9094\n",
            "Epoch 80 Batch 100 Loss 0.2940 Accuracy 0.9105\n",
            "Epoch 80 Batch 150 Loss 0.2905 Accuracy 0.9112\n",
            "Epoch 80 Batch 200 Loss 0.2876 Accuracy 0.9124\n",
            "Epoch 80 Batch 250 Loss 0.2881 Accuracy 0.9120\n",
            "Epoch 80 Batch 300 Loss 0.2905 Accuracy 0.9112\n",
            "Epoch 80 Batch 350 Loss 0.2900 Accuracy 0.9111\n",
            "Epoch 80 Batch 400 Loss 0.2909 Accuracy 0.9107\n",
            "Epoch 80 Batch 450 Loss 0.2919 Accuracy 0.9105\n",
            "Epoch 80 Batch 500 Loss 0.2920 Accuracy 0.9106\n",
            "Epoch 80 Batch 550 Loss 0.2930 Accuracy 0.9103\n",
            "Epoch 80 Batch 600 Loss 0.2936 Accuracy 0.9102\n",
            "Epoch 80 Batch 650 Loss 0.2939 Accuracy 0.9102\n",
            "Epoch 80 Batch 700 Loss 0.2952 Accuracy 0.9097\n",
            "Epoch 80 Batch 750 Loss 0.2960 Accuracy 0.9094\n",
            "Epoch 80 Batch 800 Loss 0.2970 Accuracy 0.9091\n",
            "Epoch 80 Batch 850 Loss 0.2973 Accuracy 0.9091\n",
            "Epoch 80 Batch 900 Loss 0.2982 Accuracy 0.9088\n",
            "Epoch 80 Batch 950 Loss 0.2991 Accuracy 0.9086\n",
            "Epoch 80 Batch 1000 Loss 0.2998 Accuracy 0.9084\n",
            "Epoch 80 Batch 1050 Loss 0.3007 Accuracy 0.9080\n",
            "Epoch 80 Batch 1100 Loss 0.3013 Accuracy 0.9078\n",
            "Epoch 80 Batch 1150 Loss 0.3023 Accuracy 0.9075\n",
            "Epoch 80 Batch 1200 Loss 0.3032 Accuracy 0.9072\n",
            "Saving checkpoint for epoch 80 at ./checkpoints/train/ckpt-16\n",
            "Epoch 80 Loss 0.3038 Accuracy 0.9070\n",
            "Time taken for 1 epoch: 117.87 secs\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.2567 Accuracy 0.9132\n",
            "Epoch 81 Batch 50 Loss 0.2789 Accuracy 0.9141\n",
            "Epoch 81 Batch 100 Loss 0.2816 Accuracy 0.9124\n",
            "Epoch 81 Batch 150 Loss 0.2835 Accuracy 0.9126\n",
            "Epoch 81 Batch 200 Loss 0.2838 Accuracy 0.9127\n",
            "Epoch 81 Batch 250 Loss 0.2843 Accuracy 0.9128\n",
            "Epoch 81 Batch 300 Loss 0.2849 Accuracy 0.9125\n",
            "Epoch 81 Batch 350 Loss 0.2869 Accuracy 0.9121\n",
            "Epoch 81 Batch 400 Loss 0.2887 Accuracy 0.9116\n",
            "Epoch 81 Batch 450 Loss 0.2907 Accuracy 0.9110\n",
            "Epoch 81 Batch 500 Loss 0.2919 Accuracy 0.9106\n",
            "Epoch 81 Batch 550 Loss 0.2932 Accuracy 0.9099\n",
            "Epoch 81 Batch 600 Loss 0.2933 Accuracy 0.9099\n",
            "Epoch 81 Batch 650 Loss 0.2938 Accuracy 0.9099\n",
            "Epoch 81 Batch 700 Loss 0.2941 Accuracy 0.9099\n",
            "Epoch 81 Batch 750 Loss 0.2946 Accuracy 0.9097\n",
            "Epoch 81 Batch 800 Loss 0.2951 Accuracy 0.9095\n",
            "Epoch 81 Batch 850 Loss 0.2957 Accuracy 0.9093\n",
            "Epoch 81 Batch 900 Loss 0.2961 Accuracy 0.9092\n",
            "Epoch 81 Batch 950 Loss 0.2965 Accuracy 0.9091\n",
            "Epoch 81 Batch 1000 Loss 0.2972 Accuracy 0.9089\n",
            "Epoch 81 Batch 1050 Loss 0.2979 Accuracy 0.9088\n",
            "Epoch 81 Batch 1100 Loss 0.2986 Accuracy 0.9086\n",
            "Epoch 81 Batch 1150 Loss 0.2991 Accuracy 0.9084\n",
            "Epoch 81 Batch 1200 Loss 0.2999 Accuracy 0.9082\n",
            "Epoch 81 Loss 0.3007 Accuracy 0.9080\n",
            "Time taken for 1 epoch: 116.55 secs\n",
            "\n",
            "Epoch 82 Batch 0 Loss 0.2404 Accuracy 0.9240\n",
            "Epoch 82 Batch 50 Loss 0.2833 Accuracy 0.9128\n",
            "Epoch 82 Batch 100 Loss 0.2782 Accuracy 0.9137\n",
            "Epoch 82 Batch 150 Loss 0.2781 Accuracy 0.9142\n",
            "Epoch 82 Batch 200 Loss 0.2786 Accuracy 0.9140\n",
            "Epoch 82 Batch 250 Loss 0.2799 Accuracy 0.9135\n",
            "Epoch 82 Batch 300 Loss 0.2810 Accuracy 0.9132\n",
            "Epoch 82 Batch 350 Loss 0.2831 Accuracy 0.9128\n",
            "Epoch 82 Batch 400 Loss 0.2843 Accuracy 0.9124\n",
            "Epoch 82 Batch 450 Loss 0.2846 Accuracy 0.9123\n",
            "Epoch 82 Batch 500 Loss 0.2859 Accuracy 0.9119\n",
            "Epoch 82 Batch 550 Loss 0.2865 Accuracy 0.9117\n",
            "Epoch 82 Batch 600 Loss 0.2878 Accuracy 0.9115\n",
            "Epoch 82 Batch 650 Loss 0.2886 Accuracy 0.9112\n",
            "Epoch 82 Batch 700 Loss 0.2893 Accuracy 0.9109\n",
            "Epoch 82 Batch 750 Loss 0.2901 Accuracy 0.9106\n",
            "Epoch 82 Batch 800 Loss 0.2906 Accuracy 0.9103\n",
            "Epoch 82 Batch 850 Loss 0.2915 Accuracy 0.9101\n",
            "Epoch 82 Batch 900 Loss 0.2920 Accuracy 0.9101\n",
            "Epoch 82 Batch 950 Loss 0.2919 Accuracy 0.9101\n",
            "Epoch 82 Batch 1000 Loss 0.2931 Accuracy 0.9098\n",
            "Epoch 82 Batch 1050 Loss 0.2940 Accuracy 0.9095\n",
            "Epoch 82 Batch 1100 Loss 0.2949 Accuracy 0.9092\n",
            "Epoch 82 Batch 1150 Loss 0.2959 Accuracy 0.9088\n",
            "Epoch 82 Batch 1200 Loss 0.2971 Accuracy 0.9085\n",
            "Epoch 82 Loss 0.2982 Accuracy 0.9082\n",
            "Time taken for 1 epoch: 116.58 secs\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.3569 Accuracy 0.8941\n",
            "Epoch 83 Batch 50 Loss 0.2808 Accuracy 0.9142\n",
            "Epoch 83 Batch 100 Loss 0.2811 Accuracy 0.9146\n",
            "Epoch 83 Batch 150 Loss 0.2797 Accuracy 0.9147\n",
            "Epoch 83 Batch 200 Loss 0.2807 Accuracy 0.9142\n",
            "Epoch 83 Batch 250 Loss 0.2819 Accuracy 0.9140\n",
            "Epoch 83 Batch 300 Loss 0.2827 Accuracy 0.9136\n",
            "Epoch 83 Batch 350 Loss 0.2836 Accuracy 0.9133\n",
            "Epoch 83 Batch 400 Loss 0.2841 Accuracy 0.9133\n",
            "Epoch 83 Batch 450 Loss 0.2847 Accuracy 0.9131\n",
            "Epoch 83 Batch 500 Loss 0.2853 Accuracy 0.9130\n",
            "Epoch 83 Batch 550 Loss 0.2866 Accuracy 0.9125\n",
            "Epoch 83 Batch 600 Loss 0.2874 Accuracy 0.9123\n",
            "Epoch 83 Batch 650 Loss 0.2880 Accuracy 0.9120\n",
            "Epoch 83 Batch 700 Loss 0.2882 Accuracy 0.9119\n",
            "Epoch 83 Batch 750 Loss 0.2886 Accuracy 0.9117\n",
            "Epoch 83 Batch 800 Loss 0.2889 Accuracy 0.9117\n",
            "Epoch 83 Batch 850 Loss 0.2897 Accuracy 0.9115\n",
            "Epoch 83 Batch 900 Loss 0.2904 Accuracy 0.9113\n",
            "Epoch 83 Batch 950 Loss 0.2908 Accuracy 0.9111\n",
            "Epoch 83 Batch 1000 Loss 0.2913 Accuracy 0.9109\n",
            "Epoch 83 Batch 1050 Loss 0.2912 Accuracy 0.9109\n",
            "Epoch 83 Batch 1100 Loss 0.2921 Accuracy 0.9107\n",
            "Epoch 83 Batch 1150 Loss 0.2929 Accuracy 0.9104\n",
            "Epoch 83 Batch 1200 Loss 0.2940 Accuracy 0.9101\n",
            "Epoch 83 Loss 0.2950 Accuracy 0.9099\n",
            "Time taken for 1 epoch: 116.66 secs\n",
            "\n",
            "Epoch 84 Batch 0 Loss 0.2938 Accuracy 0.9170\n",
            "Epoch 84 Batch 50 Loss 0.2751 Accuracy 0.9178\n",
            "Epoch 84 Batch 100 Loss 0.2729 Accuracy 0.9180\n",
            "Epoch 84 Batch 150 Loss 0.2742 Accuracy 0.9167\n",
            "Epoch 84 Batch 200 Loss 0.2750 Accuracy 0.9161\n",
            "Epoch 84 Batch 250 Loss 0.2774 Accuracy 0.9152\n",
            "Epoch 84 Batch 300 Loss 0.2794 Accuracy 0.9146\n",
            "Epoch 84 Batch 350 Loss 0.2796 Accuracy 0.9146\n",
            "Epoch 84 Batch 400 Loss 0.2806 Accuracy 0.9144\n",
            "Epoch 84 Batch 450 Loss 0.2812 Accuracy 0.9143\n",
            "Epoch 84 Batch 500 Loss 0.2818 Accuracy 0.9142\n",
            "Epoch 84 Batch 550 Loss 0.2827 Accuracy 0.9139\n",
            "Epoch 84 Batch 600 Loss 0.2840 Accuracy 0.9135\n",
            "Epoch 84 Batch 650 Loss 0.2844 Accuracy 0.9133\n",
            "Epoch 84 Batch 700 Loss 0.2859 Accuracy 0.9128\n",
            "Epoch 84 Batch 750 Loss 0.2864 Accuracy 0.9125\n",
            "Epoch 84 Batch 800 Loss 0.2876 Accuracy 0.9123\n",
            "Epoch 84 Batch 850 Loss 0.2881 Accuracy 0.9120\n",
            "Epoch 84 Batch 900 Loss 0.2887 Accuracy 0.9119\n",
            "Epoch 84 Batch 950 Loss 0.2893 Accuracy 0.9116\n",
            "Epoch 84 Batch 1000 Loss 0.2899 Accuracy 0.9114\n",
            "Epoch 84 Batch 1050 Loss 0.2906 Accuracy 0.9112\n",
            "Epoch 84 Batch 1100 Loss 0.2915 Accuracy 0.9110\n",
            "Epoch 84 Batch 1150 Loss 0.2925 Accuracy 0.9107\n",
            "Epoch 84 Batch 1200 Loss 0.2933 Accuracy 0.9104\n",
            "Epoch 84 Loss 0.2940 Accuracy 0.9102\n",
            "Time taken for 1 epoch: 116.52 secs\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.2667 Accuracy 0.9163\n",
            "Epoch 85 Batch 50 Loss 0.2770 Accuracy 0.9144\n",
            "Epoch 85 Batch 100 Loss 0.2750 Accuracy 0.9155\n",
            "Epoch 85 Batch 150 Loss 0.2748 Accuracy 0.9155\n",
            "Epoch 85 Batch 200 Loss 0.2746 Accuracy 0.9159\n",
            "Epoch 85 Batch 250 Loss 0.2758 Accuracy 0.9155\n",
            "Epoch 85 Batch 300 Loss 0.2758 Accuracy 0.9157\n",
            "Epoch 85 Batch 350 Loss 0.2775 Accuracy 0.9152\n",
            "Epoch 85 Batch 400 Loss 0.2787 Accuracy 0.9149\n",
            "Epoch 85 Batch 450 Loss 0.2796 Accuracy 0.9147\n",
            "Epoch 85 Batch 500 Loss 0.2812 Accuracy 0.9141\n",
            "Epoch 85 Batch 550 Loss 0.2819 Accuracy 0.9138\n",
            "Epoch 85 Batch 600 Loss 0.2824 Accuracy 0.9138\n",
            "Epoch 85 Batch 650 Loss 0.2824 Accuracy 0.9137\n",
            "Epoch 85 Batch 700 Loss 0.2831 Accuracy 0.9136\n",
            "Epoch 85 Batch 750 Loss 0.2841 Accuracy 0.9134\n",
            "Epoch 85 Batch 800 Loss 0.2844 Accuracy 0.9133\n",
            "Epoch 85 Batch 850 Loss 0.2847 Accuracy 0.9132\n",
            "Epoch 85 Batch 900 Loss 0.2855 Accuracy 0.9131\n",
            "Epoch 85 Batch 950 Loss 0.2858 Accuracy 0.9130\n",
            "Epoch 85 Batch 1000 Loss 0.2863 Accuracy 0.9129\n",
            "Epoch 85 Batch 1050 Loss 0.2872 Accuracy 0.9125\n",
            "Epoch 85 Batch 1100 Loss 0.2880 Accuracy 0.9123\n",
            "Epoch 85 Batch 1150 Loss 0.2882 Accuracy 0.9121\n",
            "Epoch 85 Batch 1200 Loss 0.2889 Accuracy 0.9119\n",
            "Saving checkpoint for epoch 85 at ./checkpoints/train/ckpt-17\n",
            "Epoch 85 Loss 0.2899 Accuracy 0.9115\n",
            "Time taken for 1 epoch: 117.42 secs\n",
            "\n",
            "Epoch 86 Batch 0 Loss 0.2823 Accuracy 0.9084\n",
            "Epoch 86 Batch 50 Loss 0.2702 Accuracy 0.9179\n",
            "Epoch 86 Batch 100 Loss 0.2718 Accuracy 0.9164\n",
            "Epoch 86 Batch 150 Loss 0.2705 Accuracy 0.9168\n",
            "Epoch 86 Batch 200 Loss 0.2718 Accuracy 0.9166\n",
            "Epoch 86 Batch 250 Loss 0.2725 Accuracy 0.9160\n",
            "Epoch 86 Batch 300 Loss 0.2751 Accuracy 0.9151\n",
            "Epoch 86 Batch 350 Loss 0.2752 Accuracy 0.9150\n",
            "Epoch 86 Batch 400 Loss 0.2758 Accuracy 0.9149\n",
            "Epoch 86 Batch 450 Loss 0.2764 Accuracy 0.9147\n",
            "Epoch 86 Batch 500 Loss 0.2769 Accuracy 0.9147\n",
            "Epoch 86 Batch 550 Loss 0.2773 Accuracy 0.9146\n",
            "Epoch 86 Batch 600 Loss 0.2781 Accuracy 0.9143\n",
            "Epoch 86 Batch 650 Loss 0.2787 Accuracy 0.9141\n",
            "Epoch 86 Batch 700 Loss 0.2794 Accuracy 0.9140\n",
            "Epoch 86 Batch 750 Loss 0.2799 Accuracy 0.9139\n",
            "Epoch 86 Batch 800 Loss 0.2805 Accuracy 0.9138\n",
            "Epoch 86 Batch 850 Loss 0.2806 Accuracy 0.9137\n",
            "Epoch 86 Batch 900 Loss 0.2815 Accuracy 0.9135\n",
            "Epoch 86 Batch 950 Loss 0.2821 Accuracy 0.9133\n",
            "Epoch 86 Batch 1000 Loss 0.2826 Accuracy 0.9132\n",
            "Epoch 86 Batch 1050 Loss 0.2829 Accuracy 0.9131\n",
            "Epoch 86 Batch 1100 Loss 0.2833 Accuracy 0.9129\n",
            "Epoch 86 Batch 1150 Loss 0.2844 Accuracy 0.9127\n",
            "Epoch 86 Batch 1200 Loss 0.2854 Accuracy 0.9124\n",
            "Epoch 86 Loss 0.2860 Accuracy 0.9123\n",
            "Time taken for 1 epoch: 116.57 secs\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.2601 Accuracy 0.9193\n",
            "Epoch 87 Batch 50 Loss 0.2674 Accuracy 0.9184\n",
            "Epoch 87 Batch 100 Loss 0.2648 Accuracy 0.9189\n",
            "Epoch 87 Batch 150 Loss 0.2657 Accuracy 0.9184\n",
            "Epoch 87 Batch 200 Loss 0.2687 Accuracy 0.9178\n",
            "Epoch 87 Batch 250 Loss 0.2691 Accuracy 0.9174\n",
            "Epoch 87 Batch 300 Loss 0.2711 Accuracy 0.9166\n",
            "Epoch 87 Batch 350 Loss 0.2716 Accuracy 0.9163\n",
            "Epoch 87 Batch 400 Loss 0.2725 Accuracy 0.9161\n",
            "Epoch 87 Batch 450 Loss 0.2745 Accuracy 0.9154\n",
            "Epoch 87 Batch 500 Loss 0.2757 Accuracy 0.9150\n",
            "Epoch 87 Batch 550 Loss 0.2764 Accuracy 0.9148\n",
            "Epoch 87 Batch 600 Loss 0.2768 Accuracy 0.9146\n",
            "Epoch 87 Batch 650 Loss 0.2771 Accuracy 0.9146\n",
            "Epoch 87 Batch 700 Loss 0.2783 Accuracy 0.9144\n",
            "Epoch 87 Batch 750 Loss 0.2784 Accuracy 0.9143\n",
            "Epoch 87 Batch 800 Loss 0.2789 Accuracy 0.9142\n",
            "Epoch 87 Batch 850 Loss 0.2794 Accuracy 0.9140\n",
            "Epoch 87 Batch 900 Loss 0.2801 Accuracy 0.9138\n",
            "Epoch 87 Batch 950 Loss 0.2805 Accuracy 0.9137\n",
            "Epoch 87 Batch 1000 Loss 0.2810 Accuracy 0.9136\n",
            "Epoch 87 Batch 1050 Loss 0.2816 Accuracy 0.9134\n",
            "Epoch 87 Batch 1100 Loss 0.2821 Accuracy 0.9132\n",
            "Epoch 87 Batch 1150 Loss 0.2829 Accuracy 0.9129\n",
            "Epoch 87 Batch 1200 Loss 0.2837 Accuracy 0.9128\n",
            "Epoch 87 Loss 0.2850 Accuracy 0.9124\n",
            "Time taken for 1 epoch: 116.90 secs\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.2730 Accuracy 0.9152\n",
            "Epoch 88 Batch 50 Loss 0.2633 Accuracy 0.9167\n",
            "Epoch 88 Batch 100 Loss 0.2623 Accuracy 0.9184\n",
            "Epoch 88 Batch 150 Loss 0.2615 Accuracy 0.9194\n",
            "Epoch 88 Batch 200 Loss 0.2637 Accuracy 0.9185\n",
            "Epoch 88 Batch 250 Loss 0.2643 Accuracy 0.9186\n",
            "Epoch 88 Batch 300 Loss 0.2668 Accuracy 0.9181\n",
            "Epoch 88 Batch 350 Loss 0.2683 Accuracy 0.9176\n",
            "Epoch 88 Batch 400 Loss 0.2697 Accuracy 0.9172\n",
            "Epoch 88 Batch 450 Loss 0.2702 Accuracy 0.9169\n",
            "Epoch 88 Batch 500 Loss 0.2715 Accuracy 0.9165\n",
            "Epoch 88 Batch 550 Loss 0.2726 Accuracy 0.9162\n",
            "Epoch 88 Batch 600 Loss 0.2737 Accuracy 0.9159\n",
            "Epoch 88 Batch 650 Loss 0.2744 Accuracy 0.9157\n",
            "Epoch 88 Batch 700 Loss 0.2746 Accuracy 0.9155\n",
            "Epoch 88 Batch 750 Loss 0.2756 Accuracy 0.9152\n",
            "Epoch 88 Batch 800 Loss 0.2764 Accuracy 0.9151\n",
            "Epoch 88 Batch 850 Loss 0.2766 Accuracy 0.9151\n",
            "Epoch 88 Batch 900 Loss 0.2771 Accuracy 0.9150\n",
            "Epoch 88 Batch 950 Loss 0.2774 Accuracy 0.9148\n",
            "Epoch 88 Batch 1000 Loss 0.2775 Accuracy 0.9148\n",
            "Epoch 88 Batch 1050 Loss 0.2781 Accuracy 0.9146\n",
            "Epoch 88 Batch 1100 Loss 0.2789 Accuracy 0.9144\n",
            "Epoch 88 Batch 1150 Loss 0.2797 Accuracy 0.9142\n",
            "Epoch 88 Batch 1200 Loss 0.2806 Accuracy 0.9139\n",
            "Epoch 88 Loss 0.2814 Accuracy 0.9136\n",
            "Time taken for 1 epoch: 116.56 secs\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.2446 Accuracy 0.9190\n",
            "Epoch 89 Batch 50 Loss 0.2635 Accuracy 0.9175\n",
            "Epoch 89 Batch 100 Loss 0.2635 Accuracy 0.9179\n",
            "Epoch 89 Batch 150 Loss 0.2608 Accuracy 0.9192\n",
            "Epoch 89 Batch 200 Loss 0.2625 Accuracy 0.9188\n",
            "Epoch 89 Batch 250 Loss 0.2632 Accuracy 0.9186\n",
            "Epoch 89 Batch 300 Loss 0.2647 Accuracy 0.9181\n",
            "Epoch 89 Batch 350 Loss 0.2670 Accuracy 0.9175\n",
            "Epoch 89 Batch 400 Loss 0.2671 Accuracy 0.9176\n",
            "Epoch 89 Batch 450 Loss 0.2677 Accuracy 0.9174\n",
            "Epoch 89 Batch 500 Loss 0.2685 Accuracy 0.9172\n",
            "Epoch 89 Batch 550 Loss 0.2692 Accuracy 0.9170\n",
            "Epoch 89 Batch 600 Loss 0.2701 Accuracy 0.9168\n",
            "Epoch 89 Batch 650 Loss 0.2707 Accuracy 0.9166\n",
            "Epoch 89 Batch 700 Loss 0.2715 Accuracy 0.9164\n",
            "Epoch 89 Batch 750 Loss 0.2722 Accuracy 0.9162\n",
            "Epoch 89 Batch 800 Loss 0.2724 Accuracy 0.9160\n",
            "Epoch 89 Batch 850 Loss 0.2729 Accuracy 0.9159\n",
            "Epoch 89 Batch 900 Loss 0.2739 Accuracy 0.9156\n",
            "Epoch 89 Batch 950 Loss 0.2740 Accuracy 0.9156\n",
            "Epoch 89 Batch 1000 Loss 0.2748 Accuracy 0.9154\n",
            "Epoch 89 Batch 1050 Loss 0.2756 Accuracy 0.9151\n",
            "Epoch 89 Batch 1100 Loss 0.2764 Accuracy 0.9149\n",
            "Epoch 89 Batch 1150 Loss 0.2775 Accuracy 0.9145\n",
            "Epoch 89 Batch 1200 Loss 0.2782 Accuracy 0.9143\n",
            "Epoch 89 Loss 0.2789 Accuracy 0.9141\n",
            "Time taken for 1 epoch: 116.66 secs\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.2367 Accuracy 0.9268\n",
            "Epoch 90 Batch 50 Loss 0.2631 Accuracy 0.9191\n",
            "Epoch 90 Batch 100 Loss 0.2608 Accuracy 0.9193\n",
            "Epoch 90 Batch 150 Loss 0.2611 Accuracy 0.9193\n",
            "Epoch 90 Batch 200 Loss 0.2612 Accuracy 0.9194\n",
            "Epoch 90 Batch 250 Loss 0.2627 Accuracy 0.9192\n",
            "Epoch 90 Batch 300 Loss 0.2632 Accuracy 0.9192\n",
            "Epoch 90 Batch 350 Loss 0.2651 Accuracy 0.9186\n",
            "Epoch 90 Batch 400 Loss 0.2661 Accuracy 0.9182\n",
            "Epoch 90 Batch 450 Loss 0.2660 Accuracy 0.9182\n",
            "Epoch 90 Batch 500 Loss 0.2670 Accuracy 0.9179\n",
            "Epoch 90 Batch 550 Loss 0.2673 Accuracy 0.9179\n",
            "Epoch 90 Batch 600 Loss 0.2678 Accuracy 0.9178\n",
            "Epoch 90 Batch 650 Loss 0.2687 Accuracy 0.9174\n",
            "Epoch 90 Batch 700 Loss 0.2692 Accuracy 0.9173\n",
            "Epoch 90 Batch 750 Loss 0.2699 Accuracy 0.9170\n",
            "Epoch 90 Batch 800 Loss 0.2702 Accuracy 0.9169\n",
            "Epoch 90 Batch 850 Loss 0.2708 Accuracy 0.9166\n",
            "Epoch 90 Batch 900 Loss 0.2713 Accuracy 0.9166\n",
            "Epoch 90 Batch 950 Loss 0.2720 Accuracy 0.9164\n",
            "Epoch 90 Batch 1000 Loss 0.2726 Accuracy 0.9162\n",
            "Epoch 90 Batch 1050 Loss 0.2734 Accuracy 0.9160\n",
            "Epoch 90 Batch 1100 Loss 0.2745 Accuracy 0.9157\n",
            "Epoch 90 Batch 1150 Loss 0.2750 Accuracy 0.9155\n",
            "Epoch 90 Batch 1200 Loss 0.2754 Accuracy 0.9154\n",
            "Saving checkpoint for epoch 90 at ./checkpoints/train/ckpt-18\n",
            "Epoch 90 Loss 0.2759 Accuracy 0.9153\n",
            "Time taken for 1 epoch: 117.43 secs\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.2430 Accuracy 0.9168\n",
            "Epoch 91 Batch 50 Loss 0.2673 Accuracy 0.9168\n",
            "Epoch 91 Batch 100 Loss 0.2632 Accuracy 0.9185\n",
            "Epoch 91 Batch 150 Loss 0.2617 Accuracy 0.9194\n",
            "Epoch 91 Batch 200 Loss 0.2644 Accuracy 0.9184\n",
            "Epoch 91 Batch 250 Loss 0.2627 Accuracy 0.9192\n",
            "Epoch 91 Batch 300 Loss 0.2634 Accuracy 0.9191\n",
            "Epoch 91 Batch 350 Loss 0.2633 Accuracy 0.9191\n",
            "Epoch 91 Batch 400 Loss 0.2643 Accuracy 0.9190\n",
            "Epoch 91 Batch 450 Loss 0.2654 Accuracy 0.9186\n",
            "Epoch 91 Batch 500 Loss 0.2671 Accuracy 0.9180\n",
            "Epoch 91 Batch 550 Loss 0.2673 Accuracy 0.9180\n",
            "Epoch 91 Batch 600 Loss 0.2678 Accuracy 0.9178\n",
            "Epoch 91 Batch 650 Loss 0.2695 Accuracy 0.9172\n",
            "Epoch 91 Batch 700 Loss 0.2701 Accuracy 0.9170\n",
            "Epoch 91 Batch 750 Loss 0.2701 Accuracy 0.9170\n",
            "Epoch 91 Batch 800 Loss 0.2709 Accuracy 0.9167\n",
            "Epoch 91 Batch 850 Loss 0.2715 Accuracy 0.9165\n",
            "Epoch 91 Batch 900 Loss 0.2721 Accuracy 0.9163\n",
            "Epoch 91 Batch 950 Loss 0.2725 Accuracy 0.9161\n",
            "Epoch 91 Batch 1000 Loss 0.2727 Accuracy 0.9161\n",
            "Epoch 91 Batch 1050 Loss 0.2729 Accuracy 0.9161\n",
            "Epoch 91 Batch 1100 Loss 0.2735 Accuracy 0.9159\n",
            "Epoch 91 Batch 1150 Loss 0.2744 Accuracy 0.9156\n",
            "Epoch 91 Batch 1200 Loss 0.2750 Accuracy 0.9154\n",
            "Epoch 91 Loss 0.2761 Accuracy 0.9152\n",
            "Time taken for 1 epoch: 116.78 secs\n",
            "\n",
            "Epoch 92 Batch 0 Loss 0.2506 Accuracy 0.9193\n",
            "Epoch 92 Batch 50 Loss 0.2573 Accuracy 0.9213\n",
            "Epoch 92 Batch 100 Loss 0.2539 Accuracy 0.9221\n",
            "Epoch 92 Batch 150 Loss 0.2532 Accuracy 0.9220\n",
            "Epoch 92 Batch 200 Loss 0.2553 Accuracy 0.9215\n",
            "Epoch 92 Batch 250 Loss 0.2564 Accuracy 0.9212\n",
            "Epoch 92 Batch 300 Loss 0.2574 Accuracy 0.9209\n",
            "Epoch 92 Batch 350 Loss 0.2594 Accuracy 0.9202\n",
            "Epoch 92 Batch 400 Loss 0.2606 Accuracy 0.9197\n",
            "Epoch 92 Batch 450 Loss 0.2619 Accuracy 0.9192\n",
            "Epoch 92 Batch 500 Loss 0.2629 Accuracy 0.9191\n",
            "Epoch 92 Batch 550 Loss 0.2641 Accuracy 0.9188\n",
            "Epoch 92 Batch 600 Loss 0.2647 Accuracy 0.9187\n",
            "Epoch 92 Batch 650 Loss 0.2656 Accuracy 0.9184\n",
            "Epoch 92 Batch 700 Loss 0.2660 Accuracy 0.9183\n",
            "Epoch 92 Batch 750 Loss 0.2672 Accuracy 0.9178\n",
            "Epoch 92 Batch 800 Loss 0.2676 Accuracy 0.9176\n",
            "Epoch 92 Batch 850 Loss 0.2683 Accuracy 0.9174\n",
            "Epoch 92 Batch 900 Loss 0.2688 Accuracy 0.9173\n",
            "Epoch 92 Batch 950 Loss 0.2694 Accuracy 0.9171\n",
            "Epoch 92 Batch 1000 Loss 0.2698 Accuracy 0.9170\n",
            "Epoch 92 Batch 1050 Loss 0.2703 Accuracy 0.9168\n",
            "Epoch 92 Batch 1100 Loss 0.2709 Accuracy 0.9167\n",
            "Epoch 92 Batch 1150 Loss 0.2710 Accuracy 0.9167\n",
            "Epoch 92 Batch 1200 Loss 0.2713 Accuracy 0.9166\n",
            "Epoch 92 Loss 0.2721 Accuracy 0.9163\n",
            "Time taken for 1 epoch: 116.41 secs\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.2483 Accuracy 0.9230\n",
            "Epoch 93 Batch 50 Loss 0.2491 Accuracy 0.9242\n",
            "Epoch 93 Batch 100 Loss 0.2530 Accuracy 0.9236\n",
            "Epoch 93 Batch 150 Loss 0.2561 Accuracy 0.9219\n",
            "Epoch 93 Batch 200 Loss 0.2577 Accuracy 0.9210\n",
            "Epoch 93 Batch 250 Loss 0.2582 Accuracy 0.9207\n",
            "Epoch 93 Batch 300 Loss 0.2590 Accuracy 0.9203\n",
            "Epoch 93 Batch 350 Loss 0.2603 Accuracy 0.9201\n",
            "Epoch 93 Batch 400 Loss 0.2604 Accuracy 0.9199\n",
            "Epoch 93 Batch 450 Loss 0.2615 Accuracy 0.9196\n",
            "Epoch 93 Batch 500 Loss 0.2621 Accuracy 0.9194\n",
            "Epoch 93 Batch 550 Loss 0.2638 Accuracy 0.9189\n",
            "Epoch 93 Batch 600 Loss 0.2641 Accuracy 0.9188\n",
            "Epoch 93 Batch 650 Loss 0.2646 Accuracy 0.9187\n",
            "Epoch 93 Batch 700 Loss 0.2651 Accuracy 0.9185\n",
            "Epoch 93 Batch 750 Loss 0.2654 Accuracy 0.9185\n",
            "Epoch 93 Batch 800 Loss 0.2659 Accuracy 0.9183\n",
            "Epoch 93 Batch 850 Loss 0.2666 Accuracy 0.9181\n",
            "Epoch 93 Batch 900 Loss 0.2674 Accuracy 0.9179\n",
            "Epoch 93 Batch 950 Loss 0.2680 Accuracy 0.9177\n",
            "Epoch 93 Batch 1000 Loss 0.2680 Accuracy 0.9177\n",
            "Epoch 93 Batch 1050 Loss 0.2686 Accuracy 0.9175\n",
            "Epoch 93 Batch 1100 Loss 0.2692 Accuracy 0.9173\n",
            "Epoch 93 Batch 1150 Loss 0.2700 Accuracy 0.9171\n",
            "Epoch 93 Batch 1200 Loss 0.2707 Accuracy 0.9170\n",
            "Epoch 93 Loss 0.2713 Accuracy 0.9167\n",
            "Time taken for 1 epoch: 117.20 secs\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.2522 Accuracy 0.9244\n",
            "Epoch 94 Batch 50 Loss 0.2493 Accuracy 0.9224\n",
            "Epoch 94 Batch 100 Loss 0.2527 Accuracy 0.9219\n",
            "Epoch 94 Batch 150 Loss 0.2547 Accuracy 0.9216\n",
            "Epoch 94 Batch 200 Loss 0.2539 Accuracy 0.9216\n",
            "Epoch 94 Batch 250 Loss 0.2551 Accuracy 0.9212\n",
            "Epoch 94 Batch 300 Loss 0.2561 Accuracy 0.9211\n",
            "Epoch 94 Batch 350 Loss 0.2572 Accuracy 0.9209\n",
            "Epoch 94 Batch 400 Loss 0.2582 Accuracy 0.9205\n",
            "Epoch 94 Batch 450 Loss 0.2582 Accuracy 0.9205\n",
            "Epoch 94 Batch 500 Loss 0.2595 Accuracy 0.9200\n",
            "Epoch 94 Batch 550 Loss 0.2599 Accuracy 0.9198\n",
            "Epoch 94 Batch 600 Loss 0.2605 Accuracy 0.9197\n",
            "Epoch 94 Batch 650 Loss 0.2615 Accuracy 0.9194\n",
            "Epoch 94 Batch 700 Loss 0.2622 Accuracy 0.9192\n",
            "Epoch 94 Batch 750 Loss 0.2623 Accuracy 0.9193\n",
            "Epoch 94 Batch 800 Loss 0.2627 Accuracy 0.9191\n",
            "Epoch 94 Batch 850 Loss 0.2634 Accuracy 0.9191\n",
            "Epoch 94 Batch 900 Loss 0.2641 Accuracy 0.9189\n",
            "Epoch 94 Batch 950 Loss 0.2648 Accuracy 0.9187\n",
            "Epoch 94 Batch 1000 Loss 0.2656 Accuracy 0.9185\n",
            "Epoch 94 Batch 1050 Loss 0.2658 Accuracy 0.9184\n",
            "Epoch 94 Batch 1100 Loss 0.2662 Accuracy 0.9183\n",
            "Epoch 94 Batch 1150 Loss 0.2671 Accuracy 0.9180\n",
            "Epoch 94 Batch 1200 Loss 0.2680 Accuracy 0.9178\n",
            "Epoch 94 Loss 0.2685 Accuracy 0.9176\n",
            "Time taken for 1 epoch: 116.54 secs\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.2534 Accuracy 0.9162\n",
            "Epoch 95 Batch 50 Loss 0.2499 Accuracy 0.9233\n",
            "Epoch 95 Batch 100 Loss 0.2482 Accuracy 0.9231\n",
            "Epoch 95 Batch 150 Loss 0.2497 Accuracy 0.9227\n",
            "Epoch 95 Batch 200 Loss 0.2497 Accuracy 0.9224\n",
            "Epoch 95 Batch 250 Loss 0.2505 Accuracy 0.9221\n",
            "Epoch 95 Batch 300 Loss 0.2524 Accuracy 0.9218\n",
            "Epoch 95 Batch 350 Loss 0.2534 Accuracy 0.9215\n",
            "Epoch 95 Batch 400 Loss 0.2543 Accuracy 0.9214\n",
            "Epoch 95 Batch 450 Loss 0.2558 Accuracy 0.9211\n",
            "Epoch 95 Batch 500 Loss 0.2564 Accuracy 0.9208\n",
            "Epoch 95 Batch 550 Loss 0.2573 Accuracy 0.9206\n",
            "Epoch 95 Batch 600 Loss 0.2579 Accuracy 0.9205\n",
            "Epoch 95 Batch 650 Loss 0.2584 Accuracy 0.9204\n",
            "Epoch 95 Batch 700 Loss 0.2591 Accuracy 0.9203\n",
            "Epoch 95 Batch 750 Loss 0.2597 Accuracy 0.9201\n",
            "Epoch 95 Batch 800 Loss 0.2607 Accuracy 0.9198\n",
            "Epoch 95 Batch 850 Loss 0.2610 Accuracy 0.9198\n",
            "Epoch 95 Batch 900 Loss 0.2616 Accuracy 0.9196\n",
            "Epoch 95 Batch 950 Loss 0.2625 Accuracy 0.9193\n",
            "Epoch 95 Batch 1000 Loss 0.2628 Accuracy 0.9191\n",
            "Epoch 95 Batch 1050 Loss 0.2636 Accuracy 0.9189\n",
            "Epoch 95 Batch 1100 Loss 0.2640 Accuracy 0.9188\n",
            "Epoch 95 Batch 1150 Loss 0.2648 Accuracy 0.9185\n",
            "Epoch 95 Batch 1200 Loss 0.2655 Accuracy 0.9183\n",
            "Saving checkpoint for epoch 95 at ./checkpoints/train/ckpt-19\n",
            "Epoch 95 Loss 0.2663 Accuracy 0.9181\n",
            "Time taken for 1 epoch: 117.79 secs\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.2313 Accuracy 0.9240\n",
            "Epoch 96 Batch 50 Loss 0.2488 Accuracy 0.9231\n",
            "Epoch 96 Batch 100 Loss 0.2488 Accuracy 0.9231\n",
            "Epoch 96 Batch 150 Loss 0.2482 Accuracy 0.9228\n",
            "Epoch 96 Batch 200 Loss 0.2478 Accuracy 0.9230\n",
            "Epoch 96 Batch 250 Loss 0.2489 Accuracy 0.9231\n",
            "Epoch 96 Batch 300 Loss 0.2500 Accuracy 0.9227\n",
            "Epoch 96 Batch 350 Loss 0.2512 Accuracy 0.9223\n",
            "Epoch 96 Batch 400 Loss 0.2521 Accuracy 0.9221\n",
            "Epoch 96 Batch 450 Loss 0.2526 Accuracy 0.9221\n",
            "Epoch 96 Batch 500 Loss 0.2541 Accuracy 0.9217\n",
            "Epoch 96 Batch 550 Loss 0.2552 Accuracy 0.9215\n",
            "Epoch 96 Batch 600 Loss 0.2557 Accuracy 0.9213\n",
            "Epoch 96 Batch 650 Loss 0.2571 Accuracy 0.9210\n",
            "Epoch 96 Batch 700 Loss 0.2576 Accuracy 0.9209\n",
            "Epoch 96 Batch 750 Loss 0.2587 Accuracy 0.9205\n",
            "Epoch 96 Batch 800 Loss 0.2593 Accuracy 0.9204\n",
            "Epoch 96 Batch 850 Loss 0.2593 Accuracy 0.9204\n",
            "Epoch 96 Batch 900 Loss 0.2601 Accuracy 0.9202\n",
            "Epoch 96 Batch 950 Loss 0.2605 Accuracy 0.9200\n",
            "Epoch 96 Batch 1000 Loss 0.2612 Accuracy 0.9199\n",
            "Epoch 96 Batch 1050 Loss 0.2616 Accuracy 0.9198\n",
            "Epoch 96 Batch 1100 Loss 0.2622 Accuracy 0.9196\n",
            "Epoch 96 Batch 1150 Loss 0.2628 Accuracy 0.9194\n",
            "Epoch 96 Batch 1200 Loss 0.2634 Accuracy 0.9193\n",
            "Epoch 96 Loss 0.2643 Accuracy 0.9191\n",
            "Time taken for 1 epoch: 116.89 secs\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.2672 Accuracy 0.9369\n",
            "Epoch 97 Batch 50 Loss 0.2471 Accuracy 0.9246\n",
            "Epoch 97 Batch 100 Loss 0.2417 Accuracy 0.9251\n",
            "Epoch 97 Batch 150 Loss 0.2467 Accuracy 0.9237\n",
            "Epoch 97 Batch 200 Loss 0.2476 Accuracy 0.9232\n",
            "Epoch 97 Batch 250 Loss 0.2483 Accuracy 0.9231\n",
            "Epoch 97 Batch 300 Loss 0.2491 Accuracy 0.9230\n",
            "Epoch 97 Batch 350 Loss 0.2501 Accuracy 0.9226\n",
            "Epoch 97 Batch 400 Loss 0.2504 Accuracy 0.9226\n",
            "Epoch 97 Batch 450 Loss 0.2509 Accuracy 0.9227\n",
            "Epoch 97 Batch 500 Loss 0.2523 Accuracy 0.9223\n",
            "Epoch 97 Batch 550 Loss 0.2541 Accuracy 0.9218\n",
            "Epoch 97 Batch 600 Loss 0.2547 Accuracy 0.9216\n",
            "Epoch 97 Batch 650 Loss 0.2548 Accuracy 0.9216\n",
            "Epoch 97 Batch 700 Loss 0.2558 Accuracy 0.9212\n",
            "Epoch 97 Batch 750 Loss 0.2566 Accuracy 0.9210\n",
            "Epoch 97 Batch 800 Loss 0.2574 Accuracy 0.9208\n",
            "Epoch 97 Batch 850 Loss 0.2580 Accuracy 0.9206\n",
            "Epoch 97 Batch 900 Loss 0.2586 Accuracy 0.9204\n",
            "Epoch 97 Batch 950 Loss 0.2594 Accuracy 0.9202\n",
            "Epoch 97 Batch 1000 Loss 0.2598 Accuracy 0.9201\n",
            "Epoch 97 Batch 1050 Loss 0.2603 Accuracy 0.9199\n",
            "Epoch 97 Batch 1100 Loss 0.2607 Accuracy 0.9197\n",
            "Epoch 97 Batch 1150 Loss 0.2614 Accuracy 0.9195\n",
            "Epoch 97 Batch 1200 Loss 0.2617 Accuracy 0.9194\n",
            "Epoch 97 Loss 0.2623 Accuracy 0.9192\n",
            "Time taken for 1 epoch: 116.90 secs\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.2214 Accuracy 0.9331\n",
            "Epoch 98 Batch 50 Loss 0.2422 Accuracy 0.9257\n",
            "Epoch 98 Batch 100 Loss 0.2426 Accuracy 0.9259\n",
            "Epoch 98 Batch 150 Loss 0.2445 Accuracy 0.9251\n",
            "Epoch 98 Batch 200 Loss 0.2457 Accuracy 0.9245\n",
            "Epoch 98 Batch 250 Loss 0.2461 Accuracy 0.9242\n",
            "Epoch 98 Batch 300 Loss 0.2471 Accuracy 0.9238\n",
            "Epoch 98 Batch 350 Loss 0.2488 Accuracy 0.9232\n",
            "Epoch 98 Batch 400 Loss 0.2496 Accuracy 0.9229\n",
            "Epoch 98 Batch 450 Loss 0.2503 Accuracy 0.9229\n",
            "Epoch 98 Batch 500 Loss 0.2516 Accuracy 0.9225\n",
            "Epoch 98 Batch 550 Loss 0.2526 Accuracy 0.9222\n",
            "Epoch 98 Batch 600 Loss 0.2534 Accuracy 0.9220\n",
            "Epoch 98 Batch 650 Loss 0.2540 Accuracy 0.9218\n",
            "Epoch 98 Batch 700 Loss 0.2542 Accuracy 0.9219\n",
            "Epoch 98 Batch 750 Loss 0.2547 Accuracy 0.9217\n",
            "Epoch 98 Batch 800 Loss 0.2555 Accuracy 0.9214\n",
            "Epoch 98 Batch 850 Loss 0.2561 Accuracy 0.9213\n",
            "Epoch 98 Batch 900 Loss 0.2569 Accuracy 0.9210\n",
            "Epoch 98 Batch 950 Loss 0.2575 Accuracy 0.9208\n",
            "Epoch 98 Batch 1000 Loss 0.2582 Accuracy 0.9206\n",
            "Epoch 98 Batch 1050 Loss 0.2589 Accuracy 0.9204\n",
            "Epoch 98 Batch 1100 Loss 0.2595 Accuracy 0.9202\n",
            "Epoch 98 Batch 1150 Loss 0.2599 Accuracy 0.9201\n",
            "Epoch 98 Batch 1200 Loss 0.2604 Accuracy 0.9199\n",
            "Epoch 98 Loss 0.2608 Accuracy 0.9197\n",
            "Time taken for 1 epoch: 117.01 secs\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.2727 Accuracy 0.9161\n",
            "Epoch 99 Batch 50 Loss 0.2475 Accuracy 0.9241\n",
            "Epoch 99 Batch 100 Loss 0.2418 Accuracy 0.9255\n",
            "Epoch 99 Batch 150 Loss 0.2425 Accuracy 0.9252\n",
            "Epoch 99 Batch 200 Loss 0.2438 Accuracy 0.9247\n",
            "Epoch 99 Batch 250 Loss 0.2456 Accuracy 0.9240\n",
            "Epoch 99 Batch 300 Loss 0.2472 Accuracy 0.9235\n",
            "Epoch 99 Batch 350 Loss 0.2469 Accuracy 0.9234\n",
            "Epoch 99 Batch 400 Loss 0.2476 Accuracy 0.9233\n",
            "Epoch 99 Batch 450 Loss 0.2481 Accuracy 0.9231\n",
            "Epoch 99 Batch 500 Loss 0.2492 Accuracy 0.9228\n",
            "Epoch 99 Batch 550 Loss 0.2500 Accuracy 0.9225\n",
            "Epoch 99 Batch 600 Loss 0.2506 Accuracy 0.9225\n",
            "Epoch 99 Batch 650 Loss 0.2510 Accuracy 0.9224\n",
            "Epoch 99 Batch 700 Loss 0.2518 Accuracy 0.9222\n",
            "Epoch 99 Batch 750 Loss 0.2520 Accuracy 0.9221\n",
            "Epoch 99 Batch 800 Loss 0.2524 Accuracy 0.9219\n",
            "Epoch 99 Batch 850 Loss 0.2533 Accuracy 0.9217\n",
            "Epoch 99 Batch 900 Loss 0.2539 Accuracy 0.9215\n",
            "Epoch 99 Batch 950 Loss 0.2544 Accuracy 0.9214\n",
            "Epoch 99 Batch 1000 Loss 0.2550 Accuracy 0.9213\n",
            "Epoch 99 Batch 1050 Loss 0.2555 Accuracy 0.9211\n",
            "Epoch 99 Batch 1100 Loss 0.2560 Accuracy 0.9209\n",
            "Epoch 99 Batch 1150 Loss 0.2570 Accuracy 0.9207\n",
            "Epoch 99 Batch 1200 Loss 0.2576 Accuracy 0.9205\n",
            "Epoch 99 Loss 0.2584 Accuracy 0.9203\n",
            "Time taken for 1 epoch: 116.27 secs\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.3345 Accuracy 0.9008\n",
            "Epoch 100 Batch 50 Loss 0.2433 Accuracy 0.9258\n",
            "Epoch 100 Batch 100 Loss 0.2401 Accuracy 0.9269\n",
            "Epoch 100 Batch 150 Loss 0.2401 Accuracy 0.9263\n",
            "Epoch 100 Batch 200 Loss 0.2419 Accuracy 0.9257\n",
            "Epoch 100 Batch 250 Loss 0.2421 Accuracy 0.9259\n",
            "Epoch 100 Batch 300 Loss 0.2435 Accuracy 0.9254\n",
            "Epoch 100 Batch 350 Loss 0.2435 Accuracy 0.9253\n",
            "Epoch 100 Batch 400 Loss 0.2441 Accuracy 0.9250\n",
            "Epoch 100 Batch 450 Loss 0.2451 Accuracy 0.9247\n",
            "Epoch 100 Batch 500 Loss 0.2460 Accuracy 0.9243\n",
            "Epoch 100 Batch 550 Loss 0.2471 Accuracy 0.9240\n",
            "Epoch 100 Batch 600 Loss 0.2478 Accuracy 0.9238\n",
            "Epoch 100 Batch 650 Loss 0.2488 Accuracy 0.9236\n",
            "Epoch 100 Batch 700 Loss 0.2491 Accuracy 0.9235\n",
            "Epoch 100 Batch 750 Loss 0.2495 Accuracy 0.9233\n",
            "Epoch 100 Batch 800 Loss 0.2501 Accuracy 0.9232\n",
            "Epoch 100 Batch 850 Loss 0.2506 Accuracy 0.9231\n",
            "Epoch 100 Batch 900 Loss 0.2512 Accuracy 0.9230\n",
            "Epoch 100 Batch 950 Loss 0.2521 Accuracy 0.9226\n",
            "Epoch 100 Batch 1000 Loss 0.2523 Accuracy 0.9226\n",
            "Epoch 100 Batch 1050 Loss 0.2533 Accuracy 0.9223\n",
            "Epoch 100 Batch 1100 Loss 0.2543 Accuracy 0.9220\n",
            "Epoch 100 Batch 1150 Loss 0.2547 Accuracy 0.9218\n",
            "Epoch 100 Batch 1200 Loss 0.2556 Accuracy 0.9216\n",
            "Saving checkpoint for epoch 100 at ./checkpoints/train/ckpt-20\n",
            "Epoch 100 Loss 0.2563 Accuracy 0.9213\n",
            "Time taken for 1 epoch: 116.82 secs\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. 모델 평가"
      ],
      "metadata": {
        "id": "CDgebu5Fb5b1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Translator(tf.Module):\n",
        "  def __init__(self, tokenizers, transformer):\n",
        "    self.tokenizers = tokenizers\n",
        "    self.transformer = transformer\n",
        "\n",
        "  def __call__(self, sentence, max_length=20):\n",
        "    # input sentence: 영어 -> start, end token 추가\n",
        "    assert isinstance(sentence, tf.Tensor)\n",
        "    if len(sentence.shape) == 0:\n",
        "      sentence = sentence[tf.newaxis]\n",
        "\n",
        "    sentence = self.tokenizers.en.tokenize(sentence).to_tensor() # input: 영어\n",
        "\n",
        "    encoder_input = sentence\n",
        "\n",
        "    # target: 프랑스어 -> transformer에 주어지는 첫 번째 token은 프랑스어의 start token\n",
        "    start_end = self.tokenizers.fr.tokenize([''])[0] # target: 프랑스어\n",
        "    start = start_end[0][tf.newaxis]\n",
        "    end = start_end[1][tf.newaxis]\n",
        "\n",
        "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "    output_array = output_array.write(0, start)\n",
        "\n",
        "    for i in tf.range(max_length):\n",
        "      output = tf.transpose(output_array.stack())\n",
        "      predictions, _ = self.transformer([encoder_input, output], training=False)\n",
        "\n",
        "      predictions = predictions[:, -1:, :] \n",
        "\n",
        "      predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "      output_array = output_array.write(i+1, predicted_id[0])\n",
        "\n",
        "      if predicted_id == end:\n",
        "        break\n",
        "\n",
        "    output = tf.transpose(output_array.stack())\n",
        "\n",
        "    text = tokenizers.fr.detokenize(output)[0]  # output text -> 프랑스어\n",
        "\n",
        "    tokens = tokenizers.fr.lookup(output)[0] # output tokens -> 프랑스어\n",
        "\n",
        "    _, attention_weights = self.transformer([encoder_input, output[:,:-1]], training=False)\n",
        "\n",
        "    return text, tokens, attention_weights"
      ],
      "metadata": {
        "id": "pXzax8mEFgPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator(tokenizers, transformer)"
      ],
      "metadata": {
        "id": "VZ8W9p7PFhjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_translation(sentence, tokens, real):\n",
        "  print(f'{\"Input:\":15s}: {sentence}')\n",
        "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
        "  print(f'{\"Real Translation\":15s}: {real}')"
      ],
      "metadata": {
        "id": "y_Jr2nrDFjKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset에서 5개 샘플 추출 \n",
        "for en_examples, fr_examples in train_dataset.batch(5).take(1):\n",
        "  for en in en_examples.numpy():\n",
        "    print(en.decode('utf-8'))\n",
        "\n",
        "  print()\n",
        "\n",
        "  for fr in fr_examples.numpy():\n",
        "    print(fr.decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5xrMnoO2iPT",
        "outputId": "5c730c2a-2529-41f4-db48-f30aaff1477d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please put your cigarette out.\n",
            "She committed a crime.\n",
            "How did he come here?\n",
            "Dogs bark.\n",
            "Why didn't you just leave?\n",
            "\n",
            "Éteignez votre cigarette s'il vous plaît.\n",
            "Elle a commis un crime.\n",
            "Comment est-il venu ici ?\n",
            "Les chiens aboient.\n",
            "Pourquoi n'es-tu pas simplement partie ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_dataset에서 5개 샘플 추출 \n",
        "for en_examples, fr_examples in test_dataset.batch(5).take(1):\n",
        "  for en in en_examples.numpy():\n",
        "    print(en.decode('utf-8'))\n",
        "\n",
        "  print()\n",
        "\n",
        "  for fr in fr_examples.numpy():\n",
        "    print(fr.decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSKOSxke2tr1",
        "outputId": "ca9435ae-10c0-4116-c395-625f2e76477b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I want to know what's in this box.\n",
            "Tell me what to do with it.\n",
            "No one has ever kissed Tom.\n",
            "Everybody was stunned.\n",
            "Did you catch what he said?\n",
            "\n",
            "Je veux savoir ce qu'il y a dans cette boîte.\n",
            "Dis-moi ce que je dois en faire.\n",
            "Personne n'a jamais embrassé Tom.\n",
            "Tout le monde fut stupéfait.\n",
            "As-tu saisi ce qu'il a dit ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) 모델 저장"
      ],
      "metadata": {
        "id": "SbsJ4TxaeA6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### tf.Module 하위 클래스로 래핑"
      ],
      "metadata": {
        "id": "_Xd-y54GE9RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExportTranslator(tf.Module):\n",
        "  def __init__(self, translator):\n",
        "    self.translator = translator\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
        "  def __call__(self, sentence):\n",
        "    (result, \n",
        "     tokens,\n",
        "     attention_weights) = self.translator(sentence, max_length=100)\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "jEG7ATMTdXu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하위 클래스로 래핑한 모델 불러오기 \n",
        "ex_translator = ExportTranslator(translator)"
      ],
      "metadata": {
        "id": "1AWveMS9diiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ex_translator(\"I want to know what's in this box.\").numpy()"
      ],
      "metadata": {
        "id": "An09nhkqdjsQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9e5b78c-ec05-40f0-a44d-0c979d6a24a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b\"je veux savoir ce qu ' il y a dans cette boite .\""
            ]
          },
          "metadata": {},
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) 모델 비교"
      ],
      "metadata": {
        "id": "LotUghxTcG1q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2-1) EPOCHS = 30"
      ],
      "metadata": {
        "id": "g6E9whADesO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Loss 0.6392 \n",
        "\n",
        "- Accuracy 0.8205"
      ],
      "metadata": {
        "id": "5VzzpITjfGtO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2-1-1) train_dataset 샘플 번역 결과"
      ],
      "metadata": {
        "id": "CjB1ZaHIceGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Please put your cigarette out.\"\n",
        "real = \"Éteignez votre cigarette s'il vous plaît.\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "\n",
        "print_translation(sentence, translated_text, real)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oF4VPM_iCP76",
        "outputId": "c0dfad55-f3c4-4b0f-b836-e80a6264be47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : Please put your cigarette out.\n",
            "Prediction     : veuillez mettre votre boc - s ' il vous plait .\n",
            "Real Translation: Éteignez votre cigarette s'il vous plaît.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"She committed a crime.\"\n",
        "real = \"Elle a commis un crime.\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "\n",
        "print_translation(sentence, translated_text, real)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTF-diu3IM0M",
        "outputId": "64d195be-7d46-43e5-c31e-ec5ec8e3464f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : She committed a crime.\n",
            "Prediction     : elle a commis un crime .\n",
            "Real Translation: Elle a commis un crime.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"How did he come here?\"\n",
        "real = \"Comment est-il venu ici ?\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "\n",
        "print_translation(sentence, translated_text, real)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dNZPapVIOyP",
        "outputId": "05bf69a8-d694-409e-ddf7-56357dfb9571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : How did he come here?\n",
            "Prediction     : comment est - il venu ici ?\n",
            "Real Translation: Comment est-il venu ici ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Dogs bark.\"\n",
        "real = \"Les chiens aboient.\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "\n",
        "print_translation(sentence, translated_text, real)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuMTxjz1IPSR",
        "outputId": "32303dcb-934d-47e9-b74d-9be8b5a1bc99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : Dogs bark.\n",
            "Prediction     : les chiens ontboies .\n",
            "Real Translation: Les chiens aboient.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Why didn't you just leave?\"\n",
        "real = \"Pourquoi n'es-tu pas simplement partie ?\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "\n",
        "print_translation(sentence, translated_text, real)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkgJScrHIPwg",
        "outputId": "876f0321-6bbb-42f6-d7b5-c5bcfb60ecb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : Why didn't you just leave?\n",
            "Prediction     : pourquoi n ' etes - vous pas simplement partie ?\n",
            "Real Translation: Pourquoi n'es-tu pas simplement partie ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2-1-2) test_dataset 샘플 번역 결과"
      ],
      "metadata": {
        "id": "zooeMk0Kcji7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I want to know what's in this box.\"\n",
        "real = \"Je veux savoir ce qu'il y a dans cette boîte.\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "\n",
        "print_translation(sentence, translated_text, real)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFwFbB-oCQSk",
        "outputId": "adefc679-93d1-49b6-d221-72616e77e5fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : I want to know what's in this box.\n",
            "Prediction     : je veux savoir ce qui se trouve dans cette piece .\n",
            "Real Translation: Je veux savoir ce qu'il y a dans cette boîte.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Tell me what to do with it.\"\n",
        "real = \"Dis-moi ce que je dois en faire.\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "\n",
        "print_translation(sentence, translated_text, real)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRK6cfpIITFg",
        "outputId": "ddc92ed2-79a4-46e4-d033-6862f5da5c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : Tell me what to do with it.\n",
            "Prediction     : dis - moi quoi faire avec .\n",
            "Real Translation: Dis-moi ce que je dois en faire.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"\bNo one has ever kissed Tom.\"\n",
        "real = \"Personne n'a jamais embrassé Tom.\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "\n",
        "print_translation(sentence, translated_text, real)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfnI43smITua",
        "outputId": "cf7bff6f-ffa7-478d-be81-c274c9a04a26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : \bNo one has ever kissed Tom.\n",
            "Prediction     : personne n ' a jamais embrasse tom .\n",
            "Real Translation: Personne n'a jamais embrassé Tom.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"\bEverybody was stunned.\"\n",
        "real = \"Tout le monde fut stupéfait.\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "\n",
        "print_translation(sentence, translated_text, real)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVQC8c7TIUOK",
        "outputId": "3abf5de5-3efe-411e-94f2-43f2f9a9bb63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : \bEverybody was stunned.\n",
            "Prediction     : tout le monde a ete stupefait .\n",
            "Real Translation: Tout le monde fut stupéfait.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"\bDid you catch what he said?\"\n",
        "real = \"As-tu saisi ce qu'il a dit ?\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "\n",
        "print_translation(sentence, translated_text, real)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRhQBwseIUoM",
        "outputId": "7c8b9ce4-5962-4130-9975-d60650fbcc9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : \bDid you catch what he said?\n",
            "Prediction     : as - tu saisi ce qu ' il a dit ?\n",
            "Real Translation: As-tu saisi ce qu'il a dit ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cf) 임의로 입력한 문장 "
      ],
      "metadata": {
        "id": "m6QquNMUJM6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"\bfinally, i finished my project !\"\n",
        "real = \"finalement, j'ai fini mon projet !\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "\n",
        "print_translation(sentence, translated_text, real)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8YMf91QJVeM",
        "outputId": "474695bc-7c6e-4c8a-948d-47e12ebcb8c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : \bfinally, i finished my project !\n",
            "Prediction     : enfin , j ' ai fini mon travail .\n",
            "Real Translation: finalement, j'ai fini mon projet !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2-2) EPOCHS = 100"
      ],
      "metadata": {
        "id": "5PfF5pOMcUf5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Loss 0.2563 \n",
        "\n",
        "- Accuracy 0.9213"
      ],
      "metadata": {
        "id": "H-POFzNAUAtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2-2-1) train_dataset 샘플 번역 결과"
      ],
      "metadata": {
        "id": "5BDfA96lco42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Please put your cigarette out.\"\n",
        "real = \"Éteignez votre cigarette s'il vous plaît.\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "\n",
        "print_translation(sentence, translated_text, real)"
      ],
      "metadata": {
        "id": "C7uGMBJccoXf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af586558-8bec-43f5-830e-76e1a0710cc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : Please put your cigarette out.\n",
            "Prediction     : eteignez votre cigarette s ' il vous plait .\n",
            "Real Translation: Éteignez votre cigarette s'il vous plaît.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"She committed a crime.\"\n",
        "real = \"Elle a commis un crime.\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "\n",
        "print_translation(sentence, translated_text, real)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ursvnBZWmvmR",
        "outputId": "0798651f-212c-4eda-cc35-bf3da02a88e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : She committed a crime.\n",
            "Prediction     : elle a commis un crime .\n",
            "Real Translation: Elle a commis un crime.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"How did he come here?\"\n",
        "real = \"Comment est-il venu ici ?\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "\n",
        "print_translation(sentence, translated_text, real)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9en2MotFmv2a",
        "outputId": "d02031cf-fa82-494e-cd07-ccfbe5c757c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : How did he come here?\n",
            "Prediction     : comment s ' est - il passe ici ?\n",
            "Real Translation: Comment est-il venu ici ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Dogs bark.\"\n",
        "real = \"Les chiens aboient.\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "\n",
        "print_translation(sentence, translated_text, real)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prbJdsDcmwFQ",
        "outputId": "19934b23-d95c-447a-8083-48b543b00215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : Dogs bark.\n",
            "Prediction     : des chiens aboient .\n",
            "Real Translation: Les chiens aboient.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Why didn't you just leave?\"\n",
        "real = \"Pourquoi n'es-tu pas simplement partie ?\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "\n",
        "print_translation(sentence, translated_text, real)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzk3jNbymwXf",
        "outputId": "4978f3e9-3326-45a4-dda7-5dfa6a16f325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : Why didn't you just leave?\n",
            "Prediction     : pourquoi n ' etes - vous pas simplement partis ?\n",
            "Real Translation: Pourquoi n'es-tu pas simplement partie ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2-2-2) test_dataset 샘플 번역 결과"
      ],
      "metadata": {
        "id": "do4Hd4WIcp7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I want to know what's in this box.\"\n",
        "real = \"Je veux savoir ce qu'il y a dans cette boîte.\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "\n",
        "print_translation(sentence, translated_text, real)"
      ],
      "metadata": {
        "id": "D3uq3D-zcujo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b405e2e8-a94a-4011-8f50-86bd7ca8d0e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : I want to know what's in this box.\n",
            "Prediction     : je veux savoir ce qu ' il y a dans cette boite .\n",
            "Real Translation: Je veux savoir ce qu'il y a dans cette boîte.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Tell me what to do with it.\"\n",
        "real = \"Dis-moi ce que je dois en faire.\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "\n",
        "print_translation(sentence, translated_text, real)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNGak4cgm9FQ",
        "outputId": "79cce94d-2b2b-4596-8751-e4dea591041c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : Tell me what to do with it.\n",
            "Prediction     : dis - moi ce que je dois en faire .\n",
            "Real Translation: Dis-moi ce que je dois en faire.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"\bNo one has ever kissed Tom.\"\n",
        "real = \"Personne n'a jamais embrassé Tom.\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "\n",
        "print_translation(sentence, translated_text, real)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqsjYRXHm9eO",
        "outputId": "3ba99cb7-a7e6-4c69-b956-8c2167b4214f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : \bNo one has ever kissed Tom.\n",
            "Prediction     : personne n ' a jamais embrasse tom .\n",
            "Real Translation: Personne n'a jamais embrassé Tom.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"\bEverybody was stunned.\"\n",
        "real = \"Tout le monde fut stupéfait.\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "\n",
        "print_translation(sentence, translated_text, real)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6PEEq8Vm97Z",
        "outputId": "4958ef48-6340-46a1-a6cd-f2325aa9dbba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : \bEverybody was stunned.\n",
            "Prediction     : tous les mous furent anfies .\n",
            "Real Translation: Tout le monde fut stupéfait.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"\bDid you catch what he said?\"\n",
        "real = \"As-tu saisi ce qu'il a dit ?\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "\n",
        "print_translation(sentence, translated_text, real)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxSbEuSXm-NG",
        "outputId": "f3217e41-2841-4b9e-d246-e5f179ad75d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : \bDid you catch what he said?\n",
            "Prediction     : avez - vous saisi ce qu ' il a dit ?\n",
            "Real Translation: As-tu saisi ce qu'il a dit ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cf) 임의로 입력한 문장 "
      ],
      "metadata": {
        "id": "GjonhrWxnm99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"\bfinally, i finished my project !\"\n",
        "real = \"finalement, j'ai fini mon projet !\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "\n",
        "print_translation(sentence, translated_text, real)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDmY1cbEnVYN",
        "outputId": "36fbabe5-5c2d-4e42-f796-cd492ef1c77c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : \bfinally, i finished my project !\n",
            "Prediction     : enfin , j ' ai revauche mon plan .\n",
            "Real Translation: finalement, j'ai fini mon projet !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) 결과 시각화"
      ],
      "metadata": {
        "id": "853RDQizczYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Why didn't you just leave?\"\n",
        "real = \"Pourquoi n'es-tu pas simplement partie ?\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "\n",
        "print_translation(sentence, translated_text, real)"
      ],
      "metadata": {
        "id": "NmT6CpPqgdH8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "506ce8a1-642f-49ff-8ef6-5b8185670ff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : Why didn't you just leave?\n",
            "Prediction     : pourquoi n ' etes - vous pas simplement partis ?\n",
            "Real Translation: Pourquoi n'es-tu pas simplement partie ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3-1) Attention 첫 번째 head 시각화"
      ],
      "metadata": {
        "id": "hcH3w7qeVlAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention_head(in_tokens, translated_tokens, attention):\n",
        "\n",
        "  translated_tokens = translated_tokens[1:]\n",
        "\n",
        "  ax = plt.gca()\n",
        "  ax.matshow(attention)\n",
        "  ax.set_xticks(range(len(in_tokens)))\n",
        "  ax.set_yticks(range(len(translated_tokens)))\n",
        "\n",
        "  labels = [label.decode('utf-8') for label in in_tokens.numpy()]\n",
        "  ax.set_xticklabels(\n",
        "      labels, rotation=90)\n",
        "\n",
        "  labels = [label.decode('utf-8') for label in translated_tokens.numpy()]\n",
        "  ax.set_yticklabels(labels)"
      ],
      "metadata": {
        "id": "96aosbLvgf15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "head = 0\n",
        "attention_heads = tf.squeeze(\n",
        "  attention_weights['decoder_layer4_block2'], 0)\n",
        "attention = attention_heads[head]\n",
        "attention.shape"
      ],
      "metadata": {
        "id": "hbkI3bAugkdM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e51ac69-0c1a-4149-a121-ddfb8fbc989c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([11, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "in_tokens = tf.convert_to_tensor([sentence])\n",
        "in_tokens = tokenizers.en.tokenize(in_tokens).to_tensor()\n",
        "in_tokens = tokenizers.en.lookup(in_tokens)[0]\n",
        "in_tokens"
      ],
      "metadata": {
        "id": "LoMeiW99goGQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f855031-21f6-43a2-a0a4-940efb78abc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=string, numpy=\n",
              "array([b'[START]', b'why', b'didn', b\"'\", b't', b'you', b'just', b'leave',\n",
              "       b'?', b'[END]'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translated_tokens"
      ],
      "metadata": {
        "id": "udAd7_lFgpJ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecc10ffb-2e99-4dd2-cfdb-e95805fae0c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(12,), dtype=string, numpy=\n",
              "array([b'[START]', b'pourquoi', b'n', b\"'\", b'etes', b'-', b'vous',\n",
              "       b'pas', b'simplement', b'partis', b'?', b'[END]'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_attention_head(in_tokens, translated_tokens, attention)"
      ],
      "metadata": {
        "id": "IZZhKYTjgrpF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "968de683-df8b-4587-c54d-00ed8ea4753a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3-2) Attention head별 시각화"
      ],
      "metadata": {
        "id": "xzj3Wrn-V2sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention_weights(sentence, translated_tokens, attention_heads):\n",
        "  in_tokens = tf.convert_to_tensor([sentence])\n",
        "  in_tokens = tokenizers.en.tokenize(in_tokens).to_tensor()\n",
        "  in_tokens = tokenizers.en.lookup(in_tokens)[0]\n",
        "  in_tokens\n",
        "\n",
        "  fig = plt.figure(figsize=(16, 8))\n",
        "\n",
        "  for h, head in enumerate(attention_heads):\n",
        "    ax = fig.add_subplot(2, 4, h+1)\n",
        "\n",
        "    plot_attention_head(in_tokens, translated_tokens, head)\n",
        "\n",
        "    ax.set_xlabel(f'Head {h+1}')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Z2WXb1TwgwaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_attention_weights(sentence, translated_tokens,\n",
        "                       attention_weights['decoder_layer4_block2'][0])"
      ],
      "metadata": {
        "id": "pKKPdRYEg5aB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "a36a4fbe-d598-4285-a0f3-0a13880bc633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x576 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. 결론"
      ],
      "metadata": {
        "id": "te8ydeKJd2Gn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) 결론"
      ],
      "metadata": {
        "id": "P9tz95n-fi4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1-1) 가설 검정"
      ],
      "metadata": {
        "id": "jMa9ajTiZH5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "30 epochs만 진행한 모델에서도 주어에 맞는 동사변화나 시제, 도치법이 완벽하진 않더라도 비슷하게 사용되었기 때문에\n",
        "\n",
        "> 10만 개의 데이터로 학습된 번역 모델은 문법적 특성을 지키지 못한다.\n",
        "\n",
        "라는 **가설은 틀린 것**으로 검증됨.\n",
        "\n"
      ],
      "metadata": {
        "id": "MJpbXf_-Xxlq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1-2) 모델 비교"
      ],
      "metadata": {
        "id": "Iu3w-Co_ZRXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델을 30 epochs 와 100 epochs로 학습시킨 결과, <br>\n",
        "**Accuracy**는 **10% 정도 차이** 나지만 **Loss**는 100 epochs 만큼 학습시킨 모델이 **38% 적은 것으로** 나타남. "
      ],
      "metadata": {
        "id": "056i8oIFZamn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "번역 결과를 봤을 때도 **100 epochs** 만큼 학습 시킨 모델이 **실제 번역과 조금 더 유사**하게 번역했지만 <br> 프랑스어 **사전에 없는 단어들을 만들어내기도** 함."
      ],
      "metadata": {
        "id": "scKYd9y2aE2G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) 한계점 및 추후 해결 방안"
      ],
      "metadata": {
        "id": "_9bGar_nfhmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 번역 결과 출력 <br>\n",
        "- 프랑스어의 악센트(accent), 대문자 표현 불가\n",
        "\n",
        "- 토큰화된 문장을 다시 텍스트로 변환해 주는 **detokenize의 설정을 수정**해 볼 것.  "
      ],
      "metadata": {
        "id": "fOaxVQw2nnEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 모델 성능 평가지표 <br>\n",
        "- Accuracy 사용\n",
        "\n",
        "- 추후에는 기계 번역 결과와 사람이 직접 번역한 결과가 얼마나 유사한지 비교하는 대표적인 기계 번역 성능 평가 방식인 <br> **BLEU Score**를 사용해 성능을 다시 측정해 볼 것."
      ],
      "metadata": {
        "id": "1xnGMa8SbwVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 일방향 번역\n",
        "\n",
        "- 영어 -> 프랑스어로만 번역 가능\n",
        "\n",
        "- 추후에는 영어 <-> 프랑스어 양방향 번역뿐만 아니라 <br> **제로샷 학습**(Zero-shot learning), **mT5 모델**을 이용해 한국어 <-> 프랑스어 번역이 가능한 **다국어 번역 모델**로 발전시킬 것. "
      ],
      "metadata": {
        "id": "Bg5JJibcdaiq"
      }
    }
  ]
}